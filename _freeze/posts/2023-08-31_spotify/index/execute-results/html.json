{
  "hash": "d1b82c9d85b363a408da1ac7e9347c7a",
  "result": {
    "markdown": "---\ntitle: \"Exploring Music Tastes with Machine Learning\"\ndescription: Comparing a friend's and my Spotify liked songs by building a binary classfication model. \n\neditor: visual\nauthor:\n  - name: Kiran Favre\n    url: https://kiranfavre.github.io\n    affiliation: Master of Environmental Data Science\n    affiliation-url: https://ucsb-meds.github.io/\ndate: 09-06-2023\ncitation: \n  url: https://kiranfavre.github.io/posts/2022-12-08_EDS222_final/ \ncategories: [R, Machine Learning, Music]\nformat:\n  html:\n    code-fold: false\n    code-summary: \"Show code\"\n---\n\n#figure code folding\n\n**Introduction**\n\nI love listening to all types of music, and used machine learning models in R to understand my taste a little bit better and compare my friend and my music taste!\n\nI began by requesting data from the Spotify API and using the data on my liked songs to build a model that is a binary classifier. To do so, I will be using three  Machine Learning algorithms: \n\n- K-Nearest Neighbor \n\n- Decision Tree Model\n\n- Random Forest Model\n\n-----------From Mateo OH--------------- \nAnd test on the test data. For each model, we went through the general ML process. Here is this data we have to spend, and you don't need to worry about the test data being reused/leakage. Leakage can occur in each model, but set up process to ensure there is no leakage. This is why we break up into test, split, and train. With leakage, we will get a less accurate prediction since the data was used to create the model. We started with splitting, then pre-processing (using step function and the recipe) (for all models!). Then for each model we do this separately: set up specification of model w hyper parameters that you tune (finding best version of model), we use the cv folds to do this (break data into 10 sections, and 1 is test data and rest are training. then next, next). R tries a bunch of combos of the hyper parameters. Then the model creates predictions of 0 and 1. Then we fit the model we found onto the test data we left at beginning. For analyzing at the end, could look at confusion matrix of different errors made , could look at distribution of the predictions. Could look at the probability of predictions higher or lower than 50%. -----------\n\nThis week's lab is a musical lab. You'll be requesting data from the Spotify API and using it to build k-nearest neighbor and decision tree models.\n\nErica's blog: https://ericamarie9016.github.io/posts/2023-02-22-spotify/\n\n\n\n\n::: {.cell}\n\n:::\n\n\n**Accessing Spotify API**\n\nTo begin gathering data from the Spotify API, I have to create and access a token containing the client ID and client secret. To do so, I began by navigating to [Spotify for Developers](https://developer.spotify.com/) and created an application. More information on accessing the Spotify API  can be found in [the documentation](https://developer.spotify.com/documentation/web-api).\n\n\nHere, I am setting them as system values so I don't have to provide the client ID & secret each time the API is used, and combining them to create an access token. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nSys.setenv(SPOTIFY_CLIENT_ID = 'your_token')\nSys.setenv(SPOTIFY_CLIENT_SECRET = 'your_token')\n\naccess_token <- get_spotify_access_token(\n  client_id = Sys.getenv(\"SPOTIFY_CLIENT_ID\"),\n  client_secret = Sys.getenv(\"SPOTIFY_CLIENT_SECRET\")\n)\n```\n:::\n\n\n\n**Data Preparation**\n  \nThe function 'get_my_saved_tracks()' from the *spotifyr* package will request all my liked tracks on Spotify. However, when called, the Spotify API will only return a dataframe with 50 tracks at a time. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsaved_tracks <- get_my_saved_tracks() #function from the spotifyr package\n```\n:::\n\n\nSince I want to analyze all my likes, I will have to make many requests. Instead of doing this manually, I'll use a function to combine all the requests into one call. \n\n::: {.cell}\n\n```{.r .cell-code}\n#writing a function to combine my requests into one call since this function only returns up to 50 tracks at a time when called\nget_saved_tracks <- function(limit = 50,\n                             authorization,\n                             offset = 0) {\n  tracks <- data.frame()\n  for (i in 1:7) {\n    new_tracks <- get_my_saved_tracks(limit = limit,\n                                      offset = offset)\n    tracks <- rbind(tracks,\n                    as.data.frame(new_tracks))\n    offset <- offset + limit\n  }\n  return(tracks)\n}\n\nmy_tracks <- get_saved_tracks(authorization = access_token)\n```\n:::\n\n\nNow that I have all my liked songs, I am going to request more information from the Spotify API to understand my taste better. To do so, I will give the API a list of my song IDs using the function *get_track_audio_features*. This will return a dataframe of audio features, including the tracks and their attributes. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_audio_features <- data.frame()   # create base empty data frame\n\nfor(i in seq(from = 1, to = 350, by = 100)) { # loop through all songs\n  \n  # collect 100 rows starting from i\n  row_index <- i:(i + 99)   \n  \n  # pull out features for set rows\n  audio <- get_track_audio_features(my_tracks$track.id[row_index])\n  \n  # add features to dataframe\n  my_audio_features <- rbind(my_audio_features, audio)\n}\n\n\nmy_audio_features <- drop_na(my_audio_features)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# add songs_data$track.name\nkiran_audio <- cbind(my_audio_features,\n                     track.name = my_tracks$track.name,\n                     track.popularity = my_tracks$track.popularity)\n\nkiran_audio <- kiran_audio %>% \n  select(-c(uri, track_href, analysis_url, type, id))\n\n#make a csv\nwrite_csv(kiran_audio, \"kiran_audio.csv\")\n```\n:::\n\n\nNow, I swapped data with my friend, Erica. Since I want to compare our music tastes, I began by combining our data into a new dataframe.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#read in erica's data \nerica_audio <- read_csv(\"ericas_audio.csv\")\n\n#add listener id column \nkiran_audio <- kiran_audio %>%  \n  mutate(listener_id = 'Kiran')\n\n\nkiran_erica_audio <- rbind(kiran_audio, erica_audio)\n\n#downloading combined data into a csv\nwrite_csv(combined_audio, \"combined_audio.csv\")\n```\n:::\n\n\nHere I loaded in the downloaded dataframe, and appended a column called 'listener_id' so I know whose tracks are whose. \n\n::: {.cell}\n\n```{.r .cell-code}\n#combining our data \ncombined_audio <- read_csv(here(\"posts\",\n                                \"2023-08-31_spotify\",\n                                \"combined_audio.csv\")) %>%  \n    mutate(listener_id = as.factor(listener_id))\n```\n:::\n\n\n**Data Exploration**\n\nNow that I have prepared the data with the steps above, I can start exploring some aspects of my data, my friend's, and compare them!\n\nFirst, I wanted to look at all the variables that the Spotify API includes when accessing audio features. \n\n::: {.cell}\n\n```{.r .cell-code}\naudio_features <- combined_audio %>% \n  colnames() %>% \n  as.data.frame()\n\n#removing the first row of insignificant data \naudio_features_table <- as.data.frame(audio_features[-1,])\n\n#renaming column to audio feature\ncolnames(audio_features_table)[1] <- \"Audio Features\"\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naudio_features_table\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Audio Features\n1              ...2\n2      danceability\n3            energy\n4               key\n5          loudness\n6              mode\n7       speechiness\n8      acousticness\n9  instrumentalness\n10         liveness\n11          valence\n12            tempo\n13      duration_ms\n14   time_signature\n15       track.name\n16 track.popularity\n17      listener_id\n```\n:::\n:::\n\n\nI'm also curious about the relationship between some of the variables mentioned above..\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#danceability and energy\nhexplot_1 <- ggplot(data = combined_audio,\n         aes(energy, danceability)) +\n  geom_hex() +\n  scale_fill_viridis_c(option = \"magma\") \n\n#danceability and loudness\nhexplot_2 <- ggplot(data = combined_audio,\n         aes(loudness, danceability)) +\n  geom_hex() +\n  scale_fill_viridis_c(option = \"magma\")\n\n\n#acousticness and energy\nhexplot_3 <- ggplot(data = combined_audio,\n         aes(acousticness, energy)) +\n  geom_hex() +\n  scale_fill_viridis_c(option = \"magma\")\n\n\n#acousticness and energy\nhexplot_4 <- ggplot(data = combined_audio,\n         aes(tempo, loudness)) +\n  geom_hex() +\n  scale_fill_viridis_c(option = \"magma\")\n\n\nggarrange(hexplot_1, hexplot_2, hexplot_3, hexplot_4)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\n\n\nNow, I'll compare who has more popular songs!\n\n::: {.cell}\n\n```{.r .cell-code}\ncombined_audio %>%\n  arrange(desc(track.popularity)) %>% \n  select(track.popularity,\n         track.name,\n         listener_id) %>% \n  rename('track name' = track.name,\n         'listener' = listener_id) %>%  \n  head(8) %>% \n  kable()\n```\n\n::: {.cell-output-display}\n| track.popularity|track name               |listener |\n|----------------:|:------------------------|:--------|\n|               85|Neverita                 |Kiran    |\n|               84|Escapism. - Sped Up      |Kiran    |\n|               82|Pink + White             |Kiran    |\n|               81|Beggin'                  |Erica    |\n|               81|All The Stars (with SZA) |Erica    |\n|               81|Let Me Down Slowly       |Erica    |\n|               80|Formula                  |Kiran    |\n|               80|Un Coco                  |Kiran    |\n:::\n:::\n\n\nLooks like I have a the most popular songs in this dataset from this little snippet. Now I'll visualize the data to get a better picture. \n\n\n::: {.cell}\n\n```{.r .cell-code}\n#visualize the data!\n#who listens to more popular track\npopularity_plot <- ggplot(data = combined_audio, aes(x = track.popularity)) + \n  geom_bar(aes(fill = listener_id)) +\n   labs(title = \"Distribution of Song Popularity by Listener\",\n       x = \"Song Popularity\", y = \"Count\") +\n  scale_fill_manual(values = c(\"#1721d4\", \"#02b34b\")) +\n  theme_minimal() \n\npopularity_plot\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n**Modeling**\n  \nAs I mentioned earlier, I will create two models, a K-Nearest Neighbor model and a decision tree model. Now that I have the data prepared and I understand it better, I can make these models predict whether a track belongs to me or Erica's Spotify list. \n\nI'm starting by splitting the data into three sets: testing, splitting, and training data sets. \n\n::: {.cell}\n\n```{.r .cell-code}\n#remove track id  & index \ncombined_audio <- combined_audio %>% \n  select(-c(...1,...2,track.name))\n\n#set seed for reproducibility\nset.seed(711)\n\n#split the data \naudio_split <- initial_split(combined_audio)\naudio_test <- testing(audio_split)\naudio_train <- training(audio_split)\n```\n:::\n\n\nNow, I will run through the three algorithms mentioned above. With each model, I will go through the following steps:\n\n1) Preprocessing: Using a step function and recipe on the training data. \n\n2) Set model specification: Tune specification of model with hyper parameters to finding best version of model. I will use cross validation folds to do this, which basically breaks the data into 10 sections, leaving 1 section as test data and rest are training. Then R continues thing process through all the broken up sections of data to determine the best hyper-parameters. The model will create predictions of 0 or 1 based on this tuning step. \n\n3) Model fitting: Then we fit the model with the best hyper-parameters onto the test data we split at beginning.\n\n\nMODEL #1: K-Nearest Neighbors \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#preprocessing\n#recipe always define by training data\nmusic_rec <- recipe(listener_id ~., \n                    data = audio_train) %>%  \n  step_dummy(all_nominal(),\n             -all_outcomes(),\n             one_hot = TRUE) %>% \n  step_normalize(all_numeric(),\n                 -all_outcomes()) %>% \n  prep()\n\n#bake \nbaked_audio <- bake(music_rec, audio_train)\n\n#apply recipe to test data \nbaked_test <- bake(music_rec, audio_test)\n\n#specify knn model\nknn_spec <- nearest_neighbor() %>% \n  set_engine(\"kknn\") %>% \n  set_mode(\"classification\")\n\n#resampling folds\ncv_folds <- audio_train %>% \n  vfold_cv(v = 5)\n\n#put together into workflow\nknn_workflow <- workflow() %>% \n  add_model(knn_spec) %>% \n  add_recipe(music_rec)\n\n#fit resamples\nknn_resample <- knn_workflow %>% \n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#Define our KNN model with tuning\nknn_spec_tuned  <- \n  nearest_neighbor(neighbors = tune()) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"kknn\")\n#Check the model\nknn_spec_tuned\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune()\n\nComputational engine: kknn \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define a new workflow\nwf_knn_tuned <- workflow() |> \n  add_model(knn_spec_tuned) |> \n  add_recipe(music_rec)\n    \n# Fit the workflow on our predefined folds and hyperparameters\nfit_knn_cv <- wf_knn_tuned |> \n  tune_grid(\n    cv_folds, #tuning based on these folds \n    grid = data.frame(neighbors = c(1,5, seq(10,100,10)))\n    \n  )\n\n# Check the performance with collect_metrics()\nfit_knn_cv |>  collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 24 × 7\n   neighbors .metric  .estimator  mean     n std_err .config              \n       <dbl> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1         1 accuracy binary     0.633     5  0.0342 Preprocessor1_Model01\n 2         1 roc_auc  binary     0.633     5  0.0400 Preprocessor1_Model01\n 3         5 accuracy binary     0.658     5  0.0356 Preprocessor1_Model02\n 4         5 roc_auc  binary     0.708     5  0.0462 Preprocessor1_Model02\n 5        10 accuracy binary     0.667     5  0.0299 Preprocessor1_Model03\n 6        10 roc_auc  binary     0.731     5  0.0374 Preprocessor1_Model03\n 7        20 accuracy binary     0.667     5  0.0182 Preprocessor1_Model04\n 8        20 roc_auc  binary     0.744     5  0.0229 Preprocessor1_Model04\n 9        30 accuracy binary     0.677     5  0.0173 Preprocessor1_Model05\n10        30 roc_auc  binary     0.750     5  0.0160 Preprocessor1_Model05\n# ℹ 14 more rows\n```\n:::\n\n```{.r .cell-code}\nfinal_knn_wf <- wf_knn_tuned |>  \n    finalize_workflow(select_best(fit_knn_cv,\n                                  metric = \"accuracy\"))\n\n# Fitting our final workflow \nfinal_knn_fit <- final_knn_wf |> \n  fit(data = audio_train)\n\nmusic_pred <- final_knn_fit |> \n  predict(new_data = audio_test)\n\n# Write over 'final_fit' with this last_fit() approach \nfinal_knn_fit <- final_knn_wf |> \n  last_fit(audio_split)\n\nfinal_knn_fit$.predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n# A tibble: 159 × 6\n   .pred_Erica .pred_Kiran  .row .pred_class listener_id .config             \n         <dbl>       <dbl> <int> <fct>       <fct>       <chr>               \n 1       0.266       0.734     1 Kiran       Kiran       Preprocessor1_Model1\n 2       0.529       0.471     2 Erica       Kiran       Preprocessor1_Model1\n 3       0.215       0.785     5 Kiran       Kiran       Preprocessor1_Model1\n 4       0.533       0.467    10 Erica       Kiran       Preprocessor1_Model1\n 5       0.392       0.608    11 Kiran       Kiran       Preprocessor1_Model1\n 6       0.594       0.406    30 Erica       Kiran       Preprocessor1_Model1\n 7       0.366       0.634    31 Kiran       Kiran       Preprocessor1_Model1\n 8       0.699       0.301    34 Erica       Kiran       Preprocessor1_Model1\n 9       0.416       0.584    35 Kiran       Kiran       Preprocessor1_Model1\n10       0.401       0.599    41 Kiran       Kiran       Preprocessor1_Model1\n# ℹ 149 more rows\n```\n:::\n\n```{.r .cell-code}\n# Collect metrics on the test data\nknn_metrics <- final_knn_fit |> \n  collect_metrics()\n\nknn_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.667 Preprocessor1_Model1\n2 roc_auc  binary         0.758 Preprocessor1_Model1\n```\n:::\n:::\n\n\n\n  \n**MODEL #2: DECISION TREE**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#preprocess\ndec_tree_rec <- recipe(listener_id ~ .,\n                       data = audio_train) %>% \n  step_dummy(all_nominal(),\n             -all_outcomes(),\n             one_hot = TRUE) %>% \n  step_normalize(all_numeric(),\n                 -all_outcomes())\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n#dec tree specification tuned to the optimal parameters\n\n#tell the model that we are tuning hyperparams\ndec_tree_spec_tune <- decision_tree(\n  cost_complexity = tune(), #to tune, call tune()\n  tree_depth = tune(), \n  min_n = tune()) %>%  \n  set_engine(\"rpart\") %>%\n  set_mode(\"classification\")\n\ndec_tree_grid <- grid_regular(cost_complexity(),\n                              tree_depth(),\n                              min_n(),\n                              levels = 4) \n\ndec_tree_grid \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 64 × 3\n   cost_complexity tree_depth min_n\n             <dbl>      <int> <int>\n 1    0.0000000001          1     2\n 2    0.0000001             1     2\n 3    0.0001                1     2\n 4    0.1                   1     2\n 5    0.0000000001          5     2\n 6    0.0000001             5     2\n 7    0.0001                5     2\n 8    0.1                   5     2\n 9    0.0000000001         10     2\n10    0.0000001            10     2\n# ℹ 54 more rows\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndoParallel::registerDoParallel() #build trees in parallel\n#200s\ndec_tree_rs <- tune_grid(\n  dec_tree_spec_tune, \n  as.factor(listener_id)~.,\n  resamples = cv_folds,\n  grid = dec_tree_grid,\n  metrics = metric_set(accuracy)\n)\ndec_tree_rs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits           id    .metrics          .notes          \n  <list>           <chr> <list>            <list>          \n1 <split [379/95]> Fold1 <tibble [64 × 7]> <tibble [0 × 3]>\n2 <split [379/95]> Fold2 <tibble [64 × 7]> <tibble [0 × 3]>\n3 <split [379/95]> Fold3 <tibble [64 × 7]> <tibble [0 × 3]>\n4 <split [379/95]> Fold4 <tibble [64 × 7]> <tibble [0 × 3]>\n5 <split [380/94]> Fold5 <tibble [64 × 7]> <tibble [0 × 3]>\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Selecting best models \nshow_best(dec_tree_rs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric  .estimator  mean     n std_err\n            <dbl>      <int> <int> <chr>    <chr>      <dbl> <int>   <dbl>\n1    0.0000000001         10    14 accuracy binary     0.675     5  0.0177\n2    0.0000001            10    14 accuracy binary     0.675     5  0.0177\n3    0.0001               10    14 accuracy binary     0.675     5  0.0177\n4    0.0000000001         15    14 accuracy binary     0.675     5  0.0177\n5    0.0000001            15    14 accuracy binary     0.675     5  0.0177\n# ℹ 1 more variable: .config <chr>\n```\n:::\n\n```{.r .cell-code}\nselect_best(dec_tree_rs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config              \n            <dbl>      <int> <int> <chr>                \n1    0.0000000001         10    14 Preprocessor1_Model25\n```\n:::\n\n```{.r .cell-code}\n# Finalizing our model \nfinal_dec_tree <- finalize_model(dec_tree_spec_tune,\n                                 select_best(dec_tree_rs))\n\nfinal_dec_tree_fit <- last_fit(final_dec_tree,\n                               as.factor(listener_id) ~.,\n                               audio_split)\n\n# Outputting Metrics \nfinal_dec_tree_fit$.predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n# A tibble: 159 × 6\n   .pred_Erica .pred_Kiran  .row .pred_class `as.factor(listener_id)` .config   \n         <dbl>       <dbl> <int> <fct>       <fct>                    <chr>     \n 1      0.114       0.886      1 Kiran       Kiran                    Preproces…\n 2      0.875       0.125      2 Erica       Kiran                    Preproces…\n 3      0.667       0.333      5 Erica       Kiran                    Preproces…\n 4      0.0732      0.927     10 Kiran       Kiran                    Preproces…\n 5      0.0732      0.927     11 Kiran       Kiran                    Preproces…\n 6      0.0357      0.964     30 Kiran       Kiran                    Preproces…\n 7      0.455       0.545     31 Kiran       Kiran                    Preproces…\n 8      0.952       0.0476    34 Erica       Kiran                    Preproces…\n 9      0.114       0.886     35 Kiran       Kiran                    Preproces…\n10      0.778       0.222     41 Erica       Kiran                    Preproces…\n# ℹ 149 more rows\n```\n:::\n\n```{.r .cell-code}\ndec_tree_metrics <- final_dec_tree_fit %>% \n  collect_metrics()\n\ndec_tree_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.648 Preprocessor1_Model1\n2 roc_auc  binary         0.689 Preprocessor1_Model1\n```\n:::\n:::\n\n\nThen validate and compare the performance of the models I  made\n\n\n   \n  \n**MODEL #3: Random Forest **\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define validating set \nvalidation_set <- validation_split(audio_train, \n                                   strata = listener_id, \n                                   prop = 0.70)\n\n# random forest spec \nrand_forest_spec <-\n  rand_forest(mtry = tune(),\n              min_n = tune(),\n              trees = 1000)  %>%  \n  set_engine(\"ranger\") %>% \n  set_mode(\"classification\")\n\n# random forest workflow\nrand_forest_workflow <- workflow() %>%  \n  add_recipe(music_rec) %>%  \n  add_model(rand_forest_spec)\n\n# buuild in parallel \ndoParallel::registerDoParallel()\n\nrand_forest_res <- \n  rand_forest_workflow %>%  \n  tune_grid(validation_set,\n            grid = 25,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(accuracy))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\ni Creating pre-processing data to finalize unknown parameter: mtry\n```\n:::\n\n```{.r .cell-code}\n## model metrics\nrand_forest_res %>% collect_metrics()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 25 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config              \n   <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n 1     9    30 accuracy binary     0.734     1      NA Preprocessor1_Model01\n 2     3    35 accuracy binary     0.748     1      NA Preprocessor1_Model02\n 3     4    19 accuracy binary     0.748     1      NA Preprocessor1_Model03\n 4     5    14 accuracy binary     0.741     1      NA Preprocessor1_Model04\n 5     7     7 accuracy binary     0.762     1      NA Preprocessor1_Model05\n 6     9    28 accuracy binary     0.755     1      NA Preprocessor1_Model06\n 7     3     9 accuracy binary     0.734     1      NA Preprocessor1_Model07\n 8     8    18 accuracy binary     0.741     1      NA Preprocessor1_Model08\n 9    12    31 accuracy binary     0.762     1      NA Preprocessor1_Model09\n10    11     3 accuracy binary     0.741     1      NA Preprocessor1_Model10\n# ℹ 15 more rows\n```\n:::\n\n```{.r .cell-code}\n# find best accuracy metric \nrand_forest_res %>%  \n  show_best(metric = \"accuracy\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 × 8\n   mtry min_n .metric  .estimator  mean     n std_err .config              \n  <int> <int> <chr>    <chr>      <dbl> <int>   <dbl> <chr>                \n1    14    38 accuracy binary     0.769     1      NA Preprocessor1_Model19\n2     7     7 accuracy binary     0.762     1      NA Preprocessor1_Model05\n3    12    31 accuracy binary     0.762     1      NA Preprocessor1_Model09\n4    10    15 accuracy binary     0.762     1      NA Preprocessor1_Model13\n5    13     6 accuracy binary     0.762     1      NA Preprocessor1_Model25\n```\n:::\n\n```{.r .cell-code}\n# plot \nautoplot(rand_forest_res)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# choose best random forest model \nbest_rand_forest <- select_best(rand_forest_res, \"accuracy\")\nbest_rand_forest\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n   mtry min_n .config              \n  <int> <int> <chr>                \n1    14    38 Preprocessor1_Model19\n```\n:::\n\n```{.r .cell-code}\n# output preds\nrand_forest_res %>%   \n  collect_predictions()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3,575 × 7\n   id         .pred_class  .row  mtry min_n listener_id .config              \n   <chr>      <fct>       <int> <int> <int> <fct>       <chr>                \n 1 validation Kiran           5     9    30 Kiran       Preprocessor1_Model01\n 2 validation Erica           6     9    30 Erica       Preprocessor1_Model01\n 3 validation Kiran           7     9    30 Kiran       Preprocessor1_Model01\n 4 validation Erica          13     9    30 Kiran       Preprocessor1_Model01\n 5 validation Kiran          14     9    30 Kiran       Preprocessor1_Model01\n 6 validation Erica          21     9    30 Erica       Preprocessor1_Model01\n 7 validation Kiran          24     9    30 Erica       Preprocessor1_Model01\n 8 validation Kiran          27     9    30 Kiran       Preprocessor1_Model01\n 9 validation Erica          33     9    30 Erica       Preprocessor1_Model01\n10 validation Kiran          35     9    30 Kiran       Preprocessor1_Model01\n# ℹ 3,565 more rows\n```\n:::\n\n```{.r .cell-code}\n# final model working in parallel \ndoParallel::registerDoParallel()\nlast_rand_forest_model <- \n  rand_forest(mtry = 2, min_n = 3, trees = 1000) %>%  \n  set_engine(\"ranger\", importance = \"impurity\") %>%  \n  set_mode(\"classification\")\n\n#Updating our workflow \nlast_rand_forest_workflow <- \n  rand_forest_workflow %>%  \n  update_model(last_rand_forest_model)\n\n# Updating our model fit \nlast_rand_forest_fit <- \n  last_rand_forest_workflow %>%  \n  last_fit(audio_split)\n\n# Outputting model metrics \nrand_forest_metrics <- last_rand_forest_fit %>%   \n  collect_metrics()\n\nrand_forest_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  <chr>    <chr>          <dbl> <chr>               \n1 accuracy binary         0.748 Preprocessor1_Model1\n2 roc_auc  binary         0.829 Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\n# find most important variables to our model \nlast_rand_forest_fit %>%  \n  extract_fit_parsnip() %>%  \n  vip::vip(num_features = 12) + \n  ggtitle(\"Order of Variable Importance in Random Forest Model\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-22-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# nearest neighbors metrics\nknn_accuracy <- knn_metrics$.estimate[1]\n\n# decision tree metrics\ndec_tree_accuracy <- dec_tree_metrics$.estimate[1]\n\n# random forest  metrics\nrandom_forest_accuracy <- rand_forest_metrics$.estimate[1]\n\nmodel_accuracy <- tribble(\n  ~\"model\", ~\"accuracy\",\n  \"K-Nearest Neighbor\", knn_accuracy,\n  \"Decision Tree\", dec_tree_accuracy,\n  \"Random Forest\", random_forest_accuracy\n)\n\n# Plotting bar chart to compare models accuracy \nggplot(data = model_accuracy, aes(x = model,\n                                  y = accuracy)) +\n  geom_col(fill = c(\"red\",\"purple\",\"blue\")) +\n  theme_minimal() +\n  labs(title = \"Comparison of Model Accuracy for Spotify Data\",\n       x = \"Model\",\n       y = \"Accuracy\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/Plotting and Charting Accuracy-1.png){width=672}\n:::\n:::\n\n\n**This analysis suggests that the Bagging model has the best accuracy at 69.2% and the worst model is the Decision model with 64.8% accuracy.**",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}