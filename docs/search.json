[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "AboutHobbiesTravel\n\n\n\nMore about me!\n\nI recently earned a Master of Environmental Data Science degree from the Bren School of Environmental Science & Management at the University of California, Santa Barbara. I built two interactive web applications for a client with a team of three other graduate students. These applications were developed in response to our client’s needs of improving their data management system and creating a way to visualize the data they collected. Their work directly impacts Marine Protected Areas by identifiying gaps in enforcement of these regions, collecting data on categories related to enforcement, and working with partners to improve the protection of these habitats. Our applications allowed them to reduce errors in data compilation, save time by giving them an automated data entry system, and a tool to visualize trends in their data. In addition to this capstone project, I learned about geospatial analysis, data management, and open data science through coursework and connections at the Bren School and the National Center for Ecological Analysis and Synthesis.\nI graduated from the University of California, Santa Cruz in 2022 with a Bachelor’s degree in Environmental Sciences, where I focused on climate change and oceanography. While there, I was a Santa Cruz Climate Action Program Intern who collected data on potential community responses to coastal erosion. Surveying and collecting data as an intern, doing field work through courses at UCSC, and working with climate models in my undergraduate capstone project inspired my curiosity for data as a tool of scientific communication. Interest in undergraduate environmental data analysis and climate modeling drove me to seek out the Bren School for Environmental Science & Management to deepen my understanding of data driven solutions to environmental problems. I hopes to use my degree to contribute to sustainable technology and to pursue a career in data science.\n\n\n\nSpending time by the ocean\nHiking and camping with my pups\nMusic (going to concerts or playing it!)\nScuba diving\nExploring new restaurants\n\n\n\n\nI love to travel, and have been fortunate to have had the opportunity to. I look forward to travelling more in my life since my travels so far have given me the chance to learn more about different perspectives and cultures. Here where I’ve been so far!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kiran Favre",
    "section": "",
    "text": "Hi!\nI’m Kiran, and I a recent graduate from the Bren School for Environmental Science & Management at the University of California, Santa Barbara. I earned my Master of Environmental Data Science, and worked on a capstone project with a group to create an application that automates our client’s data entry workflow, improving their data management system, and a developed a data visualization application. I am interested in climate change, science communication, and green technology.\n\n\nEducation\n\nMaster’s of Environmental Data Science, 2023\n\n\nBren School for Environmental Science & Management, University of California, Santa Barbara\n\n\nBS in Environmental Sciences, 2022\n\n\nUniversity of California, Santa Cruz"
  },
  {
    "objectID": "posts/2022-12-03_geospatial/index.html",
    "href": "posts/2022-12-03_geospatial/index.html",
    "title": "Determining Suitable Oyster Habitats in West Coast EEZ’s",
    "section": "",
    "text": "This blog post is from my Geospatial Analysis and Remote Sensing course as a part of my master’s program at the Bren School for Environmental Science & Management. I will be determining which Exclusive Economic Zones (EEZ’s) on the West Coast of the United States are the most suitable for developing marine aquaculture for many species of oysters. Then, I make a function to map any species’ suitable habitats within West Coast EEZs by allowing for user inputs of species’ suitable habitats based on sea surface temperatures and depth."
  },
  {
    "objectID": "posts/2022-12-03_geospatial/index.html#background",
    "href": "posts/2022-12-03_geospatial/index.html#background",
    "title": "Determining Suitable Oyster Habitats in West Coast EEZ’s",
    "section": "",
    "text": "This blog post is from my Geospatial Analysis and Remote Sensing course as a part of my master’s program at the Bren School for Environmental Science & Management. I will be determining which Exclusive Economic Zones (EEZ’s) on the West Coast of the United States are the most suitable for developing marine aquaculture for many species of oysters. Then, I make a function to map any species’ suitable habitats within West Coast EEZs by allowing for user inputs of species’ suitable habitats based on sea surface temperatures and depth."
  },
  {
    "objectID": "posts/2022-12-03_geospatial/index.html#data",
    "href": "posts/2022-12-03_geospatial/index.html#data",
    "title": "Determining Suitable Oyster Habitats in West Coast EEZ’s",
    "section": "Data",
    "text": "Data\n\nSea Surface Temperature Data\nI will use average annual sea surface temperature (SST) from the years 2008 to 2012 to characterize the average sea surface temperature within the region. The data we are working with was originally generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\n\nBathymetry Data\nTo characterize the depth of the ocean, I will use the General Bathymetric Chart of the Oceans (GEBCO).[^3]\n\n\nExclusive Economic Zones Data\nWe will be designating maritime boundaries using Exclusive Economic Zones off of the west coast of US from Marineregions.org."
  },
  {
    "objectID": "posts/2022-12-03_geospatial/index.html#process",
    "href": "posts/2022-12-03_geospatial/index.html#process",
    "title": "Determining Suitable Oyster Habitats in West Coast EEZ’s",
    "section": "Process",
    "text": "Process\nI will start with loading the necessary data listed above and validate that all the data has the same coordinate reference system.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(terra)\nlibrary(tmap)\nlibrary(tmaptools)\n\n\n#file path set by Rproj\nsetwd(here())\n\n\n\n\nShow code\n#read in West coast EEZ shape file w terra \nwc_EEZ_regions &lt;- st_read(here(\"posts\",\n                               \"2022-12-03_geospatial\",\n                               \"data\",\n                               \"wc_regions_clean.shp\"))\n\n\nReading layer `wc_regions_clean' from data source \n  `C:\\Users\\kiran\\Documents\\MEDS 2022-2023\\kiranfavre.github.io\\posts\\2022-12-03_geospatial\\data\\wc_regions_clean.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n\n\nShow code\n#read in SST rasters\navg_sst_2008 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2008.tif\"))\navg_sst_2009 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2009.tif\"))\navg_sst_2010 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2010.tif\"))\navg_sst_2011 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2011.tif\"))\navg_sst_2012 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2012.tif\"))\n\n#combine into raster stack \nall_sst &lt;- c(avg_sst_2008,\n             avg_sst_2009,\n             avg_sst_2010,\n             avg_sst_2011,\n             avg_sst_2012)\n\n#read in bathymetry raster\ndepth &lt;- rast(here(\"posts\",\n                    \"2022-12-03_geospatial\",\n                    \"data\",\n                    \"depth.tif\"))\n\n\nChecking if the coordinate reference systems are the same. The sea surface temperature data needs to be reprojected to match the coordinate reference systems of the EEZ and depth data.\n\n\nShow code\nst_crs(wc_EEZ_regions) # crs EPSG:4326\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nShow code\nst_crs(all_sst)  #crs EPSG:9122\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"unknown\",\n        ELLIPSOID[\"WGS84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433,\n            ID[\"EPSG\",9122]]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]]]\n\n\nShow code\nst_crs(depth)  #crs EPSG 4326\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nShow code\n#re project using the terra way\nall_sst_reproj &lt;- project(all_sst,\n                          wc_EEZ_regions)\n\n\nWarning: [project,SpatRaster] argument y (the crs) should be a character value"
  },
  {
    "objectID": "posts/2022-12-03_geospatial/index.html#process-data",
    "href": "posts/2022-12-03_geospatial/index.html#process-data",
    "title": "Determining Suitable Oyster Habitats in West Coast EEZ’s",
    "section": "Process data",
    "text": "Process data\nNext, I need process the SST and depth data so that they can be combined, I will resample to match the SST data using the nearest neighbor approach since the extents, resolutions, and positions of the depth and SST are different.\n\n\nShow code\n#find the mean SST from 2008-2012\nmean_sst &lt;- mean(all_sst_reproj)\n\n#convert sst from K to C\nmean_sst_c &lt;- mean_sst - 273.15\n\n\n\n\nShow code\n#crop depth rast to match the extent of the SST rast\ndepth_cropped &lt;- crop(depth,mean_sst_c)\n\n\n#using nearest neighbor approach to resample\ndepth_resampled &lt;- resample(depth_cropped,\n                mean_sst_c,\n                method = \"near\")\n\n#stack to check if they have the same resolution\nresolution_test &lt;- c(depth_resampled,\n                       all_sst_reproj)\n#they stacked properly, indicating that they both have same resolutions, extent, and crs\n\n\n\nFind suitable locations\nTo determine suitable locations for marine aquaculture, I need to look for locations that are suitable in terms of both SST and depth.\n\n\nShow code\n#oyster happy spots\n#sea surface temperature: 11-30&deg;C\\\n#depth: 0-70 meters below sea level\n\n#reclassify sst into suitable locations for oysters\n#reclassification matrix for suitable range of sst\n\nrcl_sst &lt;- matrix(c(-Inf, 11, NA, 11, 30,\n                    1,30, Inf, NA), \n              ncol = 3, byrow = TRUE) #anything outside of range is NA\n#reclassifying raster using a reclassification matrix\nsst_suitable_locs &lt;- classify(mean_sst_c,\n                     rcl = rcl_sst, \n                     include.lowest = TRUE)\n\n\n\n\nShow code\n#reclassify depth into suitable locations for oysters\n#reclassification matrix for suitable range of depths\n\nrcl_depth &lt;- matrix(c(-Inf, -70, NA,\n                      -70, 0, 1,\n                      0, Inf, NA),\n                    ncol = 3, byrow = TRUE)\n\n#reclassifying raster using a reclassification matrix\ndepth_suitable_locs &lt;- classify(depth_resampled,\n                     rcl = rcl_depth,\n                     include.lowest = TRUE)\n\n\n\n\nShow code\n#define function\nmult_fun &lt;- function(x, y) {\n  return(x*y)}\n\n\n#find locations that satisfy both conditions\noyst_suitable_habitat &lt;- lapp(c(sst_suitable_locs, depth_suitable_locs), mult_fun)\n\n\n\n\nDetermining the most suitable EEZ\nI want to determine the total suitable area within each EEZ so I can rank zones by priority. To do this, I will need to find the total area of suitable locations within each EEZ.\n\n\nShow code\n# making obj of individual cell size of areas that are suitable for oysters so we can find the size of each cell\ngrid_cell_size &lt;- cellSize(oyst_suitable_habitat,\n                           mask = TRUE, #keep values within the suitable habitat\n                           transform = TRUE,\n                           unit = \"km\")\n\nplot(grid_cell_size, main = \"Cell Size\")\n\n\n\n\n\n\n\nShow code\n# rasterize wc shape file to help find total suitable area\n# Rasterize(): Transfer values associated with 'object' type spatial data (points, lines, polygons) to raster cells.\n# wc_EEZ regions are the points, and we will transfer those values to the raster i set up earlier for suitable oyster habitats\nwc_rasterized &lt;- rasterize(wc_EEZ_regions,\n                           oyst_suitable_habitat,\n                           field = \"rgn\")\n\n\n\n\nShow code\n#make a mask of wc raster and suitable locs for oysters\n#goal is to have areas of suitable oyster habitat that also fall in w coast EEZ regions\nwc_mask &lt;- mask(wc_rasterized, oyst_suitable_habitat,\n                updatevalue = NA,\n                inverse = FALSE)\n\n\n\n\nShow code\n#find area of each suitable zones with zonal() in each of 5 regions of EEZ\nwc_suitable_area &lt;- zonal(grid_cell_size, wc_mask, na.rm = TRUE, sum)\n\n\n\n\nShow code\n#join data\nwc_suitable_EEZ &lt;- full_join(wc_EEZ_regions, wc_suitable_area, by = \"rgn\") |&gt; \n  mutate(suitable_area = area, #rename area to suitable area \n         percentage_suitable = (suitable_area/area_km2 * 100), #percentage of suitable area\n         .before = geometry)\n\nprint(wc_suitable_EEZ)\n\n\nSimple feature collection with 5 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n                  rgn rgn_key      area_m2 rgn_id  area_km2      area\n1              Oregon      OR 179994061293      1 179994.06 1074.2562\n2 Northern California    CA-N 164378809215      2 164378.81  178.0246\n3  Central California    CA-C 202738329147      3 202738.33 4069.5671\n4 Southern California    CA-S 206860777840      4 206860.78 3508.1870\n5          Washington      WA  66898309678      5  66898.31 2378.2758\n  suitable_area percentage_suitable                       geometry\n1     1074.2562           0.5968287 MULTIPOLYGON (((-123.4318 4...\n2      178.0246           0.1083014 MULTIPOLYGON (((-124.2102 4...\n3     4069.5671           2.0073003 MULTIPOLYGON (((-122.9928 3...\n4     3508.1870           1.6959169 MULTIPOLYGON (((-120.6505 3...\n5     2378.2758           3.5550611 MULTIPOLYGON (((-122.7675 4...\n\n\n\n\nVisualize results\n\n\nShow code\n#set to interactive mode\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing\n\n\n\n\nShow code\n#map for total suitable area for oysters by region\n\noyst_area_map &lt;- tm_basemap(\"Stamen.Terrain\") +\n  tm_shape(wc_suitable_EEZ) +\n  tm_polygons(col = 'area',\n              palette = 'Oranges',\n              alpha = 0.75,\n              border.col = 'black',\n              title = \"Total Suitable Area\") +\n  tm_text(\"rgn\", size = 0.54) +\n  tm_scale_bar(position = c(\"left\", \"right\"))\n \noyst_area_map\n\n\n\n\n\n\n\n\n\nShow code\n#map for percent suitable area by region\noyst_percentage_map &lt;- tm_basemap(\"Stamen.Terrain\") +\n  tm_shape(wc_suitable_EEZ) +\n  tm_polygons(col = 'percentage_suitable',\n              palette = 'Purples',\n              alpha = 0.75,\n              border.col = 'black',\n              title = \"Percentage of Suitable Area\") +\n  tm_text(\"rgn\", size = 0.54) +\n  tm_scale_bar(position = c(\"left\", \"right\"))\n\noyst_percentage_map\n\n\n\n\n\n\n\n\n\nExpanding this workflow to other species\n\n\nShow code\nfind_suitable_locs &lt;- function(SST_low, SST_high, depth_low, depth_high, spp_name) {\n\n  #read in West coast EEZ shape file w terra \nwc_EEZ_regions &lt;- st_read(here(\"posts\",\n                               \"2022-12-03_geospatial\",\n                               \"data\",\n                               \"wc_regions_clean.shp\"))\n\n\n#read in SST rasters\navg_sst_2008 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2008.tif\"))\navg_sst_2009 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2009.tif\"))\navg_sst_2010 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2010.tif\"))\navg_sst_2011 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2011.tif\"))\navg_sst_2012 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2012.tif\"))\n\n#combine into raster stack \nall_sst &lt;- c(avg_sst_2008,\n             avg_sst_2009,\n             avg_sst_2010,\n             avg_sst_2011,\n             avg_sst_2012)\n\n#read in bathymetry raster\ndepth &lt;- rast(here(\"posts\",\n                    \"2022-12-03_geospatial\",\n                    \"data\",\n                    \"depth.tif\"))\n  \n  #re project using the terra way\n  all_sst_reproj &lt;- project(all_sst, depth)\n  \n  #find the mean SST from 2008-2012\n  mean_sst &lt;- mean(all_sst_reproj)\n  \n  #convert sst from K to C\n  mean_sst_c &lt;- mean_sst - 273.15\n  \n  #crop depth rast to match the extent of the SST rast\n  depth_cropped &lt;- crop(depth,mean_sst_c)\n  \n  #using nearest neighbor approach to resample\n  depth_resampled &lt;- resample(depth_cropped,\n                              mean_sst_c,\n                              method = \"near\")\n  \n  #make reclassification matrix\n  rcl_sst &lt;- matrix(c(-Inf, SST_low, NA, SST_low, SST_high,\n                      1, SST_high, Inf, NA), \n                    ncol = 3, byrow = TRUE) \n  \n  \n  #reclassifying sst raster using a reclassification matrix\n  sst_suitable_locs &lt;- classify(mean_sst_c,\n                                rcl = rcl_sst, \n                                include.lowest = TRUE)\n  \n  #reclassifying depth raster using a reclassification matrix\n  rcl_depth &lt;- matrix(c(-Inf, depth_low, NA,\n                        depth_low, depth_high, 1,\n                        depth_high, Inf, NA),\n                      ncol = 3, byrow = TRUE)\n  \n  \n  depth_suitable_locs &lt;- classify(depth_resampled,\n                                  rcl = rcl_depth,\n                                  include.lowest = TRUE)\n  \n  #define function to multiply layers\n  mult_fun &lt;- function(x, y) {\n    return(x*y)}\n  \n  #find locations that satisfy both conditions\n  oyst_suitable_habitat &lt;- lapp(c(sst_suitable_locs, depth_suitable_locs), mult_fun)\n  \n  \n  #find grid cell size\n  grid_cell_size &lt;- cellSize(oyst_suitable_habitat,\n                             mask = TRUE, \n                             transform = TRUE,\n                             unit = \"km\")\n  \n  wc_rasterized &lt;- rasterize(wc_EEZ_regions,\n                             oyst_suitable_habitat,\n                             field = \"rgn\")\n  \n  wc_mask &lt;- mask(wc_rasterized, oyst_suitable_habitat,\n                  updatevalue = NA,\n                  inverse = FALSE)\n  \n  wc_suitable_area &lt;- zonal(grid_cell_size, wc_mask, na.rm = TRUE, sum)\n  \n  wc_suitable_EEZ &lt;- full_join(wc_EEZ_regions, wc_suitable_area, by = \"rgn\") |&gt; \n    mutate(suitable_area = area, #rename area to suitable area \n           percentage_suitable = (suitable_area/area_km2 * 100), #percentage of suitable area\n           .before = geometry)\n  \n  ##make maps \n  \n  #set to interactive viewing \n  tmap_mode(\"view\")\n  \n  #total area\n  viz_total_area &lt;- tm_basemap(\"Stamen.Terrain\")+\n   tm_shape(wc_suitable_EEZ) +\n    tm_polygons(col = 'area',\n               palette = 'Oranges',\n                alpha = 0.75,\n                border.col = 'black',\n               title = paste0(\"Total Suitable Area for \", spp_name ,\" in West Coast EEZs\")) +\n    tm_text(\"rgn\", size = 0.54) \n \n   viz_total_area\n  \n  #percentage suitable\n  viz_percentage &lt;- tm_basemap(\"Stamen.Terrain\") +\n    tm_shape(wc_suitable_EEZ) +\n    tm_polygons(col = 'percentage_suitable',\n                palette = 'Purples',\n                alpha = 0.75,\n                border.col = 'black',\n                title = paste0(\"Percentage of Suitable Area for \", spp_name ,\" in West Coast EEZs\")) +\n    tm_text(\"rgn\", size = 0.54) \n  \n  viz_percentage\n  \n  tmap_arrange(viz_total_area, viz_percentage)\n}\n\n\nI chose the Red Abalone, Haliotis rufescens. https://www.sealifebase.ca/summary/Haliotis-rufescens.html\n\n\nShow code\nabalone_habitats &lt;- find_suitable_locs(8, 18, 0, 24, \"Red Abalone\")\n\n\nReading layer `wc_regions_clean' from data source \n  `C:\\Users\\kiran\\Documents\\MEDS 2022-2023\\kiranfavre.github.io\\posts\\2022-12-03_geospatial\\data\\wc_regions_clean.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\ntmap mode set to interactive viewing\n\n\nShow code\nabalone_habitats"
  },
  {
    "objectID": "posts/2022-12-08_EDS222_final/index.html",
    "href": "posts/2022-12-08_EDS222_final/index.html",
    "title": "Analysis of California Coastal Erosion",
    "section": "",
    "text": "Coastal erosion is a natural process of sediment removal that occurs due to storms and waves. Climate change has caused sea levels to rise, increasing concerns for possible effects on the coastlines. While this is a natural process, some coastal communities could be facing more coastal erosion than others considering the level of development along the coast. As of 2015, approximately 68% (26.3 million people) of California residents live in a coastal area (within approximately half a mile of the mean high water line) and have likely witnessed some level of coastal erosion. California’s coastlines are also susceptible to erosion due to its tectonic activity and exposure to extreme weather events such as ENSO.\nEfforts to mitigate and adapt to erosion along the coastline have been made as concerns have rised for the displacement of communities and general safety. The Department of Boating & Waterways is responsible for responding to coastal erosion in California. In areas in need of attention, such as where roads or homes collapse off of cliff tops or beach front properties being flooded, there are different levels of responses depending on the intensity of erosion. Sand replenishment and seawall or jetty installation are currently used to maintain the current infrastructure along the coastlines. This type of analysis can help in evaluating areas that need different solutions depending on the level of shoreline change."
  },
  {
    "objectID": "posts/2022-12-08_EDS222_final/index.html#motivation",
    "href": "posts/2022-12-08_EDS222_final/index.html#motivation",
    "title": "Analysis of California Coastal Erosion",
    "section": "",
    "text": "Coastal erosion is a natural process of sediment removal that occurs due to storms and waves. Climate change has caused sea levels to rise, increasing concerns for possible effects on the coastlines. While this is a natural process, some coastal communities could be facing more coastal erosion than others considering the level of development along the coast. As of 2015, approximately 68% (26.3 million people) of California residents live in a coastal area (within approximately half a mile of the mean high water line) and have likely witnessed some level of coastal erosion. California’s coastlines are also susceptible to erosion due to its tectonic activity and exposure to extreme weather events such as ENSO.\nEfforts to mitigate and adapt to erosion along the coastline have been made as concerns have rised for the displacement of communities and general safety. The Department of Boating & Waterways is responsible for responding to coastal erosion in California. In areas in need of attention, such as where roads or homes collapse off of cliff tops or beach front properties being flooded, there are different levels of responses depending on the intensity of erosion. Sand replenishment and seawall or jetty installation are currently used to maintain the current infrastructure along the coastlines. This type of analysis can help in evaluating areas that need different solutions depending on the level of shoreline change."
  },
  {
    "objectID": "posts/2022-12-08_EDS222_final/index.html#data",
    "href": "posts/2022-12-08_EDS222_final/index.html#data",
    "title": "Analysis of California Coastal Erosion",
    "section": "DATA",
    "text": "DATA\nThe data used for this analysis was produced by the Coastal and Marine Hazards and Resources Program with the United States Geological Survey. This data is publicly available through the USGS ScienceBase-Catalog in a data release from the Pacific Coastal and Marine Science Center. The spatial range spans the California coast, given in degrees latitude and longitude and Universal Transverse Mercator (UTM). California is broken into three regions: Northern, Central, and Southern. \nThe dataset I used includes shoreline change data along the coast of California from 2015-2016. Net shoreline movement, in meters, serves as the metric of shoreline change and what I will be observing. This data was compiled to observe changes in the shoreline in response to extreme weather, as it was collected during an El Niño event (2015 marking ‘before an El Niño’ and 2016 marking ‘after an El Niño’). Net shoreline movement was ’calculated at a transect spacing of 50 meters as a proxy for sandy shoreline change throughout the El Nino winter season.\nA limitation in this data is that I am using observations derived from USGS’s DSAS and Light Detection and Ranging (LiDAR) digital elevation models (DEMs), where these may have used their own assumptions that could introduce bias into this data set. Another limitation is the unequal distribution of observations per region: Southern California has many more observations than Northern or Central California, so an analysis on Southern California may be more representative of the population than the other regions studied.\nTo explore the data, I made histograms to understand the distribution of net shoreline movement in each region of California. The data is approximately normally distributed in all of California, with the most normal distribution in Southern California.\n\n\nShow code\n#read in 2015 data\nCA_2015_shoreline &lt;- st_read(file.path(rootdir,\n                       \"data\",\n                       \"CA_shoreline_changes\",\n                       \"2015_2016_shoreline_changes\",\n                       \"CA_2015_2016_shoreline_change.shp\"))\n\n\n\n\nShow code\n#has only norcal observations\nnorcal_NSM &lt;- CA_2015_shoreline |&gt;\n  filter(Region == \"n\") |&gt; \n  dplyr::select(NSM, Lat, Long) |&gt; \n  na.omit()\n\n\n#has only cencal observations\ncencal_NSM &lt;- (CA_2015_shoreline) |&gt;\n  filter(Region == \"c\") |&gt; \n  dplyr::select(NSM, Lat, Long) |&gt; \n  na.omit()\n\n\n#has only socal observations\nsocal_NSM &lt;- (CA_2015_shoreline) |&gt; \n  filter(Region == \"s\") |&gt; \n  dplyr::select(NSM, Lat, Long) |&gt; \n  na.omit()\n\n\n\n\nShow code\n#Norcal\nnorcal_NSM_hist_n &lt;- ggplot(norcal_NSM, aes(x = NSM)) +\n  geom_histogram(bins = 50,\n                 fill = \"skyblue\") +\n  labs(x = \"Net Shoreline Movement (m)\",\n       y = \"Count\",\n       title = \"Distribution of Net Shoreline Movement in Northern California\",\n       ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size=10), \n        axis.title = element_text(size = 7))  \n\nnorcal_NSM_hist_n\n\n\n\n\n\n\n\nShow code\n#Cencal\ncencal_NSM_hist &lt;- ggplot(cencal_NSM, aes(x = NSM)) +\n  geom_histogram(bins = 50,\n                 fill = \"skyblue\") +\n  labs(x = \"Net Shoreline Movement (m)\",\n       y = \"Count\",\n       title = \"Distribution of Net Shoreline Movement in Central California\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=10),\n        axis.title = element_text(size = 7)) \n\ncencal_NSM_hist\n\n\n\n\n\n\n\nShow code\n#Socalsocal_NSM_hist\nsocal_NSM_hist &lt;- ggplot(socal_NSM, aes(x = NSM)) +\n  geom_histogram(bins = 50,\n                 fill = \"skyblue\") +\n  labs(x = \"Net Shoreline Movement (m)\",\n       y = \"Count\",\n       title = \"Distribution of Net Shoreline Movement in Southern California\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=10), \n        axis.title = element_text(size = 7))  \n\nsocal_NSM_hist"
  },
  {
    "objectID": "posts/2022-12-08_EDS222_final/index.html#analysis",
    "href": "posts/2022-12-08_EDS222_final/index.html#analysis",
    "title": "Analysis of California Coastal Erosion",
    "section": "ANALYSIS",
    "text": "ANALYSIS\nTo understand if there is a relationship between location along the coast and net shoreline movement, I chose to analyze net shoreline movement by the three study regions: Northern, Central, and Southern California. I will begin with hypothesis testing to determine whether the mean net shoreline movement is the same in each region. The null hypothesis is that each region has the same average net shoreline movement. The alternative hypothesis is that each region does not have the same average net shoreline movement.\n\\[\nH_0:\\mu_{Norcal} - \\mu_{Cencal} - \\mu_{Socal} = 0,\nH_1: \\mu_{Norcal} - \\mu_{Cencal} - \\mu_{Socal} \\neq 0\n\\]\nI calculated the average of each region, and found that the average net shoreline movement is not the same across regions. The average net shoreline movement was least in Central California and most in Southern California. A positive value for net shoreline movement indicates an extending coastline, where a negative value indicates erosion, so Central California is estimated to be experiencing the most erosion while Southern California is estimated to be experiencing the least.\n\n\nShow code\nnorcal_mean_NSM &lt;- mean(norcal_NSM$NSM)\n\ncencal_mean_NSM &lt;- mean(cencal_NSM$NSM)\n\nsocal_mean_NSM &lt;- mean(socal_NSM$NSM)\n\n#make df to plot\nRegion &lt;- c(\"Northern CA\",\n            \"Central CA\",\n            \"Southern CA\")\nMean_NSM &lt;- c(\"-25.512\", \"-45.702\", \"-9.743\")\n\ndf &lt;- data.frame(Region, Mean_NSM)\nprint(df)\n\n\n       Region Mean_NSM\n1 Northern CA  -25.512\n2  Central CA  -45.702\n3 Southern CA   -9.743\n\n\nNext, I ran a multiple linear regression of the impact of location on net shoreline movement. Using the equation:\n\\[NSM =\\beta_{0}+\\beta_{1} \\cdot Longitude +\\beta_{2} \\cdot \\text Latitude+\\varepsilon_i\\] I broke the data into the regions, and used this model for each region. Using RStudio, I was able to calculate the values for the coefficients, listed below."
  },
  {
    "objectID": "posts/2022-12-08_EDS222_final/index.html#results",
    "href": "posts/2022-12-08_EDS222_final/index.html#results",
    "title": "Analysis of California Coastal Erosion",
    "section": "RESULTS",
    "text": "RESULTS\nNorthern California\n\n\nShow code\n#make model \nmod_n &lt;- lm(NSM ~ Long + Lat,\n            data = norcal_NSM)\nsummary(mod_n)\n\n#plot longitude vs NSM\nnorcal_mod_lon &lt;- ggplot(data = norcal_NSM,\n       aes(x = Long,\n             y = NSM,\n             color = NSM)) +\n  geom_point() +\n  geom_jitter() +\n  geom_line(data = augment(mod_n), aes(y = .fitted,),\n            color = \"blue\") +\n  labs(x = \"Long\",\n       y = \"NSM\",\n       main = \"Multiple linear regression of NSM ~ Long + Lat\") +\n  theme_minimal() +\n  scale_color_gradient(low = \"purple\", high = \"orange\") +\n  theme(panel.grid = element_blank(),\n        axis.text.x= element_text(size=5.5),\n        axis.title = element_text(size = 8),\n        legend.key.size = unit(0.4, 'cm'),\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 7))\n\n#plot latitude vs NSM \nnorcal_mod_lat &lt;- ggplot(data = norcal_NSM, aes(x = Lat,\n                                        y = NSM,\n                                        color = NSM)) +\n  geom_point() +\n  geom_jitter() +\n  geom_line(data = augment(mod_n), aes(y = .fitted,),\n            color = \"blue\") +\n  labs(x = \"Lat\",\n       y = \"NSM\",\n       main = \"Multiple linear regression of NSM ~ Long + Lat\") +\n  theme_minimal() +\n  scale_color_gradient(low = \"purple\", high = \"orange\") +\n  theme(panel.grid = element_blank(),\n        axis.text.x= element_text(size=5.5),\n        axis.title = element_text(size = 8),\n        legend.key.size = unit(0.4, 'cm'),\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 7))\n\nggarrange(norcal_mod_lon, norcal_mod_lat)\n\n\n\n\n\nCentral California:\n\n\nShow code\n#make model for cencal \nmod_c &lt;- lm(NSM ~ Long + Lat,\n            data = cencal_NSM)\nsummary(mod_c)\n#plot mod on Long vs NSM\ncencal_mod_lon &lt;- ggplot(data = cencal_NSM, aes(x = Long,\n                                        y = NSM,\n                                        color = NSM)) +\n  geom_point() +\n  geom_jitter() +\n  geom_line(data = augment(mod_c), aes(y = .fitted,),\n            color = \"blue\") +\n  labs(x = \"Long\",\n       y = \"NSM\") +\n  theme_minimal() +\n  scale_color_gradient(low = \"purple\", high = \"orange\") +\n  theme(panel.grid = element_blank(),\n        axis.text.x= element_text(size=5.5),\n        axis.title = element_text(size = 8),\n        legend.key.size = unit(0.4, 'cm'),\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 7))\n\n#model on latitude vs NSM\ncencal_mod_lat &lt;- ggplot(data = cencal_NSM, aes(x = Lat,\n                                        y = NSM,\n                                        color = NSM)) +\n  geom_point() +\n  geom_jitter() +\n  geom_line(data = augment(mod_c), aes(y = .fitted,),\n            color = \"blue\") +\n  labs(x = \"Lat\",\n       y = \"NSM\",\n       main = \"Multiple linear regression of NSM ~ Long + Lat\") +\n  theme_minimal() +\n  scale_color_gradient(low = \"purple\", high = \"orange\") +\n  theme(panel.grid = element_blank(),\n        axis.text.x= element_text(size=5.5),\n        axis.title = element_text(size = 8),\n        legend.key.size = unit(0.4, 'cm'),\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 7))\n\n#plot side by side\nggarrange(cencal_mod_lon, cencal_mod_lat)\n\n\n\n\n\nSouthern California:\n\n\nShow code\n#make model for southern CA\nmod_s &lt;- glm(NSM ~ Long + Lat,\n            data = socal_NSM)\nsummary(mod_s)\n\n#plot model on longitude vs NSM\nsocal_mod_lon &lt;- ggplot(data = socal_NSM, aes(x = Long,\n                                        y = NSM,\n                                        color = NSM)) +\n  geom_point() +\n  geom_jitter() +\n  geom_line(data = augment(mod_s), aes(y = .fitted,),\n            color = \"blue\") +\n  labs(x = \"Longitude (degrees)\",\n       y = \"NSM\") +\n  theme_minimal() +\n  scale_color_gradient(low = \"purple\", high = \"orange\") +\n  theme(panel.grid = element_blank(),\n        axis.text.x=element_text(size=5.5),\n        axis.title = element_text(size = 8),\n        legend.key.size = unit(0.4, 'cm'),\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 7))\n\n#model on lat vs NSM\nsocal_mod_lat &lt;- ggplot(data = socal_NSM, aes(x = Lat,\n                                        y = NSM,\n                                        color = NSM)) +\n  geom_point() +\n  geom_jitter() +\n  geom_line(data = augment(mod_s), aes(y = .fitted,),\n            color = \"blue\") +\n  labs(x = \"Latitude(degrees)\",\n       y = \"NSM\") +\n  theme_minimal() +\n  scale_color_gradient(low = \"purple\", high = \"orange\") +\n  theme(panel.grid = element_blank(),\n        axis.text.x= element_text(size=5.5),\n        axis.title = element_text(size = 8),\n        legend.key.size = unit(0.4, 'cm'),\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 7)) \n\n#plot side by side\nggarrange(socal_mod_lon, socal_mod_lat)\n\n\n\n\n\nNorthern California Regression Model Summary:\n\n\nShow code\nmod_n &lt;- lm(NSM ~ Long + Lat,\n            data = norcal_NSM)\ntab_model(mod_n)\n\n\n\n\n\n \nNSM\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-1284.29\n-1838.13 – -730.45\n&lt;0.001\n\n\nLong\n-10.08\n-15.13 – -5.03\n&lt;0.001\n\n\nLat\n0.28\n-1.68 – 2.23\n0.782\n\n\nObservations\n1524\n\n\nR2 / R2 adjusted\n0.068 / 0.067\n\n\n\n\n\n\n\nCentral California Regression Model Summary:\n\n\nShow code\nmod_c &lt;- lm(NSM ~ Long + Lat,\n            data = cencal_NSM)\ntab_model(mod_c)\n\n\n\n\n\n \nNSM\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-13353.72\n-16584.15 – -10123.30\n&lt;0.001\n\n\nLong\n-134.26\n-167.45 – -101.07\n&lt;0.001\n\n\nLat\n-82.71\n-104.86 – -60.57\n&lt;0.001\n\n\nObservations\n1033\n\n\nR2 / R2 adjusted\n0.112 / 0.110\n\n\n\n\n\n\n\nSouthern California Regression Model Summary:\n\n\nShow code\nmod_s &lt;- lm(NSM ~ Long + Lat,\n            data = socal_NSM)\ntab_model(mod_s)\n\n\n\n\n\n \nNSM\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n254.24\n186.79 – 321.69\n&lt;0.001\n\n\nLong\n9.95\n8.88 – 11.01\n&lt;0.001\n\n\nLat\n27.11\n25.16 – 29.05\n&lt;0.001\n\n\nObservations\n5616\n\n\nR2 / R2 adjusted\n0.188 / 0.188"
  },
  {
    "objectID": "posts/2022-12-08_EDS222_final/index.html#discussion",
    "href": "posts/2022-12-08_EDS222_final/index.html#discussion",
    "title": "Analysis of California Coastal Erosion",
    "section": "DISCUSSION",
    "text": "DISCUSSION\nAs the results of the hypothesis test show, the average net shoreline movement in each region of California is not equal. Due to this, I reject the null hypothesis that the average net shoreline movement will be equal in across all regions in California.\nThese plots indicate that there is a relationship between location in California and net shoreline movement. For the most part, as longitude decreases (moving eastward), the predicted net shoreline movement decrease (coastal erosion). As latitude increases (moving northward), the predicted net shoreline movement increases (coastal extension). These models estimate that shorelines that are more southeastern are at risk for more coastal erosion.\nThe regression summary tables can further help in understanding the estimated relationship between location and net shoreline movement. The model predicts the following: in Northern California, about 6.8% of the variance in net shoreline movement is due to its latitude and longitude. In Central California, about 11.2% of the variance in net shoreline movement is due to its latitude and longitude. In Southern California, about 18.8% of the variance in net shoreline movement is due to its latitude and longitude. The coefficients represent the magnitude of effect of that variable on net shoreline movement. These differ greatly between each region, which could mean that each region faces different levels of environmental stressors causing differing levels of coastal erosion."
  },
  {
    "objectID": "posts/2022-12-08_EDS222_final/index.html#future-research",
    "href": "posts/2022-12-08_EDS222_final/index.html#future-research",
    "title": "Analysis of California Coastal Erosion",
    "section": "FUTURE RESEARCH",
    "text": "FUTURE RESEARCH\nMore monitoring of this coastline will be essential in forming helpful policy and adaptation strategies. With how dynamic the shoreline is in lieu of natural processes, in addition to anthropogenic influences, it is important to improve the understanding of how the coastline is changing. The effects of climate change are being understood as they happen, which is why improving monitoring is crucial in forming an analysis of coastal erosion.\nAlong with the data collection itself, the model used here could be improved in the future to account for discrepancies here. Another analysis that could be helpful in answering this question would be a time series decomposition analysis. This data did not have time observations, rather metrics already calculated based on the time interval provided. These models can also be applied to other areas in the United States or globally that are facing similar problems. Future analyses could also include spatial interpolation to estimate coastal erosion for all of California. This recent study inspired my suggestion for spatial interpolation as a future analysis.\n\nREFERENCES\n\nBarnard, P.L., Smith, S.A., and Foxgrover, A.C., 2020, California shorelines and shoreline change data, 1998-2016: U.S. Geological Survey data release, https://doi.org/10.5066/P91QSGXF\n“California.” NOAA Office for Coastal Management, https://coast.noaa.gov/states/california.html. \nKoppes, Steve. New High-Resolution Study on California Coastal Cliff Erosion Released, Scripps Institution of Oceanography, 4 Aug. 2022, https://scripps.ucsd.edu/news/new-high-resolution-study-california-coastal-cliff-erosion-released. \nNOAA Office for Coastal Management ADS Group. “Coastal Zone Management Programs.” NOAA Office for Coastal Management | States and Territories Working on Ocean and Coastal Management, https://coast.noaa.gov/czm/mystate/#:~:text=The%20California%20coastal%20zone%20generally,line%20of%20highest%20tidal%20action. \n“U.S. Climate Resilience Toolkit.” Coastal Erosion | U.S. Climate Resilience Toolkit, https://toolkit.climate.gov/topics/coastal-flood-risk/coastal-erosion"
  },
  {
    "objectID": "posts/2023-03-21_calcofi_ml/index.html",
    "href": "posts/2023-03-21_calcofi_ml/index.html",
    "title": "Using Machine Learning for Ocean Chemistry Prediction",
    "section": "",
    "text": "For our final project in my machine learning course, we participated in a Kaggle competition to predict the concentration of dissolved inorganic carbon in water samples by using ocean chemistry data. This data comes from the California Cooperative Oceanic Fisheries Investigations (CalCOFI) program."
  },
  {
    "objectID": "posts/2023-03-21_calcofi_ml/index.html#background",
    "href": "posts/2023-03-21_calcofi_ml/index.html#background",
    "title": "Using Machine Learning for Ocean Chemistry Prediction",
    "section": "",
    "text": "For our final project in my machine learning course, we participated in a Kaggle competition to predict the concentration of dissolved inorganic carbon in water samples by using ocean chemistry data. This data comes from the California Cooperative Oceanic Fisheries Investigations (CalCOFI) program."
  },
  {
    "objectID": "posts/2023-03-21_calcofi_ml/index.html#objective",
    "href": "posts/2023-03-21_calcofi_ml/index.html#objective",
    "title": "Using Machine Learning for Ocean Chemistry Prediction",
    "section": "Objective",
    "text": "Objective\nTo predict dissolved inorganic carbon, we will be using a Linear Regression Model in R to make these predictions. We will use the CalCOFI data to train our model to make predictions of inorganic dissolved carbon concentrations in different parts of the ocean that aren’t included in the training data.\nThe variables we are using as predictors in our model are:\n\nNO2uM - Micromoles of Nitrite per liter of seawater\nNO3uM - Micromoles of Nitrate per liter of seawater\nNH3uM - Micromoles of Ammonia per liter of seawater\nR_TEMP - Reported (Potential) Temperature (degrees Celsius)\nR_Depth - Reported Depth from pressure (meters)\nR_Sal - Reported Salinity (from Specific Volume Anomoly, M³ per Kg)\nR_DYNHT - Reported Dynamic Height (work per unit mass)\nR_Nuts - Reported Ammonium concentration (micromoles per Liter)\nR_Oxy_micromol.Kg - Reported Oxygen concentration (micromoles per kilogram)\nPO4uM - Micromoles of Phosphate per liter of seawater\nSiO3uM - Micromoles of Silicate per liter of seawater\nTA1 - Total Alkalinity (micromoles per kilogram solution)\nSalinity1 - Salinity (Practical Salinity Scale 1978)\nTemperature_degC - Temperature (degrees Celsius)"
  },
  {
    "objectID": "posts/2023-03-21_calcofi_ml/index.html#load-and-split-data",
    "href": "posts/2023-03-21_calcofi_ml/index.html#load-and-split-data",
    "title": "Using Machine Learning for Ocean Chemistry Prediction",
    "section": "Load and split data",
    "text": "Load and split data\nTo train machine learning models using a data set, the model must have training data to learn from and test data to compare its predictions to to evaluate model performance. We will then split the training data further into two groups, a validation set and training set. The training set will still be used to train the model, while the validation set will be used to evaluate how well the model performed.\n\n\nShow code\n#Reading in data used to train model\ntraining_data &lt;- read_csv(here(\"posts\",\n                               \"2023-03-21_calcofi_ml\",\n                               \"data\",\n                               \"train.csv\")) %&gt;%\n  clean_names() %&gt;%\n  select(-x13) #remove this since its all NA\n\n#Reading in data that will be used to test model\ntesting_data &lt;- read_csv(here(\"posts\",\n                               \"2023-03-21_calcofi_ml\",\n                               \"data\",\n                               \"test.csv\")) %&gt;%\n  clean_names() %&gt;% \n  mutate(ta1_x = ta1)\n\n\n#split the training data into training and evaluation sets, stratify by dissolved inorganic carbon concentration\ndata_split &lt;- initial_split(training_data,\n                            strata = dic)\n\n#extract training and test data from the training data\ntraining_set &lt;- training(data_split)\nevaluation_set &lt;- testing(data_split)\n\n#take a look at training and testing data \nhead(training_data)\nhead(evaluation_set)"
  },
  {
    "objectID": "posts/2023-03-21_calcofi_ml/index.html#pre-processing-data-creating-recipe-creating-models-and-creating-workflow",
    "href": "posts/2023-03-21_calcofi_ml/index.html#pre-processing-data-creating-recipe-creating-models-and-creating-workflow",
    "title": "Using Machine Learning for Ocean Chemistry Prediction",
    "section": "Pre-Processing Data, Creating Recipe, Creating Models, and Creating Workflow",
    "text": "Pre-Processing Data, Creating Recipe, Creating Models, and Creating Workflow\nTo pre-process the data for our model, we begin by creating a recipe where dissolved inorganic carbon concentration is the predicted value and all the variables mentioned above as the predictors.\n\n\nShow code\n#set seed for reproducibility\nset.seed(711)\n\n#creating a recipe\nbottle_recipe &lt;- recipe(dic ~.,\n                        data = training_set) %&gt;% \n  step_dummy(all_nominal(),\n             -all_outcomes(),\n             one_hot = TRUE) %&gt;% \n  step_normalize(all_numeric(),\n                 -all_outcomes()) %&gt;% \n  prep()\n\n#creating model specification of linear regression\nbottle_model &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n#bundle recipe and model spec into a workflow\nbottle_wf &lt;- workflow() %&gt;% \n  add_recipe(bottle_recipe) %&gt;% \n  add_model(bottle_model)"
  },
  {
    "objectID": "posts/2023-03-21_calcofi_ml/index.html#fit-model-to-training-data-and-make-predictions",
    "href": "posts/2023-03-21_calcofi_ml/index.html#fit-model-to-training-data-and-make-predictions",
    "title": "Using Machine Learning for Ocean Chemistry Prediction",
    "section": "Fit Model to Training Data and Make Predictions",
    "text": "Fit Model to Training Data and Make Predictions\n\n\nShow code\n#creating and training a model on the training data\nfit_bottle &lt;- bottle_wf %&gt;%\n  fit(training_set)\n\n#using the model to make predictions on the validation data   \nbottle_results &lt;- fit_bottle %&gt;% \n  predict(evaluation_set) %&gt;%\n  bind_cols(evaluation_set) %&gt;% \n  mutate(dic_prediction = .pred_res) %&gt;% \n  relocate(dic,\n           .before = id) %&gt;% \n  relocate(dic_prediction,\n           .before = id) %&gt;% \n  select(-.pred_res)\n\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from rank-deficient fit; attr(*, \"non-estim\") has\ndoubtful cases\n\n\nShow code\n#retrieve and evaluate our predictions\nbottle_metrics &lt;- bottle_results %&gt;%\n  metrics(estimate = dic_prediction,\n          truth = dic)\n\nbottle_metrics\n\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       6.82 \n2 rsq     standard       0.996\n3 mae     standard       3.45"
  },
  {
    "objectID": "posts/2023-03-21_calcofi_ml/index.html#test-model",
    "href": "posts/2023-03-21_calcofi_ml/index.html#test-model",
    "title": "Using Machine Learning for Ocean Chemistry Prediction",
    "section": "Test Model",
    "text": "Test Model\n\n\nShow code\n## Outputting predictions for our testing data\ntest_data_predictions &lt;- fit_bottle %&gt;% \n  predict(testing_data) %&gt;%\n  bind_cols(testing_data) %&gt;% \n  mutate(DIC = .pred_res) %&gt;% \n  relocate(DIC,\n           .before = id) %&gt;% \n  select(id,\n         DIC)\n\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from rank-deficient fit; attr(*, \"non-estim\") has\ndoubtful cases\n\n\nShow code\ntest_data_predictions\n\n\n# A tibble: 485 × 2\n      id   DIC\n   &lt;dbl&gt; &lt;dbl&gt;\n 1  1455 2173.\n 2  1456 2194.\n 3  1457 2325.\n 4  1458 1993.\n 5  1459 2147.\n 6  1460 2036.\n 7  1461 2159.\n 8  1462 2196.\n 9  1463 2270.\n10  1464 2314.\n# ℹ 475 more rows"
  },
  {
    "objectID": "posts/2023-09-07_spotify/index.html",
    "href": "posts/2023-09-07_spotify/index.html",
    "title": "Exploring Music Tastes with Machine Learning",
    "section": "",
    "text": "Introduction\nI love listening to all types of music, and used machine learning models in R to understand my taste a little bit better and compare my friend and my music taste!\nI began by requesting data from the Spotify API and using the data on my liked songs to build a model that is a binary classifier. To do so, I will be using three Machine Learning algorithms:\n\nK-Nearest Neighbor\nDecision Tree Model\nRandom Forest Model\n\nAccessing Spotify API\nTo begin gathering data from the Spotify API, I have to create and access a token containing the client ID and client secret. To do so, I began by navigating to Spotify for Developers and created an application. More information on accessing the Spotify API can be found in the documentation.\nHere, I am setting them as system values so I don’t have to provide the client ID & secret each time the API is used, and combining them to create an access token.\n\nSys.setenv(SPOTIFY_CLIENT_ID = 'your_token')\nSys.setenv(SPOTIFY_CLIENT_SECRET = 'your_token')\n\naccess_token &lt;- get_spotify_access_token(\n  client_id = Sys.getenv(\"SPOTIFY_CLIENT_ID\"),\n  client_secret = Sys.getenv(\"SPOTIFY_CLIENT_SECRET\")\n)\n\nData Preparation\nThe function ‘get_my_saved_tracks()’ from the spotifyr package will request all my liked tracks on Spotify. However, when called, the Spotify API will only return a dataframe with 50 tracks at a time.\n\nsaved_tracks &lt;- get_my_saved_tracks() #function from the spotifyr package\n\nSince I want to analyze all my likes, I will have to make many requests. Instead of doing this manually, I’ll use a function to combine all the requests into one call.\n\n#writing a function to combine my requests into one call since this function only returns up to 50 tracks at a time when called\nget_saved_tracks &lt;- function(limit = 50,\n                             authorization,\n                             offset = 0) {\n  tracks &lt;- data.frame()\n  for (i in 1:7) {\n    new_tracks &lt;- get_my_saved_tracks(limit = limit,\n                                      offset = offset)\n    tracks &lt;- rbind(tracks,\n                    as.data.frame(new_tracks))\n    offset &lt;- offset + limit\n  }\n  return(tracks)\n}\n\nmy_tracks &lt;- get_saved_tracks(authorization = access_token)\n\nNow that I have all my liked songs, I am going to request more information from the Spotify API to understand my taste better. To do so, I will give the API a list of my song IDs using the function get_track_audio_features. This will return a dataframe of audio features, including the tracks and their attributes.\n\nmy_audio_features &lt;- data.frame()   # create base empty data frame\n\nfor(i in seq(from = 1, to = 350, by = 100)) { # loop through all songs\n  \n  # collect 100 rows starting from i\n  row_index &lt;- i:(i + 99)   \n  \n  # pull out features for set rows\n  audio &lt;- get_track_audio_features(my_tracks$track.id[row_index])\n  \n  # add features to dataframe\n  my_audio_features &lt;- rbind(my_audio_features, audio)\n}\n\n\nmy_audio_features &lt;- drop_na(my_audio_features)\n\n\n# add songs_data$track.name\nkiran_audio &lt;- cbind(my_audio_features,\n                     track.name = my_tracks$track.name,\n                     track.popularity = my_tracks$track.popularity)\n\nkiran_audio &lt;- kiran_audio %&gt;% \n  select(-c(uri, track_href, analysis_url, type, id))\n\n#make a csv\nwrite_csv(kiran_audio, \"kiran_audio.csv\")\n\nNow, I swapped data with my friend, Erica. Since I want to compare our music tastes, I began by combining our data into a new dataframe.\n\n#read in erica's data \nerica_audio &lt;- read_csv(\"ericas_audio.csv\")\n\n#add listener id column \nkiran_audio &lt;- kiran_audio %&gt;%  \n  mutate(listener_id = 'Kiran')\n\n\nkiran_erica_audio &lt;- rbind(kiran_audio, erica_audio)\n\n#downloading combined data into a csv\nwrite_csv(combined_audio, \"combined_audio.csv\")\n\nHere I loaded in the downloaded dataframe, and appended a column called ‘listener_id’ so I know whose tracks are whose.\n\n#combining our data \ncombined_audio &lt;- read_csv(here(\"posts\",\n                                \"2023-09-07_spotify\",\n                                \"combined_audio.csv\")) %&gt;%  \n    mutate(listener_id = as.factor(listener_id))\n\nData Exploration\nNow that I have prepared the data with the steps above, I can start exploring some aspects of my data, my friend’s, and compare them!\nFirst, I wanted to look at all the variables that the Spotify API includes when accessing audio features.\n\naudio_features &lt;- combined_audio %&gt;% \n  colnames() %&gt;% \n  as.data.frame()\n\n#removing the first row of insignificant data \naudio_features_table &lt;- as.data.frame(audio_features[-1,])\n\n#renaming column to audio feature\ncolnames(audio_features_table)[1] &lt;- \"Audio Features\"\n\n\naudio_features_table\n\n     Audio Features\n1              ...2\n2      danceability\n3            energy\n4               key\n5          loudness\n6              mode\n7       speechiness\n8      acousticness\n9  instrumentalness\n10         liveness\n11          valence\n12            tempo\n13      duration_ms\n14   time_signature\n15       track.name\n16 track.popularity\n17      listener_id\n\n\nI’m also curious about the relationship between some of the variables mentioned above..\n\n#danceability and energy\nhexplot_1 &lt;- ggplot(data = combined_audio,\n         aes(energy, danceability)) +\n  geom_hex() +\n  scale_fill_viridis_c(option = \"magma\") \n\n#danceability and loudness\nhexplot_2 &lt;- ggplot(data = combined_audio,\n         aes(loudness, danceability)) +\n  geom_hex() +\n  scale_fill_viridis_c(option = \"magma\")\n\n\n#acousticness and energy\nhexplot_3 &lt;- ggplot(data = combined_audio,\n         aes(acousticness, energy)) +\n  geom_hex() +\n  scale_fill_viridis_c(option = \"magma\")\n\n\n#acousticness and energy\nhexplot_4 &lt;- ggplot(data = combined_audio,\n         aes(tempo, loudness)) +\n  geom_hex() +\n  scale_fill_viridis_c(option = \"magma\")\n\n\nggarrange(hexplot_1, hexplot_2, hexplot_3, hexplot_4)\n\n\n\n\nNow, I’ll compare who has more popular songs!\n\ncombined_audio %&gt;%\n  arrange(desc(track.popularity)) %&gt;% \n  select(track.popularity,\n         track.name,\n         listener_id) %&gt;% \n  rename('track name' = track.name,\n         'listener' = listener_id) %&gt;%  \n  head(8) %&gt;% \n  kable()\n\n\n\n\ntrack.popularity\ntrack name\nlistener\n\n\n\n\n85\nNeverita\nKiran\n\n\n84\nEscapism. - Sped Up\nKiran\n\n\n82\nPink + White\nKiran\n\n\n81\nBeggin’\nErica\n\n\n81\nAll The Stars (with SZA)\nErica\n\n\n81\nLet Me Down Slowly\nErica\n\n\n80\nFormula\nKiran\n\n\n80\nUn Coco\nKiran\n\n\n\n\n\nLooks like I have a the most popular songs in this dataset from this little snippet. Now I’ll visualize the data to get a better picture.\n\n#visualize the data!\n#who listens to more popular track\npopularity_plot &lt;- ggplot(data = combined_audio, aes(x = track.popularity)) + \n  geom_bar(aes(fill = listener_id)) +\n   labs(title = \"Distribution of Song Popularity by Listener\",\n       x = \"Song Popularity\", y = \"Count\") +\n  scale_fill_manual(values = c(\"#1721d4\", \"#02b34b\")) +\n  theme_minimal() \n\npopularity_plot\n\n\n\n\nModeling\nAs I mentioned earlier, I will create two models, a K-Nearest Neighbor model and a decision tree model. Now that I have the data prepared and I understand it better, I can make these models predict whether a track belongs to me or Erica’s Spotify list.\nI’m starting by splitting the data into three sets: testing, splitting, and training data sets.\n\n#remove track id  & index \ncombined_audio &lt;- combined_audio %&gt;% \n  select(-c(...1,...2,track.name))\n\n#set seed for reproducibility\nset.seed(711)\n\n#split the data \naudio_split &lt;- initial_split(combined_audio)\naudio_test &lt;- testing(audio_split)\naudio_train &lt;- training(audio_split)\n\nNow, I will run through the three algorithms mentioned above. With each model, I will go through the following steps:\n\nPreprocessing: Using a step function and recipe on the training data.\nSet model specification: Tune specification of model with hyper parameters to finding best version of model. I will use cross validation folds to do this, which basically breaks the data into 10 sections, leaving 1 section as test data and rest are training. Then R continues thing process through all the broken up sections of data to determine the best hyper-parameters. The model will create predictions of 0 or 1 based on this tuning step.\nModel fitting: Then we fit the model with the best hyper-parameters onto the test data we split at beginning.\n\nMODEL #1: K-Nearest Neighbors\n\n#preprocessing\n#recipe always define by training data\nmusic_rec &lt;- recipe(listener_id ~., \n                    data = audio_train) %&gt;%  \n  step_dummy(all_nominal(),\n             -all_outcomes(),\n             one_hot = TRUE) %&gt;% \n  step_normalize(all_numeric(),\n                 -all_outcomes()) %&gt;% \n  prep()\n\n#bake \nbaked_audio &lt;- bake(music_rec, audio_train)\n\n#apply recipe to test data \nbaked_test &lt;- bake(music_rec, audio_test)\n\n#specify knn model\nknn_spec &lt;- nearest_neighbor() %&gt;% \n  set_engine(\"kknn\") %&gt;% \n  set_mode(\"classification\")\n\n#resampling folds\ncv_folds &lt;- audio_train %&gt;% \n  vfold_cv(v = 5)\n\n#put together into workflow\nknn_workflow &lt;- workflow() %&gt;% \n  add_model(knn_spec) %&gt;% \n  add_recipe(music_rec)\n\n#fit resamples\nknn_resample &lt;- knn_workflow %&gt;% \n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n\n\n#Define our KNN model with tuning\nknn_spec_tuned  &lt;- \n  nearest_neighbor(neighbors = tune()) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"kknn\")\n#Check the model\nknn_spec_tuned\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune()\n\nComputational engine: kknn \n\n\n\n# Define a new workflow\nwf_knn_tuned &lt;- workflow() |&gt; \n  add_model(knn_spec_tuned) |&gt; \n  add_recipe(music_rec)\n    \n# Fit the workflow on our predefined folds and hyperparameters\nfit_knn_cv &lt;- wf_knn_tuned |&gt; \n  tune_grid(\n    cv_folds, #tuning based on these folds \n    grid = data.frame(neighbors = c(1,5, seq(10,100,10)))\n    \n  )\n\n# Check the performance with collect_metrics()\nfit_knn_cv |&gt;  collect_metrics()\n\n# A tibble: 24 × 7\n   neighbors .metric  .estimator  mean     n std_err .config              \n       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1         1 accuracy binary     0.633     5  0.0342 Preprocessor1_Model01\n 2         1 roc_auc  binary     0.633     5  0.0400 Preprocessor1_Model01\n 3         5 accuracy binary     0.658     5  0.0356 Preprocessor1_Model02\n 4         5 roc_auc  binary     0.708     5  0.0462 Preprocessor1_Model02\n 5        10 accuracy binary     0.667     5  0.0299 Preprocessor1_Model03\n 6        10 roc_auc  binary     0.731     5  0.0374 Preprocessor1_Model03\n 7        20 accuracy binary     0.667     5  0.0182 Preprocessor1_Model04\n 8        20 roc_auc  binary     0.744     5  0.0229 Preprocessor1_Model04\n 9        30 accuracy binary     0.677     5  0.0173 Preprocessor1_Model05\n10        30 roc_auc  binary     0.750     5  0.0160 Preprocessor1_Model05\n# ℹ 14 more rows\n\nfinal_knn_wf &lt;- wf_knn_tuned |&gt;  \n    finalize_workflow(select_best(fit_knn_cv,\n                                  metric = \"accuracy\"))\n\n# Fitting our final workflow \nfinal_knn_fit &lt;- final_knn_wf |&gt; \n  fit(data = audio_train)\n\nmusic_pred &lt;- final_knn_fit |&gt; \n  predict(new_data = audio_test)\n\n# Write over 'final_fit' with this last_fit() approach \nfinal_knn_fit &lt;- final_knn_wf |&gt; \n  last_fit(audio_split)\n\nfinal_knn_fit$.predictions\n\n[[1]]\n# A tibble: 159 × 6\n   .pred_Erica .pred_Kiran  .row .pred_class listener_id .config             \n         &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;       &lt;chr&gt;               \n 1       0.266       0.734     1 Kiran       Kiran       Preprocessor1_Model1\n 2       0.529       0.471     2 Erica       Kiran       Preprocessor1_Model1\n 3       0.215       0.785     5 Kiran       Kiran       Preprocessor1_Model1\n 4       0.533       0.467    10 Erica       Kiran       Preprocessor1_Model1\n 5       0.392       0.608    11 Kiran       Kiran       Preprocessor1_Model1\n 6       0.594       0.406    30 Erica       Kiran       Preprocessor1_Model1\n 7       0.366       0.634    31 Kiran       Kiran       Preprocessor1_Model1\n 8       0.699       0.301    34 Erica       Kiran       Preprocessor1_Model1\n 9       0.416       0.584    35 Kiran       Kiran       Preprocessor1_Model1\n10       0.401       0.599    41 Kiran       Kiran       Preprocessor1_Model1\n# ℹ 149 more rows\n\n# Collect metrics on the test data\nknn_metrics &lt;- final_knn_fit |&gt; \n  collect_metrics()\n\nknn_metrics\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.667 Preprocessor1_Model1\n2 roc_auc  binary         0.758 Preprocessor1_Model1\n\n\nMODEL #2: DECISION TREE\n\n#preprocess\ndec_tree_rec &lt;- recipe(listener_id ~ .,\n                       data = audio_train) %&gt;% \n  step_dummy(all_nominal(),\n             -all_outcomes(),\n             one_hot = TRUE) %&gt;% \n  step_normalize(all_numeric(),\n                 -all_outcomes())\n\n\n#dec tree specification tuned to the optimal parameters\n\n#tell the model that we are tuning hyperparams\ndec_tree_spec_tune &lt;- decision_tree(\n  cost_complexity = tune(), #to tune, call tune()\n  tree_depth = tune(), \n  min_n = tune()) %&gt;%  \n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\ndec_tree_grid &lt;- grid_regular(cost_complexity(),\n                              tree_depth(),\n                              min_n(),\n                              levels = 4) \n\ndec_tree_grid \n\n# A tibble: 64 × 3\n   cost_complexity tree_depth min_n\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt;\n 1    0.0000000001          1     2\n 2    0.0000001             1     2\n 3    0.0001                1     2\n 4    0.1                   1     2\n 5    0.0000000001          5     2\n 6    0.0000001             5     2\n 7    0.0001                5     2\n 8    0.1                   5     2\n 9    0.0000000001         10     2\n10    0.0000001            10     2\n# ℹ 54 more rows\n\n\n\ndoParallel::registerDoParallel() #build trees in parallel\n#200s\ndec_tree_rs &lt;- tune_grid(\n  dec_tree_spec_tune, \n  as.factor(listener_id)~.,\n  resamples = cv_folds,\n  grid = dec_tree_grid,\n  metrics = metric_set(accuracy)\n)\ndec_tree_rs\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits           id    .metrics          .notes          \n  &lt;list&gt;           &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [379/95]&gt; Fold1 &lt;tibble [64 × 7]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [379/95]&gt; Fold2 &lt;tibble [64 × 7]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [379/95]&gt; Fold3 &lt;tibble [64 × 7]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [379/95]&gt; Fold4 &lt;tibble [64 × 7]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [380/94]&gt; Fold5 &lt;tibble [64 × 7]&gt; &lt;tibble [0 × 3]&gt;\n\n\n\n# Selecting best models \nshow_best(dec_tree_rs)\n\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric  .estimator  mean     n std_err\n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1    0.0000000001         10    14 accuracy binary     0.675     5  0.0177\n2    0.0000001            10    14 accuracy binary     0.675     5  0.0177\n3    0.0001               10    14 accuracy binary     0.675     5  0.0177\n4    0.0000000001         15    14 accuracy binary     0.675     5  0.0177\n5    0.0000001            15    14 accuracy binary     0.675     5  0.0177\n# ℹ 1 more variable: .config &lt;chr&gt;\n\nselect_best(dec_tree_rs)\n\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001         10    14 Preprocessor1_Model25\n\n# Finalizing our model \nfinal_dec_tree &lt;- finalize_model(dec_tree_spec_tune,\n                                 select_best(dec_tree_rs))\n\nfinal_dec_tree_fit &lt;- last_fit(final_dec_tree,\n                               as.factor(listener_id) ~.,\n                               audio_split)\n\n# Outputting Metrics \nfinal_dec_tree_fit$.predictions\n\n[[1]]\n# A tibble: 159 × 6\n   .pred_Erica .pred_Kiran  .row .pred_class `as.factor(listener_id)` .config   \n         &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;                    &lt;chr&gt;     \n 1      0.114       0.886      1 Kiran       Kiran                    Preproces…\n 2      0.875       0.125      2 Erica       Kiran                    Preproces…\n 3      0.667       0.333      5 Erica       Kiran                    Preproces…\n 4      0.0732      0.927     10 Kiran       Kiran                    Preproces…\n 5      0.0732      0.927     11 Kiran       Kiran                    Preproces…\n 6      0.0357      0.964     30 Kiran       Kiran                    Preproces…\n 7      0.455       0.545     31 Kiran       Kiran                    Preproces…\n 8      0.952       0.0476    34 Erica       Kiran                    Preproces…\n 9      0.114       0.886     35 Kiran       Kiran                    Preproces…\n10      0.778       0.222     41 Erica       Kiran                    Preproces…\n# ℹ 149 more rows\n\ndec_tree_metrics &lt;- final_dec_tree_fit %&gt;% \n  collect_metrics()\n\ndec_tree_metrics\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.648 Preprocessor1_Model1\n2 roc_auc  binary         0.689 Preprocessor1_Model1\n\n\nThen validate and compare the performance of the models I made\nMODEL #3: Random Forest\n\n# Define validating set \nvalidation_set &lt;- validation_split(audio_train, \n                                   strata = listener_id, \n                                   prop = 0.70)\n\n# random forest spec \nrand_forest_spec &lt;-\n  rand_forest(mtry = tune(),\n              min_n = tune(),\n              trees = 1000)  %&gt;%  \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"classification\")\n\n# random forest workflow\nrand_forest_workflow &lt;- workflow() %&gt;%  \n  add_recipe(music_rec) %&gt;%  \n  add_model(rand_forest_spec)\n\n# buuild in parallel \ndoParallel::registerDoParallel()\n\nrand_forest_res &lt;- \n  rand_forest_workflow %&gt;%  \n  tune_grid(validation_set,\n            grid = 25,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(accuracy))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n## model metrics\nrand_forest_res %&gt;% collect_metrics()\n\n# A tibble: 25 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     9    30 accuracy binary     0.734     1      NA Preprocessor1_Model01\n 2     3    35 accuracy binary     0.748     1      NA Preprocessor1_Model02\n 3     4    19 accuracy binary     0.748     1      NA Preprocessor1_Model03\n 4     5    14 accuracy binary     0.741     1      NA Preprocessor1_Model04\n 5     7     7 accuracy binary     0.762     1      NA Preprocessor1_Model05\n 6     9    28 accuracy binary     0.755     1      NA Preprocessor1_Model06\n 7     3     9 accuracy binary     0.734     1      NA Preprocessor1_Model07\n 8     8    18 accuracy binary     0.741     1      NA Preprocessor1_Model08\n 9    12    31 accuracy binary     0.762     1      NA Preprocessor1_Model09\n10    11     3 accuracy binary     0.741     1      NA Preprocessor1_Model10\n# ℹ 15 more rows\n\n# find best accuracy metric \nrand_forest_res %&gt;%  \n  show_best(metric = \"accuracy\")\n\n# A tibble: 5 × 8\n   mtry min_n .metric  .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    14    38 accuracy binary     0.769     1      NA Preprocessor1_Model19\n2     7     7 accuracy binary     0.762     1      NA Preprocessor1_Model05\n3    12    31 accuracy binary     0.762     1      NA Preprocessor1_Model09\n4    10    15 accuracy binary     0.762     1      NA Preprocessor1_Model13\n5    13     6 accuracy binary     0.762     1      NA Preprocessor1_Model25\n\n# plot \nautoplot(rand_forest_res)\n\n\n\n# choose best random forest model \nbest_rand_forest &lt;- select_best(rand_forest_res, \"accuracy\")\nbest_rand_forest\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1    14    38 Preprocessor1_Model19\n\n# output preds\nrand_forest_res %&gt;%   \n  collect_predictions()\n\n# A tibble: 3,575 × 7\n   id         .pred_class  .row  mtry min_n listener_id .config              \n   &lt;chr&gt;      &lt;fct&gt;       &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;       &lt;chr&gt;                \n 1 validation Kiran           5     9    30 Kiran       Preprocessor1_Model01\n 2 validation Erica           6     9    30 Erica       Preprocessor1_Model01\n 3 validation Kiran           7     9    30 Kiran       Preprocessor1_Model01\n 4 validation Erica          13     9    30 Kiran       Preprocessor1_Model01\n 5 validation Kiran          14     9    30 Kiran       Preprocessor1_Model01\n 6 validation Erica          21     9    30 Erica       Preprocessor1_Model01\n 7 validation Kiran          24     9    30 Erica       Preprocessor1_Model01\n 8 validation Kiran          27     9    30 Kiran       Preprocessor1_Model01\n 9 validation Erica          33     9    30 Erica       Preprocessor1_Model01\n10 validation Kiran          35     9    30 Kiran       Preprocessor1_Model01\n# ℹ 3,565 more rows\n\n# final model working in parallel \ndoParallel::registerDoParallel()\nlast_rand_forest_model &lt;- \n  rand_forest(mtry = 2, min_n = 3, trees = 1000) %&gt;%  \n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%  \n  set_mode(\"classification\")\n\n#Updating our workflow \nlast_rand_forest_workflow &lt;- \n  rand_forest_workflow %&gt;%  \n  update_model(last_rand_forest_model)\n\n# Updating our model fit \nlast_rand_forest_fit &lt;- \n  last_rand_forest_workflow %&gt;%  \n  last_fit(audio_split)\n\n# Outputting model metrics \nrand_forest_metrics &lt;- last_rand_forest_fit %&gt;%   \n  collect_metrics()\n\nrand_forest_metrics\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.748 Preprocessor1_Model1\n2 roc_auc  binary         0.829 Preprocessor1_Model1\n\n# find most important variables to our model \nlast_rand_forest_fit %&gt;%  \n  extract_fit_parsnip() %&gt;%  \n  vip::vip(num_features = 12) + \n  ggtitle(\"Order of Variable Importance in Random Forest Model\") +\n  theme_minimal()\n\n\n\n\n\n# nearest neighbors metrics\nknn_accuracy &lt;- knn_metrics$.estimate[1]\n\n# decision tree metrics\ndec_tree_accuracy &lt;- dec_tree_metrics$.estimate[1]\n\n# random forest  metrics\nrandom_forest_accuracy &lt;- rand_forest_metrics$.estimate[1]\n\nmodel_accuracy &lt;- tribble(\n  ~\"model\", ~\"accuracy\",\n  \"K-Nearest Neighbor\", knn_accuracy,\n  \"Decision Tree\", dec_tree_accuracy,\n  \"Random Forest\", random_forest_accuracy\n)\n\n# Plotting bar chart to compare models accuracy \nggplot(data = model_accuracy, aes(x = model,\n                                  y = accuracy)) +\n  geom_col(fill = c(\"red\",\"purple\",\"blue\")) +\n  theme_minimal() +\n  labs(title = \"Comparison of Model Accuracy for Spotify Data\",\n       x = \"Model\",\n       y = \"Accuracy\")\n\n\n\n\nThis analysis suggests that the Random Forest model has the best accuracy at 69.2% and the worst model is the Decision Tree model with 64.8% accuracy.\n\n\n\nCitationBibTeX citation:@online{favre2023,\n  author = {Favre, Kiran},\n  title = {Exploring {Music} {Tastes} with {Machine} {Learning}},\n  date = {2023-09-07},\n  url = {https://kiranfavre.github.io/posts/2023-09-07/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFavre, Kiran. 2023. “Exploring Music Tastes with Machine\nLearning.” September 7, 2023. https://kiranfavre.github.io/posts/2023-09-07/."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "Exploring Music Tastes with Machine Learning\n\n\n\nR\n\n\nMachine Learning\n\n\nMusic\n\n\n\nComparing a friend’s and my Spotify liked songs by building a binary classfication model.\n\n\n\nKiran Favre\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Machine Learning for Ocean Chemistry Prediction\n\n\n\nMachine Learning\n\n\nR\n\n\nOcean Chemistry\n\n\n\nTraining machine learning models to predict dissolved inorganic carbon in water samples.\n\n\n\nKiran Favre\n\n\nMar 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of California Coastal Erosion\n\n\n\nR\n\n\nGeospatial\n\n\nCalifornia\n\n\n\nUsing muliple linear regression analysis and USGS Coastal Change data to analyze where coastlines in California are eroding the most.\n\n\n\nKiran Favre\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nDetermining Suitable Oyster Habitats in West Coast EEZ’s\n\n\n\nR\n\n\nGeospatial\n\n\nCalifornia\n\n\n\nI will be using geospatial data to map suitable oyster habitats in Exclusive Economic Zones on the West Coast, and creating a function to find these zones for any species…\n\n\n\nKiran Favre\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]