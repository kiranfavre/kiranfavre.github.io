[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "AboutHobbiesTravel\n\n\n\nMore about me!\n\nI recently earned a Master of Environmental Data Science degree from the Bren School of Environmental Science & Management at the University of California, Santa Barbara. I built two interactive web applications for a client with a team of three other graduate students. These applications were developed in response to our client’s needs of improving their data management system and creating a way to visualize the data they collected. Their work directly impacts Marine Protected Areas by identifiying gaps in enforcement of these regions, collecting data on categories related to enforcement, and working with partners to improve the protection of these habitats. Our applications allowed them to reduce errors in data compilation, save time by giving them an automated data entry system, and a tool to visualize trends in their data. In addition to this capstone project, I learned about geospatial analysis, data management, and open data science through coursework and connections at the Bren School and the National Center for Ecological Analysis and Synthesis.\nI graduated from the University of California, Santa Cruz in 2022 with a Bachelor’s degree in Environmental Sciences, where I focused on climate change and oceanography. While there, I was a Santa Cruz Climate Action Program Intern who collected data on potential community responses to coastal erosion. Surveying and collecting data as an intern, doing field work through courses at UCSC, and working with climate models in my undergraduate capstone project inspired my curiosity for data as a tool of scientific communication. Interest in undergraduate environmental data analysis and climate modeling drove me to seek out the Bren School for Environmental Science & Management to deepen my understanding of data driven solutions to environmental problems. I hopes to use my degree to contribute to sustainable technology and to pursue a career in data science.\n\n\n\nSpending time by the ocean\nHiking and camping with my pups\nMusic (going to concerts or playing it!)\nScuba diving\nExploring new restaurants\n\n\n\n\nI love to travel, and have been fortunate to have had the opportunity to. I look forward to travelling more in my life since my travels so far have given me the chance to learn more about different perspectives and cultures. Here where I’ve been so far!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kiran Favre",
    "section": "",
    "text": "Hi!\nI’m Kiran, and I a recent graduate from the Bren School for Environmental Science & Management at the University of California, Santa Barbara. I earned my Master of Environmental Data Science, and worked on a capstone project with a group to create an application that automates our client’s data entry workflow, improving their data management system, and a developed a data visualization application. I am interested in climate change, science communication, and green technology.\n\n\nEducation\n\nMaster’s of Environmental Data Science, 2023\n\n\nBren School for Environmental Science & Management, University of California, Santa Barbara\n\n\nBS in Environmental Sciences, 2022\n\n\nUniversity of California, Santa Cruz"
  },
  {
    "objectID": "posts/2022-12-03_geospatial/index.html",
    "href": "posts/2022-12-03_geospatial/index.html",
    "title": "Determining Suitable Oyster Habitats in West Coast EEZ’s",
    "section": "",
    "text": "This blog post is from my Geospatial Analysis and Remote Sensing course as a part of my master’s program at the Bren School for Environmental Science & Management. I will be determining which Exclusive Economic Zones (EEZ’s) on the West Coast of the United States are the most suitable for developing marine aquaculture for many species of oysters. Then, I make a function to map any species’ suitable habitats within West Coast EEZs by allowing for user inputs of species’ suitable habitats based on sea surface temperatures and depth."
  },
  {
    "objectID": "posts/2022-12-03_geospatial/index.html#background",
    "href": "posts/2022-12-03_geospatial/index.html#background",
    "title": "Determining Suitable Oyster Habitats in West Coast EEZ’s",
    "section": "",
    "text": "This blog post is from my Geospatial Analysis and Remote Sensing course as a part of my master’s program at the Bren School for Environmental Science & Management. I will be determining which Exclusive Economic Zones (EEZ’s) on the West Coast of the United States are the most suitable for developing marine aquaculture for many species of oysters. Then, I make a function to map any species’ suitable habitats within West Coast EEZs by allowing for user inputs of species’ suitable habitats based on sea surface temperatures and depth."
  },
  {
    "objectID": "posts/2022-12-03_geospatial/index.html#data",
    "href": "posts/2022-12-03_geospatial/index.html#data",
    "title": "Determining Suitable Oyster Habitats in West Coast EEZ’s",
    "section": "Data",
    "text": "Data\n\nSea Surface Temperature Data\nI will use average annual sea surface temperature (SST) from the years 2008 to 2012 to characterize the average sea surface temperature within the region. The data we are working with was originally generated from NOAA’s 5km Daily Global Satellite Sea Surface Temperature Anomaly v3.1.\n\nBathymetry Data\nTo characterize the depth of the ocean, I will use the General Bathymetric Chart of the Oceans (GEBCO).[^3]\n\n\nExclusive Economic Zones Data\nWe will be designating maritime boundaries using Exclusive Economic Zones off of the west coast of US from Marineregions.org."
  },
  {
    "objectID": "posts/2022-12-03_geospatial/index.html#process",
    "href": "posts/2022-12-03_geospatial/index.html#process",
    "title": "Determining Suitable Oyster Habitats in West Coast EEZ’s",
    "section": "Process",
    "text": "Process\nI will start with loading the necessary data listed above and validate that all the data has the same coordinate reference system.\n\n\nShow code\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(here)\nlibrary(terra)\nlibrary(tmap)\nlibrary(tmaptools)\n\n\n#file path set by Rproj\nsetwd(here())\n\n\n\n\nShow code\n#read in West coast EEZ shape file w terra \nwc_EEZ_regions &lt;- st_read(here(\"posts\",\n                               \"2022-12-03_geospatial\",\n                               \"data\",\n                               \"wc_regions_clean.shp\"))\n\n\nReading layer `wc_regions_clean' from data source \n  `C:\\Users\\kiran\\Documents\\MEDS 2022-2023\\kiranfavre.github.io\\posts\\2022-12-03_geospatial\\data\\wc_regions_clean.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n\n\nShow code\n#read in SST rasters\navg_sst_2008 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2008.tif\"))\navg_sst_2009 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2009.tif\"))\navg_sst_2010 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2010.tif\"))\navg_sst_2011 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2011.tif\"))\navg_sst_2012 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2012.tif\"))\n\n#combine into raster stack \nall_sst &lt;- c(avg_sst_2008,\n             avg_sst_2009,\n             avg_sst_2010,\n             avg_sst_2011,\n             avg_sst_2012)\n\n#read in bathymetry raster\ndepth &lt;- rast(here(\"posts\",\n                    \"2022-12-03_geospatial\",\n                    \"data\",\n                    \"depth.tif\"))\n\n\nChecking if the coordinate reference systems are the same. The sea surface temperature data needs to be reprojected to match the coordinate reference systems of the EEZ and depth data.\n\n\nShow code\nst_crs(wc_EEZ_regions) # crs EPSG:4326\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\n\nShow code\nst_crs(all_sst)  #crs EPSG:9122\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"unknown\",\n        ELLIPSOID[\"WGS84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1,\n                ID[\"EPSG\",9001]]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433,\n            ID[\"EPSG\",9122]]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433,\n                ID[\"EPSG\",9122]]]]\n\n\nShow code\nst_crs(depth)  #crs EPSG 4326\n\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\nShow code\n#re project using the terra way\nall_sst_reproj &lt;- project(all_sst,\n                          wc_EEZ_regions)\n\n\nWarning: [project,SpatRaster] argument y (the crs) should be a character value"
  },
  {
    "objectID": "posts/2022-12-03_geospatial/index.html#process-data",
    "href": "posts/2022-12-03_geospatial/index.html#process-data",
    "title": "Determining Suitable Oyster Habitats in West Coast EEZ’s",
    "section": "Process data",
    "text": "Process data\nNext, I need process the SST and depth data so that they can be combined, I will resample to match the SST data using the nearest neighbor approach since the extents, resolutions, and positions of the depth and SST are different.\n\n\nShow code\n#find the mean SST from 2008-2012\nmean_sst &lt;- mean(all_sst_reproj)\n\n#convert sst from K to C\nmean_sst_c &lt;- mean_sst - 273.15\n\n\n\n\nShow code\n#crop depth rast to match the extent of the SST rast\ndepth_cropped &lt;- crop(depth,mean_sst_c)\n\n\n#using nearest neighbor approach to resample\ndepth_resampled &lt;- resample(depth_cropped,\n                mean_sst_c,\n                method = \"near\")\n\n#stack to check if they have the same resolution\nresolution_test &lt;- c(depth_resampled,\n                       all_sst_reproj)\n#they stacked properly, indicating that they both have same resolutions, extent, and crs\n\n\n\nFind suitable locations\nTo determine suitable locations for marine aquaculture, I need to look for locations that are suitable in terms of both SST and depth.\n\n\nShow code\n#oyster happy spots\n#sea surface temperature: 11-30&deg;C\\\n#depth: 0-70 meters below sea level\n\n#reclassify sst into suitable locations for oysters\n#reclassification matrix for suitable range of sst\n\nrcl_sst &lt;- matrix(c(-Inf, 11, NA, 11, 30,\n                    1,30, Inf, NA), \n              ncol = 3, byrow = TRUE) #anything outside of range is NA\n#reclassifying raster using a reclassification matrix\nsst_suitable_locs &lt;- classify(mean_sst_c,\n                     rcl = rcl_sst, \n                     include.lowest = TRUE)\n\n\n\n\nShow code\n#reclassify depth into suitable locations for oysters\n#reclassification matrix for suitable range of depths\n\nrcl_depth &lt;- matrix(c(-Inf, -70, NA,\n                      -70, 0, 1,\n                      0, Inf, NA),\n                    ncol = 3, byrow = TRUE)\n\n#reclassifying raster using a reclassification matrix\ndepth_suitable_locs &lt;- classify(depth_resampled,\n                     rcl = rcl_depth,\n                     include.lowest = TRUE)\n\n\n\n\nShow code\n#define function\nmult_fun &lt;- function(x, y) {\n  return(x*y)}\n\n\n#find locations that satisfy both conditions\noyst_suitable_habitat &lt;- lapp(c(sst_suitable_locs, depth_suitable_locs), mult_fun)\n\n\n\n\nDetermining the most suitable EEZ\nI want to determine the total suitable area within each EEZ so I can rank zones by priority. To do this, I will need to find the total area of suitable locations within each EEZ.\n\n\nShow code\n# making obj of individual cell size of areas that are suitable for oysters so we can find the size of each cell\ngrid_cell_size &lt;- cellSize(oyst_suitable_habitat,\n                           mask = TRUE, #keep values within the suitable habitat\n                           transform = TRUE,\n                           unit = \"km\")\n\nplot(grid_cell_size, main = \"Cell Size\")\n\n\n\n\n\n\n\nShow code\n# rasterize wc shape file to help find total suitable area\n# Rasterize(): Transfer values associated with 'object' type spatial data (points, lines, polygons) to raster cells.\n# wc_EEZ regions are the points, and we will transfer those values to the raster i set up earlier for suitable oyster habitats\nwc_rasterized &lt;- rasterize(wc_EEZ_regions,\n                           oyst_suitable_habitat,\n                           field = \"rgn\")\n\n\n\n\nShow code\n#make a mask of wc raster and suitable locs for oysters\n#goal is to have areas of suitable oyster habitat that also fall in w coast EEZ regions\nwc_mask &lt;- mask(wc_rasterized, oyst_suitable_habitat,\n                updatevalue = NA,\n                inverse = FALSE)\n\n\n\n\nShow code\n#find area of each suitable zones with zonal() in each of 5 regions of EEZ\nwc_suitable_area &lt;- zonal(grid_cell_size, wc_mask, na.rm = TRUE, sum)\n\n\n\n\nShow code\n#join data\nwc_suitable_EEZ &lt;- full_join(wc_EEZ_regions, wc_suitable_area, by = \"rgn\") |&gt; \n  mutate(suitable_area = area, #rename area to suitable area \n         percentage_suitable = (suitable_area/area_km2 * 100), #percentage of suitable area\n         .before = geometry)\n\nprint(wc_suitable_EEZ)\n\n\nSimple feature collection with 5 features and 8 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n                  rgn rgn_key      area_m2 rgn_id  area_km2      area\n1              Oregon      OR 179994061293      1 179994.06 1074.2562\n2 Northern California    CA-N 164378809215      2 164378.81  178.0246\n3  Central California    CA-C 202738329147      3 202738.33 4069.5671\n4 Southern California    CA-S 206860777840      4 206860.78 3508.1870\n5          Washington      WA  66898309678      5  66898.31 2378.2758\n  suitable_area percentage_suitable                       geometry\n1     1074.2562           0.5968287 MULTIPOLYGON (((-123.4318 4...\n2      178.0246           0.1083014 MULTIPOLYGON (((-124.2102 4...\n3     4069.5671           2.0073003 MULTIPOLYGON (((-122.9928 3...\n4     3508.1870           1.6959169 MULTIPOLYGON (((-120.6505 3...\n5     2378.2758           3.5550611 MULTIPOLYGON (((-122.7675 4...\n\n\n\n\nVisualize results\n\n\nShow code\n#set to interactive mode\ntmap_mode(\"view\")\n\n\ntmap mode set to interactive viewing\n\n\n\n\nShow code\n#map for total suitable area for oysters by region\n\noyst_area_map &lt;- tm_basemap(\"Stamen.Terrain\") +\n  tm_shape(wc_suitable_EEZ) +\n  tm_polygons(col = 'area',\n              palette = 'Oranges',\n              alpha = 0.75,\n              border.col = 'black',\n              title = \"Total Suitable Area\") +\n  tm_text(\"rgn\", size = 0.54) +\n  tm_scale_bar(position = c(\"left\", \"right\"))\n \noyst_area_map\n\n\n\n\n\n\n\n\n\nShow code\n#map for percent suitable area by region\noyst_percentage_map &lt;- tm_basemap(\"Stamen.Terrain\") +\n  tm_shape(wc_suitable_EEZ) +\n  tm_polygons(col = 'percentage_suitable',\n              palette = 'Purples',\n              alpha = 0.75,\n              border.col = 'black',\n              title = \"Percentage of Suitable Area\") +\n  tm_text(\"rgn\", size = 0.54) +\n  tm_scale_bar(position = c(\"left\", \"right\"))\n\noyst_percentage_map\n\n\n\n\n\n\n\n\n\nExpanding this workflow to other species\n\n\nShow code\nfind_suitable_locs &lt;- function(SST_low, SST_high, depth_low, depth_high, spp_name) {\n\n  #read in West coast EEZ shape file w terra \nwc_EEZ_regions &lt;- st_read(here(\"posts\",\n                               \"2022-12-03_geospatial\",\n                               \"data\",\n                               \"wc_regions_clean.shp\"))\n\n\n#read in SST rasters\navg_sst_2008 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2008.tif\"))\navg_sst_2009 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2009.tif\"))\navg_sst_2010 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2010.tif\"))\navg_sst_2011 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2011.tif\"))\navg_sst_2012 &lt;- rast(here(\"posts\",\n                          \"2022-12-03_geospatial\",\n                          \"data\",\n                          \"average_annual_sst_2012.tif\"))\n\n#combine into raster stack \nall_sst &lt;- c(avg_sst_2008,\n             avg_sst_2009,\n             avg_sst_2010,\n             avg_sst_2011,\n             avg_sst_2012)\n\n#read in bathymetry raster\ndepth &lt;- rast(here(\"posts\",\n                    \"2022-12-03_geospatial\",\n                    \"data\",\n                    \"depth.tif\"))\n  \n  #re project using the terra way\n  all_sst_reproj &lt;- project(all_sst, depth)\n  \n  #find the mean SST from 2008-2012\n  mean_sst &lt;- mean(all_sst_reproj)\n  \n  #convert sst from K to C\n  mean_sst_c &lt;- mean_sst - 273.15\n  \n  #crop depth rast to match the extent of the SST rast\n  depth_cropped &lt;- crop(depth,mean_sst_c)\n  \n  #using nearest neighbor approach to resample\n  depth_resampled &lt;- resample(depth_cropped,\n                              mean_sst_c,\n                              method = \"near\")\n  \n  #make reclassification matrix\n  rcl_sst &lt;- matrix(c(-Inf, SST_low, NA, SST_low, SST_high,\n                      1, SST_high, Inf, NA), \n                    ncol = 3, byrow = TRUE) \n  \n  \n  #reclassifying sst raster using a reclassification matrix\n  sst_suitable_locs &lt;- classify(mean_sst_c,\n                                rcl = rcl_sst, \n                                include.lowest = TRUE)\n  \n  #reclassifying depth raster using a reclassification matrix\n  rcl_depth &lt;- matrix(c(-Inf, depth_low, NA,\n                        depth_low, depth_high, 1,\n                        depth_high, Inf, NA),\n                      ncol = 3, byrow = TRUE)\n  \n  \n  depth_suitable_locs &lt;- classify(depth_resampled,\n                                  rcl = rcl_depth,\n                                  include.lowest = TRUE)\n  \n  #define function to multiply layers\n  mult_fun &lt;- function(x, y) {\n    return(x*y)}\n  \n  #find locations that satisfy both conditions\n  oyst_suitable_habitat &lt;- lapp(c(sst_suitable_locs, depth_suitable_locs), mult_fun)\n  \n  \n  #find grid cell size\n  grid_cell_size &lt;- cellSize(oyst_suitable_habitat,\n                             mask = TRUE, \n                             transform = TRUE,\n                             unit = \"km\")\n  \n  wc_rasterized &lt;- rasterize(wc_EEZ_regions,\n                             oyst_suitable_habitat,\n                             field = \"rgn\")\n  \n  wc_mask &lt;- mask(wc_rasterized, oyst_suitable_habitat,\n                  updatevalue = NA,\n                  inverse = FALSE)\n  \n  wc_suitable_area &lt;- zonal(grid_cell_size, wc_mask, na.rm = TRUE, sum)\n  \n  wc_suitable_EEZ &lt;- full_join(wc_EEZ_regions, wc_suitable_area, by = \"rgn\") |&gt; \n    mutate(suitable_area = area, #rename area to suitable area \n           percentage_suitable = (suitable_area/area_km2 * 100), #percentage of suitable area\n           .before = geometry)\n  \n  ##make maps \n  \n  #set to interactive viewing \n  tmap_mode(\"view\")\n  \n  #total area\n  viz_total_area &lt;- tm_basemap(\"Stamen.Terrain\")+\n   tm_shape(wc_suitable_EEZ) +\n    tm_polygons(col = 'area',\n               palette = 'Oranges',\n                alpha = 0.75,\n                border.col = 'black',\n               title = paste0(\"Total Suitable Area for \", spp_name ,\" in West Coast EEZs\")) +\n    tm_text(\"rgn\", size = 0.54) \n \n   viz_total_area\n  \n  #percentage suitable\n  viz_percentage &lt;- tm_basemap(\"Stamen.Terrain\") +\n    tm_shape(wc_suitable_EEZ) +\n    tm_polygons(col = 'percentage_suitable',\n                palette = 'Purples',\n                alpha = 0.75,\n                border.col = 'black',\n                title = paste0(\"Percentage of Suitable Area for \", spp_name ,\" in West Coast EEZs\")) +\n    tm_text(\"rgn\", size = 0.54) \n  \n  viz_percentage\n  \n  tmap_arrange(viz_total_area, viz_percentage)\n}\n\n\nI chose the Red Abalone, Haliotis rufescens. https://www.sealifebase.ca/summary/Haliotis-rufescens.html\n\n\nShow code\nabalone_habitats &lt;- find_suitable_locs(8, 18, 0, 24, \"Red Abalone\")\n\n\nReading layer `wc_regions_clean' from data source \n  `C:\\Users\\kiran\\Documents\\MEDS 2022-2023\\kiranfavre.github.io\\posts\\2022-12-03_geospatial\\data\\wc_regions_clean.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -129.1635 ymin: 30.542 xmax: -117.097 ymax: 49.00031\nGeodetic CRS:  WGS 84\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n\ntmap mode set to interactive viewing\n\n\nShow code\nabalone_habitats"
  },
  {
    "objectID": "posts/2022-12-08_EDS222_final/index.html",
    "href": "posts/2022-12-08_EDS222_final/index.html",
    "title": "Analysis of California Coastal Erosion",
    "section": "",
    "text": "Coastal erosion is a natural process of sediment removal that occurs due to storms and waves. Climate change has caused sea levels to rise, increasing concerns for possible effects on the coastlines. While this is a natural process, some coastal communities could be facing more coastal erosion than others considering the level of development along the coast. As of 2015, approximately 68% (26.3 million people) of California residents live in a coastal area (within approximately half a mile of the mean high water line) and have likely witnessed some level of coastal erosion. California’s coastlines are also susceptible to erosion due to its tectonic activity and exposure to extreme weather events such as ENSO.\nEfforts to mitigate and adapt to erosion along the coastline have been made as concerns have rised for the displacement of communities and general safety. The Department of Boating & Waterways is responsible for responding to coastal erosion in California. In areas in need of attention, such as where roads or homes collapse off of cliff tops or beach front properties being flooded, there are different levels of responses depending on the intensity of erosion. Sand replenishment and seawall or jetty installation are currently used to maintain the current infrastructure along the coastlines. This type of analysis can help in evaluating areas that need different solutions depending on the level of shoreline change."
  },
  {
    "objectID": "posts/2022-12-08_EDS222_final/index.html#motivation",
    "href": "posts/2022-12-08_EDS222_final/index.html#motivation",
    "title": "Analysis of California Coastal Erosion",
    "section": "",
    "text": "Coastal erosion is a natural process of sediment removal that occurs due to storms and waves. Climate change has caused sea levels to rise, increasing concerns for possible effects on the coastlines. While this is a natural process, some coastal communities could be facing more coastal erosion than others considering the level of development along the coast. As of 2015, approximately 68% (26.3 million people) of California residents live in a coastal area (within approximately half a mile of the mean high water line) and have likely witnessed some level of coastal erosion. California’s coastlines are also susceptible to erosion due to its tectonic activity and exposure to extreme weather events such as ENSO.\nEfforts to mitigate and adapt to erosion along the coastline have been made as concerns have rised for the displacement of communities and general safety. The Department of Boating & Waterways is responsible for responding to coastal erosion in California. In areas in need of attention, such as where roads or homes collapse off of cliff tops or beach front properties being flooded, there are different levels of responses depending on the intensity of erosion. Sand replenishment and seawall or jetty installation are currently used to maintain the current infrastructure along the coastlines. This type of analysis can help in evaluating areas that need different solutions depending on the level of shoreline change."
  },
  {
    "objectID": "posts/2022-12-08_EDS222_final/index.html#data",
    "href": "posts/2022-12-08_EDS222_final/index.html#data",
    "title": "Analysis of California Coastal Erosion",
    "section": "DATA",
    "text": "DATA\nThe data used for this analysis was produced by the Coastal and Marine Hazards and Resources Program with the United States Geological Survey. This data is publicly available through the USGS ScienceBase-Catalog in a data release from the Pacific Coastal and Marine Science Center. The spatial range spans the California coast, given in degrees latitude and longitude and Universal Transverse Mercator (UTM). California is broken into three regions: Northern, Central, and Southern. \nThe dataset I used includes shoreline change data along the coast of California from 2015-2016. Net shoreline movement, in meters, serves as the metric of shoreline change and what I will be observing. This data was compiled to observe changes in the shoreline in response to extreme weather, as it was collected during an El Niño event (2015 marking ‘before an El Niño’ and 2016 marking ‘after an El Niño’). Net shoreline movement was ’calculated at a transect spacing of 50 meters as a proxy for sandy shoreline change throughout the El Nino winter season.\nA limitation in this data is that I am using observations derived from USGS’s DSAS and Light Detection and Ranging (LiDAR) digital elevation models (DEMs), where these may have used their own assumptions that could introduce bias into this data set. Another limitation is the unequal distribution of observations per region: Southern California has many more observations than Northern or Central California, so an analysis on Southern California may be more representative of the population than the other regions studied.\nTo explore the data, I made histograms to understand the distribution of net shoreline movement in each region of California. The data is approximately normally distributed in all of California, with the most normal distribution in Southern California.\n\n\nShow code\n#read in 2015 data\nCA_2015_shoreline &lt;- st_read(file.path(rootdir,\n                       \"data\",\n                       \"CA_shoreline_changes\",\n                       \"2015_2016_shoreline_changes\",\n                       \"CA_2015_2016_shoreline_change.shp\"))\n\n\n\n\nShow code\n#has only norcal observations\nnorcal_NSM &lt;- CA_2015_shoreline |&gt;\n  filter(Region == \"n\") |&gt; \n  dplyr::select(NSM, Lat, Long) |&gt; \n  na.omit()\n\n\n#has only cencal observations\ncencal_NSM &lt;- (CA_2015_shoreline) |&gt;\n  filter(Region == \"c\") |&gt; \n  dplyr::select(NSM, Lat, Long) |&gt; \n  na.omit()\n\n\n#has only socal observations\nsocal_NSM &lt;- (CA_2015_shoreline) |&gt; \n  filter(Region == \"s\") |&gt; \n  dplyr::select(NSM, Lat, Long) |&gt; \n  na.omit()\n\n\n\n\nShow code\n#Norcal\nnorcal_NSM_hist_n &lt;- ggplot(norcal_NSM, aes(x = NSM)) +\n  geom_histogram(bins = 50,\n                 fill = \"skyblue\") +\n  labs(x = \"Net Shoreline Movement (m)\",\n       y = \"Count\",\n       title = \"Distribution of Net Shoreline Movement in Northern California\",\n       ) +\n  theme_minimal() +\n  theme(plot.title = element_text(size=10), \n        axis.title = element_text(size = 7))  \n\nnorcal_NSM_hist_n\n\n\n\n\n\n\n\nShow code\n#Cencal\ncencal_NSM_hist &lt;- ggplot(cencal_NSM, aes(x = NSM)) +\n  geom_histogram(bins = 50,\n                 fill = \"skyblue\") +\n  labs(x = \"Net Shoreline Movement (m)\",\n       y = \"Count\",\n       title = \"Distribution of Net Shoreline Movement in Central California\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=10),\n        axis.title = element_text(size = 7)) \n\ncencal_NSM_hist\n\n\n\n\n\n\n\nShow code\n#Socalsocal_NSM_hist\nsocal_NSM_hist &lt;- ggplot(socal_NSM, aes(x = NSM)) +\n  geom_histogram(bins = 50,\n                 fill = \"skyblue\") +\n  labs(x = \"Net Shoreline Movement (m)\",\n       y = \"Count\",\n       title = \"Distribution of Net Shoreline Movement in Southern California\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=10), \n        axis.title = element_text(size = 7))  \n\nsocal_NSM_hist"
  },
  {
    "objectID": "posts/2022-12-08_EDS222_final/index.html#analysis",
    "href": "posts/2022-12-08_EDS222_final/index.html#analysis",
    "title": "Analysis of California Coastal Erosion",
    "section": "ANALYSIS",
    "text": "ANALYSIS\nTo understand if there is a relationship between location along the coast and net shoreline movement, I chose to analyze net shoreline movement by the three study regions: Northern, Central, and Southern California. I will begin with hypothesis testing to determine whether the mean net shoreline movement is the same in each region. The null hypothesis is that each region has the same average net shoreline movement. The alternative hypothesis is that each region does not have the same average net shoreline movement.\n\\[\nH_0:\\mu_{Norcal} - \\mu_{Cencal} - \\mu_{Socal} = 0,\nH_1: \\mu_{Norcal} - \\mu_{Cencal} - \\mu_{Socal} \\neq 0\n\\]\nI calculated the average of each region, and found that the average net shoreline movement is not the same across regions. The average net shoreline movement was least in Central California and most in Southern California. A positive value for net shoreline movement indicates an extending coastline, where a negative value indicates erosion, so Central California is estimated to be experiencing the most erosion while Southern California is estimated to be experiencing the least.\n\n\nShow code\nnorcal_mean_NSM &lt;- mean(norcal_NSM$NSM)\n\ncencal_mean_NSM &lt;- mean(cencal_NSM$NSM)\n\nsocal_mean_NSM &lt;- mean(socal_NSM$NSM)\n\n#make df to plot\nRegion &lt;- c(\"Northern CA\",\n            \"Central CA\",\n            \"Southern CA\")\nMean_NSM &lt;- c(\"-25.512\", \"-45.702\", \"-9.743\")\n\ndf &lt;- data.frame(Region, Mean_NSM)\nprint(df)\n\n\n       Region Mean_NSM\n1 Northern CA  -25.512\n2  Central CA  -45.702\n3 Southern CA   -9.743\n\n\nNext, I ran a multiple linear regression of the impact of location on net shoreline movement. Using the equation:\n\\[NSM =\\beta_{0}+\\beta_{1} \\cdot Longitude +\\beta_{2} \\cdot \\text Latitude+\\varepsilon_i\\] I broke the data into the regions, and used this model for each region. Using RStudio, I was able to calculate the values for the coefficients, listed below."
  },
  {
    "objectID": "posts/2022-12-08_EDS222_final/index.html#results",
    "href": "posts/2022-12-08_EDS222_final/index.html#results",
    "title": "Analysis of California Coastal Erosion",
    "section": "RESULTS",
    "text": "RESULTS\nNorthern California\n\n\nShow code\n#make model \nmod_n &lt;- lm(NSM ~ Long + Lat,\n            data = norcal_NSM)\nsummary(mod_n)\n\n#plot longitude vs NSM\nnorcal_mod_lon &lt;- ggplot(data = norcal_NSM,\n       aes(x = Long,\n             y = NSM,\n             color = NSM)) +\n  geom_point() +\n  geom_jitter() +\n  geom_line(data = augment(mod_n), aes(y = .fitted,),\n            color = \"blue\") +\n  labs(x = \"Long\",\n       y = \"NSM\",\n       main = \"Multiple linear regression of NSM ~ Long + Lat\") +\n  theme_minimal() +\n  scale_color_gradient(low = \"purple\", high = \"orange\") +\n  theme(panel.grid = element_blank(),\n        axis.text.x= element_text(size=5.5),\n        axis.title = element_text(size = 8),\n        legend.key.size = unit(0.4, 'cm'),\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 7))\n\n#plot latitude vs NSM \nnorcal_mod_lat &lt;- ggplot(data = norcal_NSM, aes(x = Lat,\n                                        y = NSM,\n                                        color = NSM)) +\n  geom_point() +\n  geom_jitter() +\n  geom_line(data = augment(mod_n), aes(y = .fitted,),\n            color = \"blue\") +\n  labs(x = \"Lat\",\n       y = \"NSM\",\n       main = \"Multiple linear regression of NSM ~ Long + Lat\") +\n  theme_minimal() +\n  scale_color_gradient(low = \"purple\", high = \"orange\") +\n  theme(panel.grid = element_blank(),\n        axis.text.x= element_text(size=5.5),\n        axis.title = element_text(size = 8),\n        legend.key.size = unit(0.4, 'cm'),\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 7))\n\nggarrange(norcal_mod_lon, norcal_mod_lat)\n\n\n\n\n\nCentral California:\n\n\nShow code\n#make model for cencal \nmod_c &lt;- lm(NSM ~ Long + Lat,\n            data = cencal_NSM)\nsummary(mod_c)\n#plot mod on Long vs NSM\ncencal_mod_lon &lt;- ggplot(data = cencal_NSM, aes(x = Long,\n                                        y = NSM,\n                                        color = NSM)) +\n  geom_point() +\n  geom_jitter() +\n  geom_line(data = augment(mod_c), aes(y = .fitted,),\n            color = \"blue\") +\n  labs(x = \"Long\",\n       y = \"NSM\") +\n  theme_minimal() +\n  scale_color_gradient(low = \"purple\", high = \"orange\") +\n  theme(panel.grid = element_blank(),\n        axis.text.x= element_text(size=5.5),\n        axis.title = element_text(size = 8),\n        legend.key.size = unit(0.4, 'cm'),\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 7))\n\n#model on latitude vs NSM\ncencal_mod_lat &lt;- ggplot(data = cencal_NSM, aes(x = Lat,\n                                        y = NSM,\n                                        color = NSM)) +\n  geom_point() +\n  geom_jitter() +\n  geom_line(data = augment(mod_c), aes(y = .fitted,),\n            color = \"blue\") +\n  labs(x = \"Lat\",\n       y = \"NSM\",\n       main = \"Multiple linear regression of NSM ~ Long + Lat\") +\n  theme_minimal() +\n  scale_color_gradient(low = \"purple\", high = \"orange\") +\n  theme(panel.grid = element_blank(),\n        axis.text.x= element_text(size=5.5),\n        axis.title = element_text(size = 8),\n        legend.key.size = unit(0.4, 'cm'),\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 7))\n\n#plot side by side\nggarrange(cencal_mod_lon, cencal_mod_lat)\n\n\n\n\n\nSouthern California:\n\n\nShow code\n#make model for southern CA\nmod_s &lt;- glm(NSM ~ Long + Lat,\n            data = socal_NSM)\nsummary(mod_s)\n\n#plot model on longitude vs NSM\nsocal_mod_lon &lt;- ggplot(data = socal_NSM, aes(x = Long,\n                                        y = NSM,\n                                        color = NSM)) +\n  geom_point() +\n  geom_jitter() +\n  geom_line(data = augment(mod_s), aes(y = .fitted,),\n            color = \"blue\") +\n  labs(x = \"Longitude (degrees)\",\n       y = \"NSM\") +\n  theme_minimal() +\n  scale_color_gradient(low = \"purple\", high = \"orange\") +\n  theme(panel.grid = element_blank(),\n        axis.text.x=element_text(size=5.5),\n        axis.title = element_text(size = 8),\n        legend.key.size = unit(0.4, 'cm'),\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 7))\n\n#model on lat vs NSM\nsocal_mod_lat &lt;- ggplot(data = socal_NSM, aes(x = Lat,\n                                        y = NSM,\n                                        color = NSM)) +\n  geom_point() +\n  geom_jitter() +\n  geom_line(data = augment(mod_s), aes(y = .fitted,),\n            color = \"blue\") +\n  labs(x = \"Latitude(degrees)\",\n       y = \"NSM\") +\n  theme_minimal() +\n  scale_color_gradient(low = \"purple\", high = \"orange\") +\n  theme(panel.grid = element_blank(),\n        axis.text.x= element_text(size=5.5),\n        axis.title = element_text(size = 8),\n        legend.key.size = unit(0.4, 'cm'),\n        legend.text = element_text(size = 6),\n        legend.title = element_text(size = 7)) \n\n#plot side by side\nggarrange(socal_mod_lon, socal_mod_lat)\n\n\n\n\n\nNorthern California Regression Model Summary:\n\n\nShow code\nmod_n &lt;- lm(NSM ~ Long + Lat,\n            data = norcal_NSM)\ntab_model(mod_n)\n\n\n\n\n\n \nNSM\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-1284.29\n-1838.13 – -730.45\n&lt;0.001\n\n\nLong\n-10.08\n-15.13 – -5.03\n&lt;0.001\n\n\nLat\n0.28\n-1.68 – 2.23\n0.782\n\n\nObservations\n1524\n\n\nR2 / R2 adjusted\n0.068 / 0.067\n\n\n\n\n\n\n\nCentral California Regression Model Summary:\n\n\nShow code\nmod_c &lt;- lm(NSM ~ Long + Lat,\n            data = cencal_NSM)\ntab_model(mod_c)\n\n\n\n\n\n \nNSM\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n-13353.72\n-16584.15 – -10123.30\n&lt;0.001\n\n\nLong\n-134.26\n-167.45 – -101.07\n&lt;0.001\n\n\nLat\n-82.71\n-104.86 – -60.57\n&lt;0.001\n\n\nObservations\n1033\n\n\nR2 / R2 adjusted\n0.112 / 0.110\n\n\n\n\n\n\n\nSouthern California Regression Model Summary:\n\n\nShow code\nmod_s &lt;- lm(NSM ~ Long + Lat,\n            data = socal_NSM)\ntab_model(mod_s)\n\n\n\n\n\n \nNSM\n\n\nPredictors\nEstimates\nCI\np\n\n\n(Intercept)\n254.24\n186.79 – 321.69\n&lt;0.001\n\n\nLong\n9.95\n8.88 – 11.01\n&lt;0.001\n\n\nLat\n27.11\n25.16 – 29.05\n&lt;0.001\n\n\nObservations\n5616\n\n\nR2 / R2 adjusted\n0.188 / 0.188"
  },
  {
    "objectID": "posts/2022-12-08_EDS222_final/index.html#discussion",
    "href": "posts/2022-12-08_EDS222_final/index.html#discussion",
    "title": "Analysis of California Coastal Erosion",
    "section": "DISCUSSION",
    "text": "DISCUSSION\nAs the results of the hypothesis test show, the average net shoreline movement in each region of California is not equal. Due to this, I reject the null hypothesis that the average net shoreline movement will be equal in across all regions in California.\nThese plots indicate that there is a relationship between location in California and net shoreline movement. For the most part, as longitude decreases (moving eastward), the predicted net shoreline movement decrease (coastal erosion). As latitude increases (moving northward), the predicted net shoreline movement increases (coastal extension). These models estimate that shorelines that are more southeastern are at risk for more coastal erosion.\nThe regression summary tables can further help in understanding the estimated relationship between location and net shoreline movement. The model predicts the following: in Northern California, about 6.8% of the variance in net shoreline movement is due to its latitude and longitude. In Central California, about 11.2% of the variance in net shoreline movement is due to its latitude and longitude. In Southern California, about 18.8% of the variance in net shoreline movement is due to its latitude and longitude. The coefficients represent the magnitude of effect of that variable on net shoreline movement. These differ greatly between each region, which could mean that each region faces different levels of environmental stressors causing differing levels of coastal erosion."
  },
  {
    "objectID": "posts/2022-12-08_EDS222_final/index.html#future-research",
    "href": "posts/2022-12-08_EDS222_final/index.html#future-research",
    "title": "Analysis of California Coastal Erosion",
    "section": "FUTURE RESEARCH",
    "text": "FUTURE RESEARCH\nMore monitoring of this coastline will be essential in forming helpful policy and adaptation strategies. With how dynamic the shoreline is in lieu of natural processes, in addition to anthropogenic influences, it is important to improve the understanding of how the coastline is changing. The effects of climate change are being understood as they happen, which is why improving monitoring is crucial in forming an analysis of coastal erosion.\nAlong with the data collection itself, the model used here could be improved in the future to account for discrepancies here. Another analysis that could be helpful in answering this question would be a time series decomposition analysis. This data did not have time observations, rather metrics already calculated based on the time interval provided. These models can also be applied to other areas in the United States or globally that are facing similar problems. Future analyses could also include spatial interpolation to estimate coastal erosion for all of California. This recent study inspired my suggestion for spatial interpolation as a future analysis.\n\nREFERENCES\n\nBarnard, P.L., Smith, S.A., and Foxgrover, A.C., 2020, California shorelines and shoreline change data, 1998-2016: U.S. Geological Survey data release, https://doi.org/10.5066/P91QSGXF\n“California.” NOAA Office for Coastal Management, https://coast.noaa.gov/states/california.html. \nKoppes, Steve. New High-Resolution Study on California Coastal Cliff Erosion Released, Scripps Institution of Oceanography, 4 Aug. 2022, https://scripps.ucsd.edu/news/new-high-resolution-study-california-coastal-cliff-erosion-released. \nNOAA Office for Coastal Management ADS Group. “Coastal Zone Management Programs.” NOAA Office for Coastal Management | States and Territories Working on Ocean and Coastal Management, https://coast.noaa.gov/czm/mystate/#:~:text=The%20California%20coastal%20zone%20generally,line%20of%20highest%20tidal%20action. \n“U.S. Climate Resilience Toolkit.” Coastal Erosion | U.S. Climate Resilience Toolkit, https://toolkit.climate.gov/topics/coastal-flood-risk/coastal-erosion"
  },
  {
    "objectID": "posts/2023-03-21_calcofi_ml/index.html",
    "href": "posts/2023-03-21_calcofi_ml/index.html",
    "title": "Using Machine Learning for Ocean Chemistry Prediction",
    "section": "",
    "text": "For our final project in my machine learning course, we participated in a Kaggle competition to predict the concentration of dissolved inorganic carbon in water samples by using ocean chemistry data. This data comes from the California Cooperative Oceanic Fisheries Investigations (CalCOFI) program."
  },
  {
    "objectID": "posts/2023-03-21_calcofi_ml/index.html#background",
    "href": "posts/2023-03-21_calcofi_ml/index.html#background",
    "title": "Using Machine Learning for Ocean Chemistry Prediction",
    "section": "",
    "text": "For our final project in my machine learning course, we participated in a Kaggle competition to predict the concentration of dissolved inorganic carbon in water samples by using ocean chemistry data. This data comes from the California Cooperative Oceanic Fisheries Investigations (CalCOFI) program."
  },
  {
    "objectID": "posts/2023-03-21_calcofi_ml/index.html#objective",
    "href": "posts/2023-03-21_calcofi_ml/index.html#objective",
    "title": "Using Machine Learning for Ocean Chemistry Prediction",
    "section": "Objective",
    "text": "Objective\nTo predict dissolved inorganic carbon, we will be using a Linear Regression Model in R to make these predictions. We will use the CalCOFI data to train our model to make predictions of inorganic dissolved carbon concentrations in different parts of the ocean that aren’t included in the training data.\nThe variables we are using as predictors in our model are:\n\nNO2uM - Micromoles of Nitrite per liter of seawater\nNO3uM - Micromoles of Nitrate per liter of seawater\nNH3uM - Micromoles of Ammonia per liter of seawater\nR_TEMP - Reported (Potential) Temperature (degrees Celsius)\nR_Depth - Reported Depth from pressure (meters)\nR_Sal - Reported Salinity (from Specific Volume Anomoly, M³ per Kg)\nR_DYNHT - Reported Dynamic Height (work per unit mass)\nR_Nuts - Reported Ammonium concentration (micromoles per Liter)\nR_Oxy_micromol.Kg - Reported Oxygen concentration (micromoles per kilogram)\nPO4uM - Micromoles of Phosphate per liter of seawater\nSiO3uM - Micromoles of Silicate per liter of seawater\nTA1 - Total Alkalinity (micromoles per kilogram solution)\nSalinity1 - Salinity (Practical Salinity Scale 1978)\nTemperature_degC - Temperature (degrees Celsius)"
  },
  {
    "objectID": "posts/2023-03-21_calcofi_ml/index.html#load-and-split-data",
    "href": "posts/2023-03-21_calcofi_ml/index.html#load-and-split-data",
    "title": "Using Machine Learning for Ocean Chemistry Prediction",
    "section": "Load and split data",
    "text": "Load and split data\nTo train machine learning models using a data set, the model must have training data to learn from and test data to compare its predictions to to evaluate model performance. We will then split the training data further into two groups, a validation set and training set. The training set will still be used to train the model, while the validation set will be used to evaluate how well the model performed.\n\n\nShow code\n#Reading in data used to train model\ntraining_data &lt;- read_csv(here(\"posts\",\n                               \"2023-03-21_calcofi_ml\",\n                               \"data\",\n                               \"train.csv\")) %&gt;%\n  clean_names() %&gt;%\n  select(-x13) #remove this since its all NA\n\n#Reading in data that will be used to test model\ntesting_data &lt;- read_csv(here(\"posts\",\n                               \"2023-03-21_calcofi_ml\",\n                               \"data\",\n                               \"test.csv\")) %&gt;%\n  clean_names() %&gt;% \n  mutate(ta1_x = ta1)\n\n\n#split the training data into training and evaluation sets, stratify by dissolved inorganic carbon concentration\ndata_split &lt;- initial_split(training_data,\n                            strata = dic)\n\n#extract training and test data from the training data\ntraining_set &lt;- training(data_split)\nevaluation_set &lt;- testing(data_split)\n\n#take a look at training and testing data \nhead(training_data)\nhead(evaluation_set)"
  },
  {
    "objectID": "posts/2023-03-21_calcofi_ml/index.html#pre-processing-data-creating-recipe-creating-models-and-creating-workflow",
    "href": "posts/2023-03-21_calcofi_ml/index.html#pre-processing-data-creating-recipe-creating-models-and-creating-workflow",
    "title": "Using Machine Learning for Ocean Chemistry Prediction",
    "section": "Pre-Processing Data, Creating Recipe, Creating Models, and Creating Workflow",
    "text": "Pre-Processing Data, Creating Recipe, Creating Models, and Creating Workflow\nTo pre-process the data for our model, we begin by creating a recipe where dissolved inorganic carbon concentration is the predicted value and all the variables mentioned above as the predictors.\n\n\nShow code\n#set seed for reproducibility\nset.seed(711)\n\n#creating a recipe\nbottle_recipe &lt;- recipe(dic ~.,\n                        data = training_set) %&gt;% \n  step_dummy(all_nominal(),\n             -all_outcomes(),\n             one_hot = TRUE) %&gt;% \n  step_normalize(all_numeric(),\n                 -all_outcomes()) %&gt;% \n  prep()\n\n#creating model specification of linear regression\nbottle_model &lt;- linear_reg() %&gt;% \n  set_engine(\"lm\") %&gt;% \n  set_mode(\"regression\")\n\n#bundle recipe and model spec into a workflow\nbottle_wf &lt;- workflow() %&gt;% \n  add_recipe(bottle_recipe) %&gt;% \n  add_model(bottle_model)"
  },
  {
    "objectID": "posts/2023-03-21_calcofi_ml/index.html#fit-model-to-training-data-and-make-predictions",
    "href": "posts/2023-03-21_calcofi_ml/index.html#fit-model-to-training-data-and-make-predictions",
    "title": "Using Machine Learning for Ocean Chemistry Prediction",
    "section": "Fit Model to Training Data and Make Predictions",
    "text": "Fit Model to Training Data and Make Predictions\n\n\nShow code\n#creating and training a model on the training data\nfit_bottle &lt;- bottle_wf %&gt;%\n  fit(training_set)\n\n#using the model to make predictions on the validation data   \nbottle_results &lt;- fit_bottle %&gt;% \n  predict(evaluation_set) %&gt;%\n  bind_cols(evaluation_set) %&gt;% \n  mutate(dic_prediction = .pred_res) %&gt;% \n  relocate(dic,\n           .before = id) %&gt;% \n  relocate(dic_prediction,\n           .before = id) %&gt;% \n  select(-.pred_res)\n\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from rank-deficient fit; attr(*, \"non-estim\") has\ndoubtful cases\n\n\nShow code\n#retrieve and evaluate our predictions\nbottle_metrics &lt;- bottle_results %&gt;%\n  metrics(estimate = dic_prediction,\n          truth = dic)\n\nbottle_metrics\n\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       6.82 \n2 rsq     standard       0.996\n3 mae     standard       3.45"
  },
  {
    "objectID": "posts/2023-03-21_calcofi_ml/index.html#test-model",
    "href": "posts/2023-03-21_calcofi_ml/index.html#test-model",
    "title": "Using Machine Learning for Ocean Chemistry Prediction",
    "section": "Test Model",
    "text": "Test Model\n\n\nShow code\n## Outputting predictions for our testing data\ntest_data_predictions &lt;- fit_bottle %&gt;% \n  predict(testing_data) %&gt;%\n  bind_cols(testing_data) %&gt;% \n  mutate(DIC = .pred_res) %&gt;% \n  relocate(DIC,\n           .before = id) %&gt;% \n  select(id,\n         DIC)\n\n\nWarning in predict.lm(object = object$fit, newdata = new_data, type =\n\"response\"): prediction from rank-deficient fit; attr(*, \"non-estim\") has\ndoubtful cases\n\n\nShow code\ntest_data_predictions\n\n\n# A tibble: 485 × 2\n      id   DIC\n   &lt;dbl&gt; &lt;dbl&gt;\n 1  1455 2173.\n 2  1456 2194.\n 3  1457 2325.\n 4  1458 1993.\n 5  1459 2147.\n 6  1460 2036.\n 7  1461 2159.\n 8  1462 2196.\n 9  1463 2270.\n10  1464 2314.\n# ℹ 475 more rows"
  },
  {
    "objectID": "posts/2023-05-08_fire_linreg/index.html",
    "href": "posts/2023-05-08_fire_linreg/index.html",
    "title": "Understanding the Relationship Between Temperature & Forest Area Burned",
    "section": "",
    "text": "In this blog, I want to evaluate whether weather has an influence on forest fires. I have wondered about this considering the wildfires in my region in 2020)."
  },
  {
    "objectID": "posts/2023-05-08_fire_linreg/index.html#introduction",
    "href": "posts/2023-05-08_fire_linreg/index.html#introduction",
    "title": "Understanding the Relationship Between Temperature & Forest Area Burned",
    "section": "",
    "text": "In this blog, I want to evaluate whether weather has an influence on forest fires. I have wondered about this considering the wildfires in my region in 2020)."
  },
  {
    "objectID": "posts/2023-05-08_fire_linreg/index.html#data",
    "href": "posts/2023-05-08_fire_linreg/index.html#data",
    "title": "Understanding the Relationship Between Temperature & Forest Area Burned",
    "section": "Data",
    "text": "Data\nThe data I will be using is a data set of daily forest fire area burned in the northeast region of Portugal and meteorological conditions on the recorded day (data was constructed from here)."
  },
  {
    "objectID": "posts/2023-05-08_fire_linreg/index.html#exploration",
    "href": "posts/2023-05-08_fire_linreg/index.html#exploration",
    "title": "Understanding the Relationship Between Temperature & Forest Area Burned",
    "section": "Exploration",
    "text": "Exploration\n\nVisualizing the data\nTo begin to understand this relationship, I will make a scatter plot showing area burned as it relates to temperature to determine the general relationship.\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nhere() starts at C:/Users/kiran/Documents/MEDS 2022-2023/kiranfavre.github.io\n\n\n\n\nShow code\nforestfires_data &lt;- readRDS(\"~/MEDS 2022-2023/kiranfavre.github.io/posts/2023-05-08_fire_linreg/forestfires.rds\")\n\nggplot(forestfires_data,\n       aes(x = temp,\n           y = area)) +\n  geom_point(col = \"purple\") +\n  labs(x = \"Temperature (degrees Celsius)\",\n       y = \"Daily Forest Fire Area Burned (hectares)\",\n       title = \"Temperature vs. Daily Forest Fire Area Burned\") +\n  theme_minimal()\n\n\n\n\n\nHere I see a moderate positive correlation, which intuitively makes sense to me. If temperature is higher, fires that may not have continued to burn may keep burning for longer, covering a larger area. The correlation is not very strong, but I would say there is a positive correlation nonetheless.\n\n\nEstimating the relationship with simple linear regression\nUsing the lm() command to estimate the following simple linear regression:\n\\[ \\text{area_burned}_i = \\beta_0 + \\beta_1 \\text{temp}_i + \\varepsilon_i \\]\nUsing this relationship, I will look into:\n\nHow many hectares are predicted to burn on a day that is 1 degrees Celsius?\nHow many hectares are predicted to burn on a day that is 28 degrees Celsius?\nHow many more hectares are predicted to burn on a day in winter at 12 degrees Celsius as compared to a hot summer day at 30 degrees Celsius?\n\n\n\nShow code\nlinreg &lt;- lm(area ~ temp, data = forestfires_data) |&gt; \n  summary() |&gt; #summarize\n  xtable() |&gt; #beautify\n  kable() \n\nprint(linreg)\n\n\n\n\n|            | Estimate| Std. Error|    t value| Pr(&gt;&#124;t&#124;)|\n|:-----------|--------:|----------:|----------:|------------------:|\n|(Intercept) | 14.77689| 17.1649949|  0.8608733|          0.3897084|\n|temp        | 12.25678|  0.8686797| 14.1096681|          0.0000000|\n\n\n\n\nShow code\n#one degree celsius day\narea_1C &lt;- 14.77689 + 12.2578\narea_1C\n\n\n[1] 27.03469\n\n\nShow code\n#28 degrees Celsius\narea_28C &lt;- 14.77689 + (12.2578 * 28)\narea_28C\n\n\n[1] 357.9953\n\n\nShow code\n#how many more hectares will be burned at 30 degrees vs 12\narea_diff &lt;- (14.77689 + (12.2578 * 30)) - (14.77689 + (12.2578 * 12))\narea_diff\n\n\n[1] 220.6404\n\n\nThis model predicts that at zero degrees temperature, there would be 14.78 hectares of burned area, and that for a one degree increase in temperature, the burn area will increase by 12.26 hectares. On a day that is one degree Celsius, this model predicts that there will be 27.03 hectares of burned area. On a 28 degrees Celsius day, the model predicts that 358 hectares of forest would be burned. This model predicts that there will be 220.6 more hectares burned on a 30 degree day than a 12 degree day.\n\n\nModel fit\n\n\nShow code\nggplot(data = forestfires_data,\n       aes(temp, area)) +\n  geom_smooth(se = FALSE,\n              col = \"red\") +\n  geom_point(col = \"blue\",\n             alpha = 0.25) +\n  labs(x = \"Temperature (degrees Celsius)\",\n       y = \"Daily Forest Fire Area Burned (hectares)\",\n       title = \"Temperature vs. Daily Forest Fire Area Burned\")\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\nI think the model is moderately accurate at fitting the data - it does accurately predict the moderate positive correlation that I saw from the scatter plot. The points are not super tight/close to the line but are also not very far (moderate residuals), and there are not many outliers except for at very high temperatures.\n\n\nCoefficient of Determination\nThe coefficient of determination is the percentage of variance in y that is explained by x (and any other independent variables). Therefore, computing the coefficient of determination (\\(R^2\\)) will allow me to predict what percent of variation in area burned are explained by temperature.\n\n\nShow code\n#need to do the regression without the table making\nregression &lt;- lm(area ~ temp, data = forestfires_data)\n\nR2 = summary(regression)$r.squared\nprint(R2)\n\n\n[1] 0.2787951\n\n\nApproximately 28% of variation in daily forest fire area burned in hectares can be attributed to a change in temperature. This does align with my intuition that the model is a moderate fit for this relationship. This coefficient of determination accounts for the positive correlation that I noticed and seems to account for some of the higher residuals in the middle to high values of temperature.\n\n\nOmitted Variable Bias\nDue to complex climatological phenomena, days with high temperatures tend to coincide with days that are also different in other dimensions. For example, hot days tend to be less rainy, with lower wind, and of higher or lower humidity, depending on the geographic location. This raises the concern of omitted variables bias, as these variables may also be correlated with area burned.\nTo address this, I will add relative humidity (RH) as an independent variable to my linear regression model, in addition to temperature.\n\n\nShow code\nRH_linreg &lt;- lm(area ~ temp + RH, data =forestfires_data) |&gt; \n  summary() |&gt; \n  xtable() |&gt; \n  kable() \n\nprint(RH_linreg)\n\n\n\n\n|            |    Estimate| Std. Error|   t value| Pr(&gt;&#124;t&#124;)|\n|:-----------|-----------:|----------:|---------:|------------------:|\n|(Intercept) | -15.5334175| 31.3949740| -0.494774|          0.6209711|\n|temp        |  12.8782466|  1.0221028| 12.599756|          0.0000000|\n|RH          |   0.4193295|  0.3637187|  1.152895|          0.2494893|\n\n\nAdding relative humidity to thie model changed the intercept drastically, but the coefficient on temperature did not change much. For every one degree Celsius increase, keeping relative humidity constant, the burn are will increase by 12.88 hectares. In this model without relative humidity, the slope coefficient was 12.26, so an increase of 0.62 acres is not enough to convince me that there is an omitted variable bias. Adding in relative humidity does not increase burned area greatly, which it would if this was a case of omitted variable bias."
  },
  {
    "objectID": "posts/2023-09-06-houston_spatial_analysis/index.html",
    "href": "posts/2023-09-06-houston_spatial_analysis/index.html",
    "title": "Spatial Analysis of Houston Power Outages",
    "section": "",
    "text": "The state of Texas experienced significant power loss following three severe winter storms in February 2021, affecting many families and communities in the state. More background on this can be found on Wikipedia.\nI will be estimating the number of homes in Houston that lost power due to these storms, and investigating if socioeconomic factors were predictors of community recovery from a power outage. To do so, I will be working with raster and vector data, and joining spatial data."
  },
  {
    "objectID": "posts/2023-09-06-houston_spatial_analysis/index.html#introduction",
    "href": "posts/2023-09-06-houston_spatial_analysis/index.html#introduction",
    "title": "Spatial Analysis of Houston Power Outages",
    "section": "",
    "text": "The state of Texas experienced significant power loss following three severe winter storms in February 2021, affecting many families and communities in the state. More background on this can be found on Wikipedia.\nI will be estimating the number of homes in Houston that lost power due to these storms, and investigating if socioeconomic factors were predictors of community recovery from a power outage. To do so, I will be working with raster and vector data, and joining spatial data."
  },
  {
    "objectID": "posts/2023-09-06-houston_spatial_analysis/index.html#data",
    "href": "posts/2023-09-06-houston_spatial_analysis/index.html#data",
    "title": "Spatial Analysis of Houston Power Outages",
    "section": "Data",
    "text": "Data\n\nNight lights\nI will be using remotely-sensed data on night lights, gathered from the Visible Infrared Imaging Radiometer Suite (VIIRS), which is on the Suomi satellite. Specifically, I will be accessing the VNP46A1 to detect the difference in night lights prior to and following the storms to identify areas that lost electricity due to the storms. I accessed the VIIRS data through NASA’s Level-1 and Atmospheric Archive & Distribution System Distributed Active Archive Center (LAADS DAAC).\nI will be using data from February 7, 2021 and February 16, 2021. After exploring data on NASA Worldview, these two dates have a low amount of cloud cover compared to surrounding dates, giving two clear, but contrasting imaged to understand the areas that experienced a power outage in Texas.\n\n\nRoads\nI used Geofabrik’s download sites to retrieve a shapefile of all highways in Texas and prepared a Geopackage (.gpkg file) containing just the subset of roads that intersect the Houston metropolitan area This is a third party company that redistributes OSM Data, as ingesting this data into a database where it can be subsetted and processed is a large undertaking. OpenStreetMap (OSM) is a collaborative project which creates publicly available geographic data of the world and is where the original data is gathered.\nTypically highways account for a large portion of the night lights observable from space (see Google’s Earth at Night). To minimize falsely identifying areas with reduced traffic as areas without power, I will be ignoring areas close to highways.\n\n\nHouses\nOpenStreetMap also has building data. I again downloaded from Geofabrick and prepared a file containing only houses in the Houston metropolitan area.\n\n\nSocioeconomic\nSince I can’t readily access all socioeconomic information for every home in the Houston area, I obtained data from the U.S. Census Bureau’s American Community Survey for census tracts in 2019."
  },
  {
    "objectID": "posts/2023-09-06-houston_spatial_analysis/index.html#spatial-analysis",
    "href": "posts/2023-09-06-houston_spatial_analysis/index.html#spatial-analysis",
    "title": "Spatial Analysis of Houston Power Outages",
    "section": "Spatial Analysis",
    "text": "Spatial Analysis\n\n1. Finding locations of blackouts\n\nCombining the data\nHere, I will start with loading in the appropriate packages and reading in the night light tiles I downloaded. Then, I will combine the tiles into a single object for each date (one before the storm, one after the storm).\n\n\nShow code\nlibrary(sf) #Support for simple features\nlibrary(stars) #Manipulating spatiotemporal arrays \nlibrary(dplyr) #Data manipulation\nlibrary(tmap) #map making\nlibrary(ggplot2) #plot making\nlibrary(here) #path construction\nlibrary(terra) #spatial analysis of vector data\n\n#read in night light tiles\nnightlights_pic1 &lt;- read_stars(here(\"posts/2023-09-06-houston_spatial_analysis/data/VNP46A1/VNP46A1.A2021038.h08v05.001.2021039064328.tif\"))\n\nnightlights_pic2 &lt;- read_stars(here(\"posts/2023-09-06-houston_spatial_analysis/data/VNP46A1/VNP46A1.A2021038.h08v06.001.2021039064329.tif\"))\n\nnightlights_pic3 &lt;- read_stars(here(\"posts/2023-09-06-houston_spatial_analysis/data/VNP46A1/VNP46A1.A2021047.h08v05.001.2021048091106.tif\"))\n\nnightlights_pic4 &lt;- read_stars(here(\"posts/2023-09-06-houston_spatial_analysis/data/VNP46A1/VNP46A1.A2021047.h08v06.001.2021048091105.tif\"))\n\n\n\n\nShow code\n#combine into one stars object for each day\n\n#2021-02-07\nnight_tiles_2021_02_07 &lt;- st_mosaic(nightlights_pic1, nightlights_pic2)\n\n#2021-02-16\nnight_tiles_2021_02_16 &lt;- st_mosaic(nightlights_pic3, nightlights_pic4)\n\n\n\n\nCreating a blackout mask\nMasking is a subsetting technique for spatial data. Creating a mask identifies cells with information I want to know, make that its own layer, and then can be applied to the other raster layer.\nI am finding the difference in intensity of night lights between the two dates I have spatial data for. I am making an assumption that the difference is caused by the storm. I have to reclassify the difference raster to create the mask, so I make an assumption that anywhere that had a difference of 200 nW cm-2sr-1 (unit of radiance) or more experienced a blackout. Then, I will assign NA all locations that experienced a difference of less than this threshold, since the difference is not great enough to indicate a power outage.\n\n\nShow code\n#find the change in night lights intensity caused by storm by subtracting the Feb 7 raster from Feb 16 raster\nintensity_diff &lt;- night_tiles_2021_02_16 - night_tiles_2021_02_07\n\nplot(intensity_diff,\n     main = \"Change in night lights intensity due to storm\")\n\n\n\n\n\nFigure 1. First step in creating blackout mask is to find the difference in intensity of lights from before and after the storm.\n\n\n\n\nShow code\n#start by reclassifying intensity diff raster to recognize &gt;200 as a blackout\nmask &lt;- cut(intensity_diff, c(200, Inf), labels = c(\"Blackout\"))\n\n#reassign raster to assign NA to all locations of less than 200 n\nmask[mask &lt; 200] = NA\n\n#blackout mask with &lt;200 = NA and &gt;200 = blackout\nplot(mask, breaks = \"equal\", col = \"black\") \n\n\n\n\n\nFigure 2. Blackout mask, where &lt;200 nW cm-2sr-1 = NA and &gt;200 nW cm-2sr-1 = blackout\n\n\n\n\nVectorize the mask\nNow, I will use st_as_sf() from the sf package to vectorize the blackout mask so I can continue working with the mask I made, ultimately being able to overlay it with other data and determine where the blackouts occurred. I also will use st_make_valid from the same package to fix any invalid geometries.\n\n\nShow code\nvector_mask &lt;- st_as_sf(mask) |&gt; #vectorize sf mask\n  st_make_valid() #fix invalid geoms\n\nplot(vector_mask)\n\n\n\n\n\nFigure 3. Vectorized blackout mask\n\n\n\n\nCropping the vectorized map to our region of interest\nI am interest in the Houston Metropolitan area, and will be defining this area by the following coordinates:\n(-96.5, 29), (-96.5, 30.5), (-94.5, 30.5), (-94.5, 29)\nNow that I have the coordinates of interest, I will turn them into a polygon using st_polygon function from the sf package. A polygon is a set of discrete spatial points, composed of vertices of many spatial coordinates.\nThen, I will convert the polygon into a simple feature collection using st_sfc() and assign a coordinate reference system (CRS). I am assigning a CRS since the polygon’s CRS must match that of the night lights data I downloaded if I want to join them. So, I will reproject the cropped blackout data to EPSG:3083 (NAD83 / Texas Centric Albers Equal Area).\nThen, I will spatially subset/crop the blackout mask I created to the Houston region I defined as a polygon.\n\n\nShow code\n# define coords \nne &lt;- c(-94.5, 30.5) \nse &lt;- c(-94.5, 29) \nsw &lt;- c(-96.5, 29)\nnw &lt;- c(-96.5, 30.5) \n\n# st_polygon input (since we can't just put these points in by themselves, input has to be a list) and we will make polygon of Houston using st_polygons\nhouston_boundary &lt;- list(rbind(sw, nw, ne, se, sw))\n\n# polygon to represent Houston\nhouston_points &lt;- st_polygon(x = houston_boundary, if (length(x)) \"XYZ\" else \"XY\")\n\n# Convert houston to a simple feature collection to feed in into st_crop\nhouston_sf &lt;- st_sfc(houston_points, crs = 4326)\n\n# Crop the blackout mask to houston area\nblackout_cropped &lt;- st_crop(vector_mask, houston_sf)\n\n# Reproject cropped blackout mask onto crs of interest (3083)\nreproj_houst_blackout &lt;- st_transform(blackout_cropped, crs = 3083)\n\nplot(reproj_houst_blackout)\n\n\n\n\n\nFigure 4. Cropped blackout mask overlaid in Houston region\n\n\n\n\nExcluding highways from blackout mask\nThe data I downloaded for the roads includes data on roads other than highways. So, to avoid reading in data I won’t be using, I will take advantage of the st_read function’s ability to subset using a SQL query.\nTo do so, I will start by defining a SQL query and load only the highway data using this query and st_read. I then have to reproject this data to the same CRS I used earlier (EPSG:3038).\nTo identify areas that are within 200 meters of all highways, I will use st_buffer that will compute a polygon that represents all points this range. This will allow me to find the areas that experienced blackouts that are further than 200 meters from a highway (to eliminate the lights produced by street lamps and other lights along highways that are detected).\n\n\nShow code\n#define a SQL query\nquery &lt;- \"SELECT * FROM gis_osm_roads_free_1 WHERE fclass='motorway'\"\n\n#load highway data from geopackage  THIS PART NEEDS TO BE FIXED!!!!!!!!!!\nhighways &lt;- st_read(\"C:/Users/kiran/Documents/MEDS 2022-2023/kiranfavre.github.io/posts/2023-09-06-houston_spatial_analysis/data/gis_osm_roads_free_1.gpkg\", query = query)\n\n#reproject to EPSG 3083\nreproj_highways &lt;- st_transform(highways, crs = 3083)\n\n#identify areas within 200m of all highways\nhighway_buffers &lt;- st_buffer(reproj_highways, dist = 200) |&gt; \n  st_union()\n\n#identify areas that are further away than 200, from a highway that had a blackout\noutside_hwy_buffers &lt;- st_disjoint(x = highway_buffers, y = reproj_houst_blackout) \n\n\n\n\n\n2. Finding homes impacted by blackouts\n\nLoading buildings data\nTo find homes impacted by blackouts, I will start by loading in the building dataset and use another SQL query to only select residential buildings. As I did with the earlier data, I will also reproject this data to the same CRS.\n\n\nShow code\n#make sql query for building data\nbldg_query &lt;- \"SELECT * FROM gis_osm_buildings_a_free_1 WHERE type IS NULL AND name IS NULL OR type in ('residential', 'apartments', 'house', 'static_caravan', 'detached')\"\n\n#load in building data\nbuildings &lt;- st_read(\"C:/Users/kiran/Documents/MEDS 2022-2023/kiranfavre.github.io/posts/2023-09-06-houston_spatial_analysis/data/gis_osm_buildings_a_free_1.gpkg\", query = bldg_query)\n\n#reproject building data ro crs EPSG:3083\nreproj_buildings &lt;- st_transform(buildings, crs = 3083)\n\n\n\n\nFinding homes in blackout areas\nNow, I will filter the data to homes within the blackout area by using the blackout mask I created and overlay that onto the data on Houston residential buildings, and count the number of impacted homes.\n\n\nShow code\n#use blackout mask, overlay onto houston homes\nblackout_homes &lt;- st_join(reproj_houst_blackout, reproj_buildings)\n\n#filter houston with blackout mask\nnumber_blackout_homes &lt;- blackout_homes |&gt; \n  filter(fclass == \"building\") |&gt;  #filtering for homes\n  summarize(count_blackout_homes = n())  #give number of homes that were impacted by blackout\n\n#number of homes in blackout area\nprint(paste0(\"There were \" ,number_blackout_homes$count_blackout_homes, \" homes in Houston that were impacted by the blackout\",\".\"))\n\n\n\n\n\n3. Investigating socioeconomic factors\n\nLoading Census data\nI will again use st_read() to load the data, and separate the layers from the geodatabase into geometries and income data. I will select the median income field from this layer, and also reproject this data.\n\n\nShow code\n#ACS geodatabase geomoetries\ntexas_geoms &lt;- st_read(dsn = \"C:/Users/kiran/Documents/MEDS 2022-2023/kiranfavre.github.io/posts/2023-09-06-houston_spatial_analysis/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n                       layer = \"ACS_2019_5YR_TRACT_48_TEXAS\")\n\n#read in income layer from geometries\nincome_data &lt;- st_read(dsn = \"C:/Users/kiran/Documents/MEDS 2022-2023/kiranfavre.github.io/posts/2023-09-06-houston_spatial_analysis/data/ACS_2019_5YR_TRACT_48_TEXAS.gdb\",\n                       layer = \"X19_INCOME\")\n\n#select median income field from income layer\nmedian_income &lt;- income_data |&gt; \n  select('GEOID','B19013e1') \n\n#reproj texas geomms\nreproj_tx_geoms &lt;- st_transform(texas_geoms, crs = 3083)\n\n\n\n\nDetermining which census tracts experienced blackouts\nNow, I will join the income data to the census tract geometries by geometry ID, and spatially join census tract data with buildings determined to be impacted by blackouts to find which census tracts had blackouts.\n\n\nShow code\n#want to join all of both data sets by GEOMID, so using a left join\njoined_income_census &lt;- left_join(reproj_tx_geoms, median_income, by = c(\"GEOID_Data\" = \"GEOID\")) \n\n#spatially join tx census data with buildings impacted by blackouts\nblackout_bldgs_census &lt;- st_join(blackout_homes,joined_income_census)\n\n#determine which census tracts were impacted by the blackout\nblackout_tracts &lt;- blackout_bldgs_census |&gt; \n  group_by(TRACTCE) |&gt; \n  summarize(B19013e1 = n()) \n#just has tract id, median income, and associated geometries\n\n\n\n\nComparison of incomes of impacted tracts to unimpacted tracts\n\n\nShow code\n#median income by tract, \njoined_median_income_census &lt;- joined_income_census |&gt; group_by(TRACTCE, B19013e1)|&gt;\n  summarize()\n#just has tract id, median income, and associated geometries\n\n#map of median income homes by census tract that experienced blackouts\ntm_shape(houston_sf) + tm_polygons(col = \"white\") +\n  tm_shape(joined_median_income_census) +\n  tm_polygons(col = \"white\") +\n  tm_shape(blackout_tracts) +\n  tm_polygons(col = \"blue\") +\n  tm_layout(main.title = \"Median Income Homes in Houston Impacted by Blackouts\", title.size = 0.1)  ##FIX THIS!\n\n\n\n\n\nFigure 5. Map of median income by census tract\n\n\n\n\nShow code\n#distribution of income in immpacted tracts - histogram w ggplot\n#make histogram \nimpacted_hist &lt;- ggplot(joined_median_income_census) +\n  geom_histogram(aes(x = B19013e1),\n                 color = \"black\",\n                 fill = \"seagreen3\") +\n  labs(x = \"Median Income ($)\",\n       y = \"Count\",\n       title = \"Distribution of income in impacted tracts\") +\n  theme_classic()\n\nimpacted_hist\n\n\n\n\n\nFigure 6. Distribution of income in impacted tracts\n\n\n\n\nShow code\n#need to take out geom for both median income census data and blackout datato find tracts not affected\njoined_income_census_nogeom &lt;- joined_median_income_census |&gt; \n  st_drop_geometry()\n\nblackout_tracts_nogeom &lt;- blackout_tracts |&gt; \n  st_drop_geometry()\n\n##not joined_income_census\n\n#need to find tracts that were not impacted by blackout\nnon_impacted_tracts &lt;- anti_join(x = joined_income_census_nogeom, y = blackout_tracts_nogeom, by = \"TRACTCE\")\n\n#make hist of dist\nnon_impacted_hist &lt;- ggplot(non_impacted_tracts) +\n  geom_histogram(aes(x = B19013e1),\n                 color = \"black\",\n                 fill = \"orchid\") +\n  labs(x = \"Median Income ($)\",\n       y = \"Count\",\n       title = \"Distribution of income in non-impacted tracts\") +\n  theme_classic()\n\nnon_impacted_hist\n\n\n\n\n\nFigure 7. Distribution of income in non-impacted tracts\n\n\n\n\nShow code\nlibrary(gridExtra)\n#plot both side by side to compare\nboth_hists &lt;- grid.arrange(impacted_hist,non_impacted_hist)\n\nboth_hists\n\n\n\n\n\nFigure 8. Comparison of distribution of income fpr impacted versus non-impacted tracts\n\n\nThis study showed that there were more houses with a lower median income impacted by the blackout than those not impacted by the blackout. However, the distribution of income in impacted areas and non impacted areas are very similar (both being skewed to the right). Limitations of this study could include non-response bias with the Census data or could have used differing sizes for the highway buffers. People could have not responded to Census data collection, or families could have impacted the intensity of light coming from their home depending if they were there or not the days the images were collected."
  },
  {
    "objectID": "posts/2023-09-07_spotify/index.html",
    "href": "posts/2023-09-07_spotify/index.html",
    "title": "Exploring Music Tastes with Machine Learning",
    "section": "",
    "text": "Introduction\nI love listening to all types of music, and used machine learning models in R to understand my taste a little bit better and compare my friend and my music taste!\nI began by requesting data from the Spotify API and using the data on my liked songs to build a model that is a binary classifier. To do so, I will be using three Machine Learning algorithms:\n\nK-Nearest Neighbor\nDecision Tree Model\nRandom Forest Model\n\nAccessing Spotify API\nTo begin gathering data from the Spotify API, I have to create and access a token containing the client ID and client secret. To do so, I began by navigating to Spotify for Developers and created an application. More information on accessing the Spotify API can be found in the documentation.\nHere, I am setting them as system values so I don’t have to provide the client ID & secret each time the API is used, and combining them to create an access token.\n\nSys.setenv(SPOTIFY_CLIENT_ID = 'your_token')\nSys.setenv(SPOTIFY_CLIENT_SECRET = 'your_token')\n\naccess_token &lt;- get_spotify_access_token(\n  client_id = Sys.getenv(\"SPOTIFY_CLIENT_ID\"),\n  client_secret = Sys.getenv(\"SPOTIFY_CLIENT_SECRET\")\n)\n\nData Preparation\nThe function ‘get_my_saved_tracks()’ from the spotifyr package will request all my liked tracks on Spotify. However, when called, the Spotify API will only return a dataframe with 50 tracks at a time.\n\nsaved_tracks &lt;- get_my_saved_tracks() #function from the spotifyr package\n\nSince I want to analyze all my likes, I will have to make many requests. Instead of doing this manually, I’ll use a function to combine all the requests into one call.\n\n#writing a function to combine my requests into one call since this function only returns up to 50 tracks at a time when called\nget_saved_tracks &lt;- function(limit = 50,\n                             authorization,\n                             offset = 0) {\n  tracks &lt;- data.frame()\n  for (i in 1:7) {\n    new_tracks &lt;- get_my_saved_tracks(limit = limit,\n                                      offset = offset)\n    tracks &lt;- rbind(tracks,\n                    as.data.frame(new_tracks))\n    offset &lt;- offset + limit\n  }\n  return(tracks)\n}\n\nmy_tracks &lt;- get_saved_tracks(authorization = access_token)\n\nNow that I have all my liked songs, I am going to request more information from the Spotify API to understand my taste better. To do so, I will give the API a list of my song IDs using the function get_track_audio_features. This will return a dataframe of audio features, including the tracks and their attributes.\n\nmy_audio_features &lt;- data.frame()   # create base empty data frame\n\nfor(i in seq(from = 1, to = 350, by = 100)) { # loop through all songs\n  \n  # collect 100 rows starting from i\n  row_index &lt;- i:(i + 99)   \n  \n  # pull out features for set rows\n  audio &lt;- get_track_audio_features(my_tracks$track.id[row_index])\n  \n  # add features to dataframe\n  my_audio_features &lt;- rbind(my_audio_features, audio)\n}\n\n\nmy_audio_features &lt;- drop_na(my_audio_features)\n\n\n# add songs_data$track.name\nkiran_audio &lt;- cbind(my_audio_features,\n                     track.name = my_tracks$track.name,\n                     track.popularity = my_tracks$track.popularity)\n\nkiran_audio &lt;- kiran_audio %&gt;% \n  select(-c(uri, track_href, analysis_url, type, id))\n\n#make a csv\nwrite_csv(kiran_audio, \"kiran_audio.csv\")\n\nNow, I swapped data with my friend, Erica. Since I want to compare our music tastes, I began by combining our data into a new dataframe.\n\n#read in erica's data \nerica_audio &lt;- read_csv(\"ericas_audio.csv\")\n\n#add listener id column \nkiran_audio &lt;- kiran_audio %&gt;%  \n  mutate(listener_id = 'Kiran')\n\n\nkiran_erica_audio &lt;- rbind(kiran_audio, erica_audio)\n\n#downloading combined data into a csv\nwrite_csv(combined_audio, \"combined_audio.csv\")\n\nHere I loaded in the downloaded dataframe, and appended a column called ‘listener_id’ so I know whose tracks are whose.\n\n#combining our data \ncombined_audio &lt;- read_csv(here(\"posts\",\n                                \"2023-09-07_spotify\",\n                                \"combined_audio.csv\")) %&gt;%  \n    mutate(listener_id = as.factor(listener_id))\n\nData Exploration\nNow that I have prepared the data with the steps above, I can start exploring some aspects of my data, my friend’s, and compare them!\nFirst, I wanted to look at all the variables that the Spotify API includes when accessing audio features.\n\naudio_features &lt;- combined_audio %&gt;% \n  colnames() %&gt;% \n  as.data.frame()\n\n#removing the first row of insignificant data \naudio_features_table &lt;- as.data.frame(audio_features[-1,])\n\n#renaming column to audio feature\ncolnames(audio_features_table)[1] &lt;- \"Audio Features\"\n\n\naudio_features_table\n\n     Audio Features\n1              ...2\n2      danceability\n3            energy\n4               key\n5          loudness\n6              mode\n7       speechiness\n8      acousticness\n9  instrumentalness\n10         liveness\n11          valence\n12            tempo\n13      duration_ms\n14   time_signature\n15       track.name\n16 track.popularity\n17      listener_id\n\n\nI’m also curious about the relationship between some of the variables mentioned above..\n\n#danceability and energy\nhexplot_1 &lt;- ggplot(data = combined_audio,\n         aes(energy, danceability)) +\n  geom_hex() +\n  scale_fill_viridis_c(option = \"magma\") \n\n#danceability and loudness\nhexplot_2 &lt;- ggplot(data = combined_audio,\n         aes(loudness, danceability)) +\n  geom_hex() +\n  scale_fill_viridis_c(option = \"magma\")\n\n\n#acousticness and energy\nhexplot_3 &lt;- ggplot(data = combined_audio,\n         aes(acousticness, energy)) +\n  geom_hex() +\n  scale_fill_viridis_c(option = \"magma\")\n\n\n#acousticness and energy\nhexplot_4 &lt;- ggplot(data = combined_audio,\n         aes(tempo, loudness)) +\n  geom_hex() +\n  scale_fill_viridis_c(option = \"magma\")\n\n\nggarrange(hexplot_1, hexplot_2, hexplot_3, hexplot_4)\n\n\n\n\nNow, I’ll compare who has more popular songs!\n\ncombined_audio %&gt;%\n  arrange(desc(track.popularity)) %&gt;% \n  select(track.popularity,\n         track.name,\n         listener_id) %&gt;% \n  rename('track name' = track.name,\n         'listener' = listener_id) %&gt;%  \n  head(8) %&gt;% \n  kable()\n\n\n\n\ntrack.popularity\ntrack name\nlistener\n\n\n\n\n85\nNeverita\nKiran\n\n\n84\nEscapism. - Sped Up\nKiran\n\n\n82\nPink + White\nKiran\n\n\n81\nBeggin’\nErica\n\n\n81\nAll The Stars (with SZA)\nErica\n\n\n81\nLet Me Down Slowly\nErica\n\n\n80\nFormula\nKiran\n\n\n80\nUn Coco\nKiran\n\n\n\n\n\nLooks like I have a the most popular songs in this dataset from this little snippet. Now I’ll visualize the data to get a better picture.\n\n#visualize the data!\n#who listens to more popular track\npopularity_plot &lt;- ggplot(data = combined_audio, aes(x = track.popularity)) + \n  geom_bar(aes(fill = listener_id)) +\n   labs(title = \"Distribution of Song Popularity by Listener\",\n       x = \"Song Popularity\", y = \"Count\") +\n  scale_fill_manual(values = c(\"#1721d4\", \"#02b34b\")) +\n  theme_minimal() \n\npopularity_plot\n\n\n\n\nModeling\nAs I mentioned earlier, I will create two models, a K-Nearest Neighbor model and a decision tree model. Now that I have the data prepared and I understand it better, I can make these models predict whether a track belongs to me or Erica’s Spotify list.\nI’m starting by splitting the data into three sets: testing, splitting, and training data sets.\n\n#remove track id  & index \ncombined_audio &lt;- combined_audio %&gt;% \n  select(-c(...1,...2,track.name))\n\n#set seed for reproducibility\nset.seed(711)\n\n#split the data \naudio_split &lt;- initial_split(combined_audio)\naudio_test &lt;- testing(audio_split)\naudio_train &lt;- training(audio_split)\n\nNow, I will run through the three algorithms mentioned above. With each model, I will go through the following steps:\n\nPreprocessing: Using a step function and recipe on the training data.\nSet model specification: Tune specification of model with hyper parameters to finding best version of model. I will use cross validation folds to do this, which basically breaks the data into 10 sections, leaving 1 section as test data and rest are training. Then R continues thing process through all the broken up sections of data to determine the best hyper-parameters. The model will create predictions of 0 or 1 based on this tuning step.\nModel fitting: Then we fit the model with the best hyper-parameters onto the test data we split at beginning.\n\nMODEL #1: K-Nearest Neighbors\n\n#preprocessing\n#recipe always define by training data\nmusic_rec &lt;- recipe(listener_id ~., \n                    data = audio_train) %&gt;%  \n  step_dummy(all_nominal(),\n             -all_outcomes(),\n             one_hot = TRUE) %&gt;% \n  step_normalize(all_numeric(),\n                 -all_outcomes()) %&gt;% \n  prep()\n\n#bake \nbaked_audio &lt;- bake(music_rec, audio_train)\n\n#apply recipe to test data \nbaked_test &lt;- bake(music_rec, audio_test)\n\n#specify knn model\nknn_spec &lt;- nearest_neighbor() %&gt;% \n  set_engine(\"kknn\") %&gt;% \n  set_mode(\"classification\")\n\n#resampling folds\ncv_folds &lt;- audio_train %&gt;% \n  vfold_cv(v = 5)\n\n#put together into workflow\nknn_workflow &lt;- workflow() %&gt;% \n  add_model(knn_spec) %&gt;% \n  add_recipe(music_rec)\n\n#fit resamples\nknn_resample &lt;- knn_workflow %&gt;% \n  fit_resamples(\n    resamples = cv_folds,\n    control = control_resamples(save_pred = TRUE)\n  )\n\n\n#Define our KNN model with tuning\nknn_spec_tuned  &lt;- \n  nearest_neighbor(neighbors = tune()) %&gt;% \n  set_mode(\"classification\") %&gt;% \n  set_engine(\"kknn\")\n#Check the model\nknn_spec_tuned\n\nK-Nearest Neighbor Model Specification (classification)\n\nMain Arguments:\n  neighbors = tune()\n\nComputational engine: kknn \n\n\n\n# Define a new workflow\nwf_knn_tuned &lt;- workflow() |&gt; \n  add_model(knn_spec_tuned) |&gt; \n  add_recipe(music_rec)\n    \n# Fit the workflow on our predefined folds and hyperparameters\nfit_knn_cv &lt;- wf_knn_tuned |&gt; \n  tune_grid(\n    cv_folds, #tuning based on these folds \n    grid = data.frame(neighbors = c(1,5, seq(10,100,10)))\n    \n  )\n\n# Check the performance with collect_metrics()\nfit_knn_cv |&gt;  collect_metrics()\n\n# A tibble: 24 × 7\n   neighbors .metric  .estimator  mean     n std_err .config              \n       &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1         1 accuracy binary     0.633     5  0.0342 Preprocessor1_Model01\n 2         1 roc_auc  binary     0.633     5  0.0400 Preprocessor1_Model01\n 3         5 accuracy binary     0.658     5  0.0356 Preprocessor1_Model02\n 4         5 roc_auc  binary     0.708     5  0.0462 Preprocessor1_Model02\n 5        10 accuracy binary     0.667     5  0.0299 Preprocessor1_Model03\n 6        10 roc_auc  binary     0.731     5  0.0374 Preprocessor1_Model03\n 7        20 accuracy binary     0.667     5  0.0182 Preprocessor1_Model04\n 8        20 roc_auc  binary     0.744     5  0.0229 Preprocessor1_Model04\n 9        30 accuracy binary     0.677     5  0.0173 Preprocessor1_Model05\n10        30 roc_auc  binary     0.750     5  0.0160 Preprocessor1_Model05\n# ℹ 14 more rows\n\nfinal_knn_wf &lt;- wf_knn_tuned |&gt;  \n    finalize_workflow(select_best(fit_knn_cv,\n                                  metric = \"accuracy\"))\n\n# Fitting our final workflow \nfinal_knn_fit &lt;- final_knn_wf |&gt; \n  fit(data = audio_train)\n\nmusic_pred &lt;- final_knn_fit |&gt; \n  predict(new_data = audio_test)\n\n# Write over 'final_fit' with this last_fit() approach \nfinal_knn_fit &lt;- final_knn_wf |&gt; \n  last_fit(audio_split)\n\nfinal_knn_fit$.predictions\n\n[[1]]\n# A tibble: 159 × 6\n   .pred_Erica .pred_Kiran  .row .pred_class listener_id .config             \n         &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;       &lt;chr&gt;               \n 1       0.266       0.734     1 Kiran       Kiran       Preprocessor1_Model1\n 2       0.529       0.471     2 Erica       Kiran       Preprocessor1_Model1\n 3       0.215       0.785     5 Kiran       Kiran       Preprocessor1_Model1\n 4       0.533       0.467    10 Erica       Kiran       Preprocessor1_Model1\n 5       0.392       0.608    11 Kiran       Kiran       Preprocessor1_Model1\n 6       0.594       0.406    30 Erica       Kiran       Preprocessor1_Model1\n 7       0.366       0.634    31 Kiran       Kiran       Preprocessor1_Model1\n 8       0.699       0.301    34 Erica       Kiran       Preprocessor1_Model1\n 9       0.416       0.584    35 Kiran       Kiran       Preprocessor1_Model1\n10       0.401       0.599    41 Kiran       Kiran       Preprocessor1_Model1\n# ℹ 149 more rows\n\n# Collect metrics on the test data\nknn_metrics &lt;- final_knn_fit |&gt; \n  collect_metrics()\n\nknn_metrics\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.667 Preprocessor1_Model1\n2 roc_auc  binary         0.758 Preprocessor1_Model1\n\n\nMODEL #2: DECISION TREE\n\n#preprocess\ndec_tree_rec &lt;- recipe(listener_id ~ .,\n                       data = audio_train) %&gt;% \n  step_dummy(all_nominal(),\n             -all_outcomes(),\n             one_hot = TRUE) %&gt;% \n  step_normalize(all_numeric(),\n                 -all_outcomes())\n\n\n#dec tree specification tuned to the optimal parameters\n\n#tell the model that we are tuning hyperparams\ndec_tree_spec_tune &lt;- decision_tree(\n  cost_complexity = tune(), #to tune, call tune()\n  tree_depth = tune(), \n  min_n = tune()) %&gt;%  \n  set_engine(\"rpart\") %&gt;%\n  set_mode(\"classification\")\n\ndec_tree_grid &lt;- grid_regular(cost_complexity(),\n                              tree_depth(),\n                              min_n(),\n                              levels = 4) \n\ndec_tree_grid \n\n# A tibble: 64 × 3\n   cost_complexity tree_depth min_n\n             &lt;dbl&gt;      &lt;int&gt; &lt;int&gt;\n 1    0.0000000001          1     2\n 2    0.0000001             1     2\n 3    0.0001                1     2\n 4    0.1                   1     2\n 5    0.0000000001          5     2\n 6    0.0000001             5     2\n 7    0.0001                5     2\n 8    0.1                   5     2\n 9    0.0000000001         10     2\n10    0.0000001            10     2\n# ℹ 54 more rows\n\n\n\ndoParallel::registerDoParallel() #build trees in parallel\n#200s\ndec_tree_rs &lt;- tune_grid(\n  dec_tree_spec_tune, \n  as.factor(listener_id)~.,\n  resamples = cv_folds,\n  grid = dec_tree_grid,\n  metrics = metric_set(accuracy)\n)\ndec_tree_rs\n\n# Tuning results\n# 5-fold cross-validation \n# A tibble: 5 × 4\n  splits           id    .metrics          .notes          \n  &lt;list&gt;           &lt;chr&gt; &lt;list&gt;            &lt;list&gt;          \n1 &lt;split [379/95]&gt; Fold1 &lt;tibble [64 × 7]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [379/95]&gt; Fold2 &lt;tibble [64 × 7]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [379/95]&gt; Fold3 &lt;tibble [64 × 7]&gt; &lt;tibble [0 × 3]&gt;\n4 &lt;split [379/95]&gt; Fold4 &lt;tibble [64 × 7]&gt; &lt;tibble [0 × 3]&gt;\n5 &lt;split [380/94]&gt; Fold5 &lt;tibble [64 × 7]&gt; &lt;tibble [0 × 3]&gt;\n\n\n\n# Selecting best models \nshow_best(dec_tree_rs)\n\n# A tibble: 5 × 9\n  cost_complexity tree_depth min_n .metric  .estimator  mean     n std_err\n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1    0.0000000001         10    14 accuracy binary     0.675     5  0.0177\n2    0.0000001            10    14 accuracy binary     0.675     5  0.0177\n3    0.0001               10    14 accuracy binary     0.675     5  0.0177\n4    0.0000000001         15    14 accuracy binary     0.675     5  0.0177\n5    0.0000001            15    14 accuracy binary     0.675     5  0.0177\n# ℹ 1 more variable: .config &lt;chr&gt;\n\nselect_best(dec_tree_rs)\n\n# A tibble: 1 × 4\n  cost_complexity tree_depth min_n .config              \n            &lt;dbl&gt;      &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1    0.0000000001         10    14 Preprocessor1_Model25\n\n# Finalizing our model \nfinal_dec_tree &lt;- finalize_model(dec_tree_spec_tune,\n                                 select_best(dec_tree_rs))\n\nfinal_dec_tree_fit &lt;- last_fit(final_dec_tree,\n                               as.factor(listener_id) ~.,\n                               audio_split)\n\n# Outputting Metrics \nfinal_dec_tree_fit$.predictions\n\n[[1]]\n# A tibble: 159 × 6\n   .pred_Erica .pred_Kiran  .row .pred_class `as.factor(listener_id)` .config   \n         &lt;dbl&gt;       &lt;dbl&gt; &lt;int&gt; &lt;fct&gt;       &lt;fct&gt;                    &lt;chr&gt;     \n 1      0.114       0.886      1 Kiran       Kiran                    Preproces…\n 2      0.875       0.125      2 Erica       Kiran                    Preproces…\n 3      0.667       0.333      5 Erica       Kiran                    Preproces…\n 4      0.0732      0.927     10 Kiran       Kiran                    Preproces…\n 5      0.0732      0.927     11 Kiran       Kiran                    Preproces…\n 6      0.0357      0.964     30 Kiran       Kiran                    Preproces…\n 7      0.455       0.545     31 Kiran       Kiran                    Preproces…\n 8      0.952       0.0476    34 Erica       Kiran                    Preproces…\n 9      0.114       0.886     35 Kiran       Kiran                    Preproces…\n10      0.778       0.222     41 Erica       Kiran                    Preproces…\n# ℹ 149 more rows\n\ndec_tree_metrics &lt;- final_dec_tree_fit %&gt;% \n  collect_metrics()\n\ndec_tree_metrics\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.648 Preprocessor1_Model1\n2 roc_auc  binary         0.689 Preprocessor1_Model1\n\n\nThen validate and compare the performance of the models I made\nMODEL #3: Random Forest\n\n# Define validating set \nvalidation_set &lt;- validation_split(audio_train, \n                                   strata = listener_id, \n                                   prop = 0.70)\n\n# random forest spec \nrand_forest_spec &lt;-\n  rand_forest(mtry = tune(),\n              min_n = tune(),\n              trees = 1000)  %&gt;%  \n  set_engine(\"ranger\") %&gt;% \n  set_mode(\"classification\")\n\n# random forest workflow\nrand_forest_workflow &lt;- workflow() %&gt;%  \n  add_recipe(music_rec) %&gt;%  \n  add_model(rand_forest_spec)\n\n# buuild in parallel \ndoParallel::registerDoParallel()\n\nrand_forest_res &lt;- \n  rand_forest_workflow %&gt;%  \n  tune_grid(validation_set,\n            grid = 25,\n            control = control_grid(save_pred = TRUE),\n            metrics = metric_set(accuracy))\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n## model metrics\nrand_forest_res %&gt;% collect_metrics()\n\n# A tibble: 25 × 8\n    mtry min_n .metric  .estimator  mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     9    30 accuracy binary     0.734     1      NA Preprocessor1_Model01\n 2     3    35 accuracy binary     0.748     1      NA Preprocessor1_Model02\n 3     4    19 accuracy binary     0.748     1      NA Preprocessor1_Model03\n 4     5    14 accuracy binary     0.741     1      NA Preprocessor1_Model04\n 5     7     7 accuracy binary     0.762     1      NA Preprocessor1_Model05\n 6     9    28 accuracy binary     0.755     1      NA Preprocessor1_Model06\n 7     3     9 accuracy binary     0.734     1      NA Preprocessor1_Model07\n 8     8    18 accuracy binary     0.741     1      NA Preprocessor1_Model08\n 9    12    31 accuracy binary     0.762     1      NA Preprocessor1_Model09\n10    11     3 accuracy binary     0.741     1      NA Preprocessor1_Model10\n# ℹ 15 more rows\n\n# find best accuracy metric \nrand_forest_res %&gt;%  \n  show_best(metric = \"accuracy\")\n\n# A tibble: 5 × 8\n   mtry min_n .metric  .estimator  mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    14    38 accuracy binary     0.769     1      NA Preprocessor1_Model19\n2     7     7 accuracy binary     0.762     1      NA Preprocessor1_Model05\n3    12    31 accuracy binary     0.762     1      NA Preprocessor1_Model09\n4    10    15 accuracy binary     0.762     1      NA Preprocessor1_Model13\n5    13     6 accuracy binary     0.762     1      NA Preprocessor1_Model25\n\n# plot \nautoplot(rand_forest_res)\n\n\n\n# choose best random forest model \nbest_rand_forest &lt;- select_best(rand_forest_res, \"accuracy\")\nbest_rand_forest\n\n# A tibble: 1 × 3\n   mtry min_n .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;                \n1    14    38 Preprocessor1_Model19\n\n# output preds\nrand_forest_res %&gt;%   \n  collect_predictions()\n\n# A tibble: 3,575 × 7\n   id         .pred_class  .row  mtry min_n listener_id .config              \n   &lt;chr&gt;      &lt;fct&gt;       &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt;       &lt;chr&gt;                \n 1 validation Kiran           5     9    30 Kiran       Preprocessor1_Model01\n 2 validation Erica           6     9    30 Erica       Preprocessor1_Model01\n 3 validation Kiran           7     9    30 Kiran       Preprocessor1_Model01\n 4 validation Erica          13     9    30 Kiran       Preprocessor1_Model01\n 5 validation Kiran          14     9    30 Kiran       Preprocessor1_Model01\n 6 validation Erica          21     9    30 Erica       Preprocessor1_Model01\n 7 validation Kiran          24     9    30 Erica       Preprocessor1_Model01\n 8 validation Kiran          27     9    30 Kiran       Preprocessor1_Model01\n 9 validation Erica          33     9    30 Erica       Preprocessor1_Model01\n10 validation Kiran          35     9    30 Kiran       Preprocessor1_Model01\n# ℹ 3,565 more rows\n\n# final model working in parallel \ndoParallel::registerDoParallel()\nlast_rand_forest_model &lt;- \n  rand_forest(mtry = 2, min_n = 3, trees = 1000) %&gt;%  \n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%  \n  set_mode(\"classification\")\n\n#Updating our workflow \nlast_rand_forest_workflow &lt;- \n  rand_forest_workflow %&gt;%  \n  update_model(last_rand_forest_model)\n\n# Updating our model fit \nlast_rand_forest_fit &lt;- \n  last_rand_forest_workflow %&gt;%  \n  last_fit(audio_split)\n\n# Outputting model metrics \nrand_forest_metrics &lt;- last_rand_forest_fit %&gt;%   \n  collect_metrics()\n\nrand_forest_metrics\n\n# A tibble: 2 × 4\n  .metric  .estimator .estimate .config             \n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy binary         0.748 Preprocessor1_Model1\n2 roc_auc  binary         0.829 Preprocessor1_Model1\n\n# find most important variables to our model \nlast_rand_forest_fit %&gt;%  \n  extract_fit_parsnip() %&gt;%  \n  vip::vip(num_features = 12) + \n  ggtitle(\"Order of Variable Importance in Random Forest Model\") +\n  theme_minimal()\n\n\n\n\n\n# nearest neighbors metrics\nknn_accuracy &lt;- knn_metrics$.estimate[1]\n\n# decision tree metrics\ndec_tree_accuracy &lt;- dec_tree_metrics$.estimate[1]\n\n# random forest  metrics\nrandom_forest_accuracy &lt;- rand_forest_metrics$.estimate[1]\n\nmodel_accuracy &lt;- tribble(\n  ~\"model\", ~\"accuracy\",\n  \"K-Nearest Neighbor\", knn_accuracy,\n  \"Decision Tree\", dec_tree_accuracy,\n  \"Random Forest\", random_forest_accuracy\n)\n\n# Plotting bar chart to compare models accuracy \nggplot(data = model_accuracy, aes(x = model,\n                                  y = accuracy)) +\n  geom_col(fill = c(\"red\",\"purple\",\"blue\")) +\n  theme_minimal() +\n  labs(title = \"Comparison of Model Accuracy for Spotify Data\",\n       x = \"Model\",\n       y = \"Accuracy\")\n\n\n\n\nThis analysis suggests that the Random Forest model has the best accuracy at 69.2% and the worst model is the Decision Tree model with 64.8% accuracy.\n\n\n\nCitationBibTeX citation:@online{favre2023,\n  author = {Favre, Kiran},\n  title = {Exploring {Music} {Tastes} with {Machine} {Learning}},\n  date = {2023-09-07},\n  url = {https://kiranfavre.github.io/posts/2023-09-07/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nFavre, Kiran. 2023. “Exploring Music Tastes with Machine\nLearning.” September 7, 2023. https://kiranfavre.github.io/posts/2023-09-07/."
  },
  {
    "objectID": "posts/2023-3-15_ethics/index.html",
    "href": "posts/2023-3-15_ethics/index.html",
    "title": "An Evaluation of Air Pollution In India",
    "section": "",
    "text": "As an Indian-America, I spent my childhood going to India every year to visit family. I had noticed the air quality seem to get worse over the years. For the completion of my Ethics & Bias in Environmental Data Science course, I decided to investigate potential areas of bias in data collected and presented on air pollution in India."
  },
  {
    "objectID": "posts/2023-3-15_ethics/index.html#introduction",
    "href": "posts/2023-3-15_ethics/index.html#introduction",
    "title": "An Evaluation of Air Pollution In India",
    "section": "",
    "text": "As an Indian-America, I spent my childhood going to India every year to visit family. I had noticed the air quality seem to get worse over the years. For the completion of my Ethics & Bias in Environmental Data Science course, I decided to investigate potential areas of bias in data collected and presented on air pollution in India."
  },
  {
    "objectID": "posts/2023-3-15_ethics/index.html#background",
    "href": "posts/2023-3-15_ethics/index.html#background",
    "title": "An Evaluation of Air Pollution In India",
    "section": "Background",
    "text": "Background\nIn 2021, the air quality in India exceeded the World Health Organization’s (WHO) health standard by over ten times. Access to clean air was recognized as a human right in July 2022, and many countries, including India, had adopted legislation to regulate air pollution long before this. So why is India’s air quality still as poor as it is? Is it due to reasons out of human control, such as climate effects trapping in air pollution, or is it a discrepancy in efforts to mitigate the problem (or both)? I will be exploring the conditions that perpetuate poor air quality in this region, data collection/access, and improvement efforts through an ethics and bias lens. This evaluation can lend itself to support the sustainable growth of the population and industrialization in this area.\n\n\n\nIndia Gate, sourced from The Guardian\n\n\nAir quality is measured by the concentration of particulate matter suspended in the air. A commonly used metric to report this is PM2.5, which is used to calculate the Air Quality Index. India’s relatively poor air quality can be attributed to poor farming practices, reliance on coal, high population density, and lack of policy fit for the current demand. Consequences of bad air quality include lung disease, increased risk of other diseases or death, and the occasional order to stay home. One quarter of all deaths in 2019 due to air pollution%20worldwide%20in%202019.) were in India, reflecting the disproportionate amount of air pollution in India compared to other regions."
  },
  {
    "objectID": "posts/2023-3-15_ethics/index.html#air-quality-in-india",
    "href": "posts/2023-3-15_ethics/index.html#air-quality-in-india",
    "title": "An Evaluation of Air Pollution In India",
    "section": "Air Quality in India",
    "text": "Air Quality in India\nThere are two legislations regarding air quality in India, the Air (Prevention and Control of Pollution) Act of 1981, amended in 1987, and the Environmental Protection Act of 1986. Both of these are aimed at defining and attempting to reduce air pollution in the overarching goal of protecting the environment. These legislations, while well intentioned, are out of date and do not reflect the current levels of air pollution that the government of India could make a more serious effort of reducing. Lack of enforcement and stalemates between the state and federal government has allowed for practices such a burning coal and old crop fields that contributes greatly to the air pollution in India.\nThe latter has been a topic of tension of India in recent years as air quality has declined more year after year. India is one of the largest global agricultural producers, so emissions from this sector are inevitable. However, the amount of emissions being released from agricultural practices could be reduced greatly. Every fall, farmers burn leftover crop residue to make space for the incoming crop season which leads to worse air quality from October to December. There has not traditionally been much government oversight over this process, but in recent years there have been more efforts to reduce emissions. These efforts from the boards that enforce the legislations mentioned above have relatively new programs, such as the National Clean Air Program, to provide farmers with new equipment to sustainably remove their crops rather than burn them. Since there is not a lot of updated infrastructure to support these types of programs, it may take years to make a difference in agricultural emissions. In the future, policy will need to follow through with the best interest of the public over profit to slowly increase air quality in response to its current state, not its state when these programs were founded.\n\n\n\nAgriculture in India, sourced from Foodtank"
  },
  {
    "objectID": "posts/2023-3-15_ethics/index.html#data-gathering-and-access",
    "href": "posts/2023-3-15_ethics/index.html#data-gathering-and-access",
    "title": "An Evaluation of Air Pollution In India",
    "section": "Data Gathering and Access",
    "text": "Data Gathering and Access\nWithout a full grasp of the problem at hand proper adjustments to current policy will be difficult to make. Using air quality data can help decision makers determine sources of air pollution. There are two federal air quality monitor types in India, both regulated by the the Central Pollution Control Board (CPCB). The first being of industrial emissions began in 2104 and is conducted through the Online Continuous Emissions/Effluents Monitoring Systems (OCEMS). As of 2021, there are 3,700 monitors across industrial regions around India. A concern with this data is that the industries being monitored are the ones commissioning and installing these monitors. This means there is likely missing data and underreported emissions, and less pressure on the same industries that are responsible for air pollution to reduce their emissions.\nThe second monitoring system in India is the Continuous Ambient Air Quality Monitoring Systems (CAAQMS) which inform the official AQI measurements for the country. There are 209 of these monitors. There is also much more monitoring of cities and industrial areas. This presents a divide in available data and a potential issue when creating projections and necessary efforts to mitigate air pollution. This monitoring system is not effective at representing the population, as in 2017 approximately 66% of the population lived in rural areas while these monitors are placed in industrial areas. Misrepresentation in data often leads to misrepresentation in government response, and in this case this could lead to less attention to reducing air pollution in rural regions since less is known about their conditions.\nPublic data access is an important part of creating diverse solutions to problems at this scale. On IQair.com, all the contributors of air quality data are listed, and 24 out of the 35 sources are listed as anonymous. This provides a data access issue, in that there is not full transparency in which groups are collecting air quality data and where to view it. The other eleven sources are government, corporate, and individual bodies. Not all of these contributors have access portals to view the data they collect. In addition to this, CAAQMS data regarding industrial emissions and pollution are not available to the public. This is where data justice comes into play. Is this lack of public access to data only one part in a larger dysfunctional system? An improvement in public access to data could be in the best interest of the public, allowing more than only the government to employ solutions to poor air quality."
  },
  {
    "objectID": "posts/2023-3-15_ethics/index.html#current-efforts",
    "href": "posts/2023-3-15_ethics/index.html#current-efforts",
    "title": "An Evaluation of Air Pollution In India",
    "section": "Current Efforts",
    "text": "Current Efforts\nArtificial intelligence projects have been approved to collect data across India on air quality. These projects will mainly monitor emissions from vehicles and levels of PM2.5 to help inform politicians in decision making. This artificial intelligence is aimed to help determine which areas may need more help than others, and could be used for accountability if the data is open access. These monitoring techniques can be useful to help regulate emissions if the information is used as intended. Artificial intelligence can be a helpful tool, but an invasive one. As seen in Coded Bias, artificial intelligence is not a silver bullet for the issues mentioned above with data quality and access. Biases of the developers of artificial intelligence often make it into their models, so this should be kept in mind when proceeding with the use of artificial intelligence.\nAlong with artificial intelligence, there are other sectors working to improve air quality in India. The Nature Conservancy (TNC) in India is working with farmers to use new technologies rather than burn their old crops. However, being able to give up current methods for a more expensive alternative is not an option, or priority, for every farmer. This is where policy lacks and where groups like TNC come into play. Many of the current policies do not have the funding to provide farmers with new, more sustainable equipment, so farmers are left with only guidance on how they should change their practices, but no support to do so. One area of improvement could be the collaboration between groups like TNC and the government to optimize these solutions at the greatest scale possible. This type of collaboration could lay a blueprint for a future of sustainable farming practices.\nWith a growing population and outdated policies, India has an air quality issue on their hands. As a country, some efforts have been made to address this problem, but there is room for improvement via collaboration, accountability, and innovation. How decisions are made and how data is accessed and distributed must change if air quality is going to improve in this region. There is opportunity for improvement in these areas: artificial intelligence can be used to help improve the understanding of air quality conditions, but collaboration between policy makers, farmers, scientists, and civilians will produce better results than solely relying on efforts by the TNC or artificial intelligence."
  },
  {
    "objectID": "posts/2023-3-15_ethics/index.html#references",
    "href": "posts/2023-3-15_ethics/index.html#references",
    "title": "An Evaluation of Air Pollution In India",
    "section": "References",
    "text": "References\n\nAjay Thakur, et al. “Clean Air Related Laws in India.” IPleaders, 21 Feb. 2017, https://blog.ipleaders.in/clean-air-related-laws-in-india/#:~:text=The%20two%20main%20laws%20that,provisions%20under%20these%20two%20acts.\nBhowmick, Nilanjana. “The Burning Problem.” The Nature Conservancy, 4 Nov. 2022, https://www.nature.org/en-us/magazine/magazine-articles/india-agriculture/.\n“Central Pollution Control Board.” CPCB, https://cpcb.nic.in/about-namp/.\n“Government to Employ Drones, AI to Check Air Quality and Fight Pollution.” Business Insider, 11 Nov. 2022, https://www.businessinsider.in/india/news/government-to-employ-drones-ai-to-check-air-quality-and-fight-pollution/articleshow/95443802.cms.\n“India Air Quality Index (AQI) and Air Pollution Information.” IQAir, https://www.iqair.com/us/india.\n“India - Food and Agriculture Value Chain.” International Trade Administration | Trade.gov, https://www.trade.gov/country-commercial-guides/india-food-and-agriculture-value-chain.\nKanwal, Sanyukta. “India - Population by Region 2017-2022.” Statista, 16 Oct. 2020, https://www.statista.com/statistics/1012239/india-population-by-region/.\n“Ministry of Coal, Goi.” Ministry of Coal, GOI, https://coal.nic.in/en/major-statistics/coal-indian-energy-choice#:~:text=%E0%A4%95%E0%A5%8B%E0%A4%AF%E0%A4%B2%E0%A4%BE%20%E0%A4%AE%E0%A4%82%E0%A4%A4%E0%A5%8D%E0%A4%B0%E0%A4%BE%E0%A4%B2%E0%A4%AF%20Ministry%20of%20Coal&text=Coal%20is%20the%20most%20important,of%20the%20country%27s%20energy%20need.\n“State of Global Air.” Global Health Impacts of Air Pollution | State of Global Air, https://www.stateofglobalair.org/health/global#:~:text=Millions%20of%20Deaths,-Air%20pollution%20is&text=Air%20pollution%20accounts%20for%20more,7.49%20million)%20worldwide%20in%202019.\nSutaria, Chetan Bhattacharji and Ronak. “Tackling Industrial Pollution in India: Where Is the Data?” ORF, 8 Feb. 2021, https://www.orfonline.org/research/tackling-industrial-pollution-in-india-where-is-the-data/.\nTiseo, Ian, and Jun 17. “India: Average Annual Deaths from Air Pollution 2019.” Statista, 17 June 2022, https://www.statista.com/statistics/935666/india-average-annual-deaths-from-air-pollution/.\n“UN General Assembly Declares Access to Clean and Healthy Environment a Universal Human Right | UN News.” United Nations, United Nations, https://news.un.org/en/story/2022/07/1123482.\nYasir, Sameer, and Mike Ives. “Air Quality in India’s Capital Is Dreadfully Bad. Again.” The New York Times, The New York Times, 4 Nov. 2022, https://www.nytimes.com/2022/11/04/world/asia/india-air-pollution-sickness.html#:~:text=Is%20Dreadfully%20Bad.-,Again.,traffic%20restrictions%20and%20political%20infighting."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Portfolio",
    "section": "",
    "text": "This blog is a collection of data science projects I have completed during my academic journey and after. This page is a work in progress, and will develop as I continue to complete projects!\nAll pictures used on this page are taken by me.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpatial Analysis of Houston Power Outages\n\n\n\nR\n\n\nGeospatial\n\n\n\nUsing geospatial data to understand Houston power outages.\n\n\n\nKiran Favre\n\n\nSep 14, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring Music Tastes with Machine Learning\n\n\n\nR\n\n\nMachine Learning\n\n\nMusic\n\n\n\nComparing a friend’s and my Spotify liked songs by building a binary classfication model.\n\n\n\nKiran Favre\n\n\nSep 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding the Relationship Between Temperature & Forest Area Burned\n\n\n\nStatistics\n\n\nR\n\n\n\nUsing a linear regression model to evaluate the relationship between temperature and hectares of forest burned.\n\n\n\nKiran Favre\n\n\nMay 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Machine Learning for Ocean Chemistry Prediction\n\n\n\nMachine Learning\n\n\nR\n\n\nOcean Chemistry\n\n\n\nTraining machine learning models to predict dissolved inorganic carbon in water samples.\n\n\n\nKiran Favre\n\n\nMar 21, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAn Evaluation of Air Pollution In India\n\n\n\nEthics\n\n\nAir Pollution\n\n\nData Access\n\n\n\nInvestigating air pollution in India, in the scope of ethic & bias in Environmental Data Science.\n\n\n\nKiran Favre\n\n\nMar 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of California Coastal Erosion\n\n\n\nR\n\n\nGeospatial\n\n\nCalifornia\n\n\n\nUsing muliple linear regression analysis and USGS Coastal Change data to analyze where coastlines in California are eroding the most.\n\n\n\nKiran Favre\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDetermining Suitable Oyster Habitats in West Coast EEZ’s\n\n\n\nR\n\n\nGeospatial\n\n\nCalifornia\n\n\n\nI will be using geospatial data to map suitable oyster habitats in Exclusive Economic Zones on the West Coast, and creating a function to find these zones for any species…\n\n\n\nKiran Favre\n\n\nDec 3, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  }
]